[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Méthodes d’analyse spatiale : un grand bol d’R",
    "section": "",
    "text": "Préface"
  },
  {
    "objectID": "index.html#sect001",
    "href": "index.html#sect001",
    "title": "Méthodes d’analyse spatiale : un grand bol d’R",
    "section": "Un manuel sous la forme d’une ressource éducative libre",
    "text": "Un manuel sous la forme d’une ressource éducative libre\nPourquoi un manuel sous licence libre?\nLes logiciels libres sont aujourd’hui très répandus. Comparativement aux logiciels propriétaires, l’accès au code source permet à quiconque de l’utiliser, de le modifier, de le dupliquer et de le partager. Le logiciel R, dans lequel sont mises en œuvre les méthodes d’analyse spatiale décrites dans ce livre, est d’ailleurs à la fois un langage de programmation et un logiciel libre (sous la licence publique générale GNU GPL2). Par analogie aux logiciels libres, il existe aussi des ressources éducatives libres (REL) « dont la licence accorde les permissions désignées par les 5R (Retenir — Réutiliser — Réviser — Remixer — Redistribuer) et donc permet nécessairement la modification » (fabriqueREL). La licence de ce livre, CC BY-SA (figure 1), permet donc de :\n\nRetenir, c’est-à-dire télécharger et imprimer gratuitement le livre. Notez qu’il aurait été plutôt surprenant d’écrire un livre payant sur un logiciel libre et donc gratuit. Aussi, nous aurions été très embarrassés que des personnes étudiantes avec des ressources financières limitées doivent payer pour avoir accès au livre, sans pour autant savoir préalablement si le contenu est réellement adapté à leurs besoins.\nRéutiliser, c’est-à-dire utiliser la totalité ou une section du livre sans limitation et sans compensation financière. Cela permet ainsi à d’autres personnes enseignantes de l’utiliser dans le cadre d’activités pédagogiques.\nRéviser, c’est-à-dire modifier, adapter et traduire le contenu en fonction d’un besoin pédagogique précis puisqu’aucun manuel n’est parfait, tant s’en faut! Le livre a d’ailleurs été écrit intégralement dans R avec Quatro. Quiconque peut ainsi télécharger gratuitement le code source du livre sur github et le modifier à sa guise (voir l’encadré intitulé Suggestions d’adaptation du manuel).\nRemixer, c’est-à-dire « combiner la ressource avec d’autres ressources dont la licence le permet aussi pour créer une nouvelle ressource intégrée » (fabriqueREL).\nRedistribuer, c’est-à-dire distribuer, en totalité ou en partie le manuel ou une version révisée sur d’autres canaux que le site Web du livre (par exemple, sur le site Moodle de votre université ou en faire une version imprimée).\n\nLa licence de ce livre, CC BY-SA (figure 1), oblige donc à :\n\nAttribuer la paternité de l’auteur dans vos versions dérivées, ainsi qu’une mention concernant les grandes modifications apportées, en utilisant la formulation suivante : Apparicio Philippe et Jérémy Gelb (2024). Méthodes d’analyses spatiales : un grand bol d’R. Université de Sherbrooke. fabriqueREL. Licence CC BY-SA.\nUtiliser la même licence ou une licence similaire à toutes versions dérivées.\n\n\n\nFigure 1: Licence Creative Commons du livre\n\n\n\n\n\n\nSuggestions d’adaptation du manuel\n\n\nPour chaque méthode d’analyse spatiale abordée dans le livre, une description détaillée et une mise en œuvre dans R sont disponibles. Par conséquent, plusieurs adaptations du manuel sont possibles :\n\nConserver uniquement les chapitres sur les méthodes ciblées dans votre cours.\nEn faire une version imprimée et la distribuer aux personnes étudiantes.\nModifier la description d’une ou de plusieurs méthodes en effectuant les mises à jour directement dans les chapitres.\nInsérer ses propres jeux de données dans les sections intitulées Mise en œuvre dans R.\nModifier les tableaux et figures.\nAjouter une série d’exercices.\nModifier les quiz de révision.\nRédiger un nouveau chapitre.\nModifier des syntaxes R. Plusieurs packages R peuvent être utilisés pour mettre en œuvre telle ou telle méthode. Ces derniers évoluent aussi très vite et de nouveaux packages sont proposés fréquemment! Par conséquent, il peut être judicieux de modifier une syntaxe R du livre en fonction de ses habitudes de programmation dans R (utilisation d’autres packages que ceux utilisés dans le manuel par exemple) ou de bien mettre à jour une syntaxe à la suite de la parution d’un nouveau package plus performant ou intéressant.\nToute autre adaptation qui permet de répondre au mieux à un besoin pédagogique."
  },
  {
    "objectID": "index.html#sect002",
    "href": "index.html#sect002",
    "title": "Méthodes d’analyse spatiale : un grand bol d’R",
    "section": "Comment lire ce manuel?",
    "text": "Comment lire ce manuel?\nLe livre comprend plusieurs types de blocs de texte qui en facilitent la lecture.\n\n\n\n\n\nBloc packages\n\n\nHabituellement localisé au début d’un chapitre, il comprend la liste des packages R utilisés pour un chapitre.\n\n\n\n\n\n\n\nBloc objectif\n\n\nIl comprend une description des objectifs d’un chapitre ou d’une section.\n\n\n\n\n\n\n\nBloc notes\n\n\nIl comprend une information secondaire sur une notion, une idée abordée dans une section.\n\n\n\n\n\n\n\nBloc pour aller plus loin\n\n\nIl comprend des références ou des extensions d’une méthode abordée dans une section.\n\n\n\n\n\n\n\nBloc astuce\n\n\nIl décrit un élément qui vous facilitera la vie : une propriété statistique, un package, une fonction, une syntaxe R.\n\n\n\n\n\n\n\nBloc attention\n\n\nIl comprend une notion ou un élément important à bien maîtriser.\n\n\n\n\n\n\n\nBloc exercice\n\n\nIl comprend un court exercice de révision à la fin de chaque chapitre."
  },
  {
    "objectID": "index.html#sect003",
    "href": "index.html#sect003",
    "title": "Méthodes d’analyse spatiale : un grand bol d’R",
    "section": "Comment utiliser les données du livre pour reproduire les exemples?",
    "text": "Comment utiliser les données du livre pour reproduire les exemples?\nCe livre comprend des exemples détaillés et appliqués dans R pour chacune des méthodes abordées. Ces exemples se basent sur des jeux de données structurés et mis à disposition avec le livre. Ils sont disponibles sur le repo github dans le sous-dossier data, à l’adresse https://github.com/SerieBoldR/MethodesAnalyseSpatiale/tree/main/data.\nUne autre option est de télécharger le repo complet du livre directement sur github (https://github.com/SerieBoldR/MethodesAnalyseSpatiale) en cliquant sur le bouton Code, puis le bouton Download ZIP (figure 2). Les données se trouvent alors dans le sous-dossier nommé data.\n\n\nFigure 2: Téléchargement de l’intégralité du livre"
  },
  {
    "objectID": "index.html#sect004",
    "href": "index.html#sect004",
    "title": "Méthodes d’analyse spatiale : un grand bol d’R",
    "section": "Liste des packages utilisés",
    "text": "Liste des packages utilisés\nDans ce livre, nous utilisons de nombreux packages R que vous pouvez installer avec le code ci-dessous.\n\n# Liste des packages\nListePackages &lt;- c(\"classInt\", \"cluster\", \"ClustGeo\", \"concaveman\", \"dbscan\", \"dplyr\", \"factoextra\", \n                   \"foot\", \"foreign\", \"future\", \"geocmeans\", \"ggplot2\", \"ggpubr\", \"ggthemes\", \n                   \"gifski\", \"Gmedian\", \"gpx\", \"gtfstools\", \"igraph\", \"lubridate\", \"MASS\", \n                   \"metR\", \"mgcv\", \"osmextract\", \"r5r\", \"raster\", \"RColorBrewer\", \"rgdal\", \n                   \"rgeoda\", \"rgl\", \"rmapshaper\", \"sf\", \"sparr\", \"spatialreg\", \"spatstat\", \n                   \"spdep\", \"spgwr\", \"spNetwork\", \"terra\", \"tmap\", \"viridis\", \"xlsx\")\n# Packages non installés dans la liste\nPackagesNonInstalles &lt;- ListePackages[!(ListePackages %in% installed.packages()[,\"Package\"])]\n# Installation des packages manquants\nif(length(new.packages)) install.packages(PackagesNonInstalles)"
  },
  {
    "objectID": "index.html#sect005",
    "href": "index.html#sect005",
    "title": "Méthodes d’analyse spatiale : un grand bol d’R",
    "section": "Structure du livre",
    "text": "Structure du livre\nLe livre est organisé autour de cinq grandes parties.\nPartie 1. Données spatiales et R. Dans cette première partie, nous voyons comment importer, manipuler, cartographier et exporter des données spatiales dans R, principalement avec les packages sf pour les données vectorielles, terra pour les données matricielles (images) et tmap pour la cartographie (chapitre 1). Maîtriser les notions abordées dans ce chapitre constitue une étape préalable et indispensable à tout projet d’analyse spatiale. D’une part, avant d’analyser des données spatiales, il convient de les structurer (importation et manipulation) et de les explorer (cartographie). D’autre part, une fois la ou les méthodes d’analyse spatiale mises en œuvre, il convient de cartographier les résultats finaux et de les exporter au besoin dans un format de données géographiques (shapefile (shp), GeoPackage (GPKG), GeoJSON (geojson), sqlite (sqlite), GeoTiff, etc.).\nPartie 2. Autocorrélation spatiale. L’autocorrélation spatiale est une notion fondamentale en analyse spatiale permettant de vérifier si les entités proches ou voisines ont tendance à être (dis)semblables en fonction d’un phénomène donné. Dans le chapitre 2, nous décrivons une panoplie de méthodes permettant de définir des matrices de pondération spatiale qui sont utilisées pour évaluer la dépendance spatiale d’une ou de plusieurs variables, soit les mesures d’autocorrélation spatiale globale et locale. La compréhension de la notion de dépendance spatiale et des différentes mesures d’autocorrélation spatiale est primordiale puisqu’elles sont largement mobilisées dans d’autres méthodes d’analyses spatiales décrites dans les chapitres suivants, notamment les régressions spatiales (chapitre 7) et les méthodes de classification spatiale (chapitre 8).\nPartie 3. Méthodes de répartition ponctuelle et de détections d’agrégats spatiaux. Cette troisième partie comprend deux chapitres. Dans le chapitre 3, nous abordons les principales méthodes qui permettent de décrire un semis de points dans un espace géographique donné : fréquence et densité des points, analyse centrographique, arrangement spatial du semis de points (méthode du plus proche voisin, méthode des quadrats), cartographie de la densité (notamment l’estimation de la densité par noyau). Dans le chapitre 4, nous abordons deux familles de méthodes de détection d’agrégats spatiaux et spatio-temporels qui s’appliquent à des géométries différentes : les méthodes de classification basées sur la densité des points (couche de points), principalement les algorithmes DBSCAN et ST-DBSCAN, et les méthodes de balayage de Kulldorff (couche de polygones).\nPartie 4. Analyses de données avec des réseaux de transport. Cette quatrième partie comprend aussi deux chapitres. Dans le chapitre 5, nous voyons comment construire un réseau multimode (voiture, marche, vélo, transport en commun) afin de calculer différentes mesures d’accessibilité spatiale dans R avec le package R5R. Dans le chapitre 6, nous décrivons plusieurs méthodes permettant de décrire la distribution spatiale d’évènements localisés sur un réseau de rues avec le package spNetwork.\nPartie 5. Méthodes de régression spatiale et de classification spatiale. Cette cinquième et dernière partie comprend deux chapitres. Le chapitre 7 est consacré à plusieurs méthodes de régression intégrant l’espace : modèles d’économétrie spatiale, modèles généralisés additifs (GAM) avec une spline bivariée sur les coordonnées géographiques, régressions géographiquement pondérées. Dans le chapitre 8, nous décrivons des méthodes permettant de regrouper des entités spatiales d’une région en plusieurs classes en fonction de leurs caractéristiques évaluées à partir de plusieurs variables. Nous distinguons deux types de méthodes de classification non supervisée spatiale : les méthodes de classification non supervisée avec contrainte spatiale visant à regrouper des entités spatiales en plusieurs régions avec une absence de mitage (algorithmes AZP, SKATER, REDCAP, etc.); les méthodes de classification non supervisée avec une dimension spatiale (méthode ClustGeo et classification floue c-moyennes spatiale)."
  },
  {
    "objectID": "index.html#sect006",
    "href": "index.html#sect006",
    "title": "Méthodes d’analyse spatiale : un grand bol d’R",
    "section": "Remerciements",
    "text": "Remerciements\nDe nombreuses personnes ont contribué à l’élaboration de ce manuel.\nCe projet a bénéficié du soutien pédagogique et financier de la fabriqueREL (ressources éducatives libres). Les différentes rencontres avec le comité de suivi nous ont permis de comprendre l’univers des ressources éducatives libres (REL) et notamment leurs fameux 5R (Retenir — Réutiliser — Réviser — Remixer — Redistribuer), de mieux définir le besoin pédagogique visé par ce manuel, d’identifier des ressources pédagogiques et des outils pertinents pour son élaboration. Ainsi, nous remercions chaleureusement les membres de la fabriqueREL pour leur soutien inconditionnel :\n\nMyriam Beaudet, bibliothécaire à l’Université de Sherbrooke.\nMarianne Dubé, coordonnatrice de la fabriqueREL, Université de Sherbrooke.\nSerge Piché, conseiller pédagogique, Université de Sherbrooke.\nClaude Potvin, conseiller en formation, Service de soutien à l’enseignement, Université Laval.\n\nNous remercions chaleureusement les personnes étudiantes du cours GMQ405 - Modélisation et analyse spatiale du Baccalauréat en géomatique appliquée à l’environnement et du Microprogramme de 1er cycle en géomatique appliquée du Département de géomatique appliquée de l’Université de Sherbrooke de la session d’été 2023 : Hermann B. Beaudin, Jérémie Durand, Marie-Hélène Gadbois Del Carpio, David Lapointe, Marc-Antoine Lecours-Toutloff, Rosemarie Légaré, Frédéric Malo, Anthony Mandeville, Gurwen Meret, Sarah Pion, Anne-Sophie Roy, William Scrive, Aldin Evrad Tampe et Kévin Therrien. Vos suggestions et commentaires nous ont permis d’améliorer grandement la version préliminaire de ce manuel qui a été utilisée dans le cadre de ce cours.\nNous remercions aussi les membres du comité de révision pour leurs commentaires et suggestions très constructifs. Ce comité est composé de quatre personnes étudiantes du Département de géomatique appliquée de l’Université de Sherbrooke :\n\nGaël Machemin et Loek Pascaud, étudiants à la maîtrise en géomatique appliquée et télédétection (type recherche).\nRosemarie Légaré, étudiante au baccalauréat en géomatique appliquée à l’environnement.\n\nMarie-Hélène Gadbois Del Carpio, étudiante de troisième année au baccalauréat en géomatique appliquée à l’environnement a participé activement à la mise en page du manuel; elle est désormais une grande spécialiste des feuilles de style en cascade (CSS) et de Quarto.\nAussi, les discussions enrichissantes avec Geneviève Crevier et Amélie Fréchette ont largement contribué à définir et à bonifier la table des matières.\nFinalement, nous remercions Denise Latreille, réviseure linguistique et chargée de cours à l’Université Sherbrooke, pour la révision du manuel."
  },
  {
    "objectID": "00-auteurs.html",
    "href": "00-auteurs.html",
    "title": "À propos des auteurs",
    "section": "",
    "text": "Philippe Apparicio est professeur titulaire au Département de géomatique appliquée de l’Université de Sherbrooke. Il y enseigne aux programmes de 1er et 2e cycles de géomatique les cours Transport et mobilité durable, Modélisation et analyse spatiale et Géomatique appliquée à la gestion urbaine. Durant les dernières années, il a offert plusieurs formations aux Écoles d’été du Centre interuniversitaire québécois de statistiques sociales (CIQSS). Géographe de formation, ses intérêts de recherche incluent la justice et l’équité environnementale, la mobilité durable, les pollutions atmosphérique et sonore, et le vélo en ville. Il a publié une centaine d’articles scientifiques dans différents domaines des études urbaines et de la géographie mobilisant la géomatique et l’analyse spatiale.\nJérémy Gelb a obtenu un doctorat en études urbaines à l’INRS en 2022 (L’exposition des cyclistes aux pollutions atmosphérique et sonore en milieu urbain : comparaison empirique de plusieurs villes à travers le monde), sous la supervision de Philippe Apparicio. Il utilise quotidiennement des systèmes d’information géographique (SIG) et des méthodes d’analyses spatiales. Il est tombé dans la marmite de l’open source avec le triptyque QGIS, R et Python au début de sa maîtrise. Il a développé deux packages : geocmeans et spNetwork, permettant respectivement d’effectuer des analyses de classification floue non supervisée pondérée spatialement et des estimations de densité par kernel sur réseau. Il travaille actuellement comme conseiller en science des données à l’Autorité régionale de transport métropolitain qui gère la planification du transport collectif dans la région de Montréal. Ces travaux portent sur la qualité des milieux urbains, l’accessibilité spatiale, le transport, l’équité environnementale, les SIG et l’analyse spatiale.\nPhilippe et Jérémy travaillent étroitement ensemble depuis plusieurs années. Avec d’autres collègues, ils ont copublié une vingtaine d’articles scientifiques et un manuel intitulé Méthodes quantitatives en sciences sociales : un grand bol d’R, avec le soutien de la fabriqueREL."
  },
  {
    "objectID": "01-ManipulationDonneesSpatiales.html#sec-011",
    "href": "01-ManipulationDonneesSpatiales.html#sec-011",
    "title": "1  Manipulation des données spatiales dans R",
    "section": "\n1.1 Importation de données géographiques",
    "text": "1.1 Importation de données géographiques\n\n\n\n\n\nQuels packages choisir pour importer et manipuler des données spatiales?\n\n\nPour les données vectorielles, il existe deux principaux packages (équivalent d’une librairie dans Python) : sp (Pebesma et Bivand 2005; Bivand, Pebesma et Gomez-Rubio 2013) et sf (Pebesma 2018). Puisque le package sp est progressivement délaissé par R, il est donc fortement conseillé d’utiliser sf.\nPour les données raster, il est possible d’utiliser les packages raster (Hijmans 2022a) et terra (Hijmans 2022b), dont le dernier, plus récent, semblerait plus rapide.\nCette transition de sp à sf et de raster à terra est assez récente et encore en cours durant l’écriture de ce livre. Il existe encore de nombreux packages basés sur sp et raster. Il est donc possible que vous ayez à les utiliser, car leur transition n’est peut-être pas encore effectuée. Notez que la façon dont ces anciens packages intègrent les données vectorielles et matricielles dans R est très différente de celle des nouveaux packages. À titre d’exemple, la fonction sp::readOGR lit un fichier shapefile, tout comme la fonction sf::st_read, mais la première produit un objet de type SpatialDataFrame, alors que la seconde produit un tbl_df. Dans le premier cas, les géométries et les données sont stockées dans deux éléments séparés, alors que dans le second cas, le tbl_df est un data.frame avec une colonne contenant les géométries.\nPour les personnes intéressées aux motivations ayant conduit à cette transition, consultez cet excellent article de blog. Il existe deux raisons principales : le mainteneur des packages rgdal et rgeos servant de fondation à raster et sp a pris sa retraite. À cela s’ajoutent le côté « vieille école » de ces packages (ayant plus de 20 ans!) et l’apparition de packages plus modernes. Il s’agit d’un bon exemple de ce qui peut arriver dans une communauté open source et des évolutions constantes de l’environnement R.\nEn résumé, privilégiez l’utilisation de sf et de terra.\nIl convient d’installer les deux packages. Notez que l’installation d’un package requiert une connexion Internet, car R accède au répertoire de packages CRAN pour télécharger le package et l’installer sur votre ordinateur. Cette opération est réalisée avec la fonction install.packages(\"nom du package\"). Notez qu’une fois que le package est installé, il est enregistré localement sur votre ordinateur et y reste à moins de le désinstaller avec la fonction remove.packages(\"nom du package\").\nAutrement dit, il n’est pas nécessaire de les installer à chaque ouverture de R! Pour utiliser les fonctions d’un package, vous devez préalablement le charger avec la fonction library(\"Nom du package\") (équivalent à la fonction import de Python).\nPour plus d’informations sur l’installation et le chargement de packages, consultez la section suivante (Apparicio et Gelb 2022).\n\n\n\n1.1.1 Importation de données vectorielles\nLa fonction st_read de sf permet d’importer une multitude de formats de données géographiques, comme des fichiers shapefile (shp), GeoPackage (GPKG), GeoJSON (geojson), sqlite (sqlite), geodatabase d’ESRI (FileGDB), Geoconcept (gxt), Keyhole Markup Language (kml), Geography Markup Language (gml), etc.\n\n1.1.1.1 Importation d’un fichier shapefile\n\nLe code R ci-dessous permet d’importer des couches géographiques au format shapefile. Notez que la fonction list.files(pattern = \".shp\") renvoie préalablement la liste des couches shapefile présentes dans le dossier de travail.\n\n## Chargement des packages\nlibrary(\"sf\")\nlibrary(\"terra\")\nlibrary(\"tmap\")\nlibrary(\"ggplot2\")\nlibrary(\"ggpubr\")\nlibrary(\"foreign\")\nlibrary(\"xlsx\")\nlibrary(\"rmapshaper\")\nlibrary(\"RColorBrewer\")\n## Obtention d'une liste des shapefiles dans le dossier de travail\nlist.files(path = \"data/chap01/shp\", pattern = \".shp\")\n\n [1] \"AbidjanPtsGPS.shp\"                         \n [2] \"AbidjanSegRue.shp\"                         \n [3] \"Arrondissements.shp\"                       \n [4] \"IncidentsSecuritePublique.shp\"             \n [5] \"Installations_sportives_et_recreatives.shp\"\n [6] \"Pistes_cyclables.shp\"                      \n [7] \"PolyX.shp\"                                 \n [8] \"PolyY.shp\"                                 \n [9] \"Quebec.shp\"                                \n[10] \"Segments_de_rue.shp\"                       \n\n## Importation des shapefiles avec sf\nArrondissements &lt;- st_read(\"data/chap01/shp/Arrondissements.shp\", quiet=TRUE)\nInstallationSport &lt;- st_read(\"data/chap01/shp/Installations_sportives_et_recreatives.shp\", quiet=TRUE)\nPistesCyclables &lt;- st_read(\"data/chap01/shp/Pistes_cyclables.shp\", quiet=TRUE)\nRues &lt;- st_read(\"data/chap01/shp/Segments_de_rue.shp\", quiet=TRUE)\n\nRegardons à présent la structure des couches importées. Pour ce faire, nous utilisons la fonction head(nom du DataFrame, n=2); notez que le paramètre n permet de spécifier le nombre des premiers enregistrements à afficher. Les informations suivantes sont ainsi disponibles :\n\n6 fields : six champs attributaires (TYPE, DETAIL, NOM, SURFACE, ECLAIRAGE, OBJECTID).\nGeometry type POINT : le type de géométrie est point.\nBounding box:  xmin: -8009681 ymin: 5686891 xmax: -8001939 ymax: 5696536 : les quatre coordonnées définissant l’enveloppe de la couche.\nProjected CRS: WGS 84 / Pseudo-Mercator : la projection cartographique. Ici, une projection cartographique utilisée par Google Maps et OpenStreetMap.\nLa géométrie est enregistrée dans le champ geometry. Pour le premier enregistrement, nous avons la valeur POINT (-8001939 5686891), soit un point avec les coordonnées géographiques (x,y) entre parenthèses.\n\n\nhead(InstallationSport, n=2)   # Visualisation des deux premiers enregistrements\n\nSimple feature collection with 2 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8009681 ymin: 5686891 xmax: -8001939 ymax: 5696536\nProjected CRS: WGS 84 / Pseudo-Mercator\n   TYPE DETAIL                     NOM SURFACE ECLAIRAGE OBJECTID\n1 Aréna   &lt;NA&gt;    Aréna Eugène-Lalonde    &lt;NA&gt;      &lt;NA&gt;        1\n2 Aréna   &lt;NA&gt; Aréna Philippe-Bergeron    &lt;NA&gt;      &lt;NA&gt;        2\n                  geometry\n1 POINT (-8001939 5686891)\n2 POINT (-8009681 5696536)\n\nnames(InstallationSport)       # Noms de champs (colonnes)\n\n[1] \"TYPE\"      \"DETAIL\"    \"NOM\"       \"SURFACE\"   \"ECLAIRAGE\" \"OBJECTID\" \n[7] \"geometry\" \n\nView(InstallationSport)        # Afficher l'ensemble de la table attributaire\n\nExplorons les types de géométries et la projection des autres couches avec le code ci-dessous. En résumé, les types de géométries sont :\n\n\nDes géométries simples\n\npoint : un seul point.\nlinestring : une séquence de deux points et plus formant une ligne.\npolygon : un seul polygone formé par une séquence de points pouvant contenir un ou plusieurs polygones intérieurs formant des trous.\n\n\n\nDes géométries multiples\n\nmultipoint : plusieurs points pour une même observation.\nmultilinestring : plusieurs lignes pour une même observation.\nmultipolygon : plusieurs polygones pour une même observation.\n\n\nUne collection de géométries (Geometrycollection) qui peut contenir différents types de géométries décrites ci-dessus pour une même observation.\n\n\nhead(PistesCyclables, n=2)\n\nSimple feature collection with 2 features and 3 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -8010969 ymin: 5666202 xmax: -7997972 ymax: 5697954\nProjected CRS: WGS 84 / Pseudo-Mercator\n                       NOM OBJECTID SHAPE__Len                       geometry\n1     Axe de la Massawippi        1   13944.09 MULTILINESTRING ((-8010969 ...\n2 Axe de la Saint-François        2   19394.28 MULTILINESTRING ((-8001909 ...\n\nhead(Rues, n=2)\n\nSimple feature collection with 2 features and 16 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -8013896 ymin: 5681299 xmax: -8008810 ymax: 5695980\nProjected CRS: WGS 84 / Pseudo-Mercator\n  ID         TOPONYMIE NOROUTE NUMEROCIVI NUMEROCI_1 NUMEROCI_2 NUMEROCI_3\n1  1 Rue Oliva-Turgeon      NA          0          0          0          0\n2  9      Rue Melville      NA          0          0          0          0\n     NOMGENERIQ TYPERUE TYPESEGMEN VITESSE         TYPESENSUN MUNICIPALI\n1 OLIVA-TURGEON     Rue     Locale      50 Pas de sens unique      43027\n2      MELVILLE     Rue     Locale      50 Pas de sens unique      43027\n  OBJECTID SHAPE__Len              CIRCULATIO                       geometry\n1        1   114.7781 Interdite en tout temps MULTILINESTRING ((-8008810 ...\n2        2   114.4441 Interdite en tout temps MULTILINESTRING ((-8013782 ...\n\nhead(Arrondissements, n=2)\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -8027109 ymin: 5668860 xmax: -8000502 ymax: 5704391\nProjected CRS: WGS 84 / Pseudo-Mercator\n  NUMERO                                                         NOM\n1      1 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville\n2      4                                  Arrondissement des Nations\n                        geometry\n1 POLYGON ((-8005013 5702777,...\n2 POLYGON ((-8005680 5690860,...\n\n\nVisualisons quelques couches importées avec ggplot().\n\n## Arrondissements et rues\nggplot()+ geom_sf(data = Arrondissements, lwd = .8)+\n  geom_sf(data = Rues, aes(colour = TYPESEGMEN))\n\n\n\n## Arrondissements, pistes cyclables et installations sportives\nggplot()+ geom_sf(data = Arrondissements, lwd = .8)+\n  geom_sf(data = PistesCyclables, aes(colour = NOM), lwd = .5)+\n  geom_sf(data = InstallationSport)\n\n\n\n\n\n1.1.1.2 Importation d’une couche dans un GeoPackage\n\nPour importer une couche stockée dans un GeoPackage (GPKG), vous devez spécifier le fichier et la couche avec respectivement les paramètres dsn et layer de la fonction st_read. Le code ci-dessous permet d’importer les secteurs de recensement de la région métropolitaine de recensement de Sherbrooke pour l’année 2021. Notez que la fonction st_layers(dsn) permet d’obtenir la liste des couches contenues dans le fichier GPKG, avec le type de géométrie, les nombre d’entités spatiales et de champs, et la projection cartographique pour chacune d’elles.\n\n## Nom du fichier GPKG\nfichierGPKG &lt;- \"data/chap01/gpkg/Recen2021Sherbrooke.gpkg\"\n## Liste des couches dans le GPKG\nst_layers(dsn=fichierGPKG, do_count = TRUE)\n\nDriver: GPKG \nAvailable layers:\n           layer_name geometry_type features fields\n1            SherbRMR Multi Polygon        1     12\n2            SherbSDR Multi Polygon       11     12\n3             SherbSR Multi Polygon       50     10\n4             SherbAD Multi Polygon      333     10\n5             SherbIL Multi Polygon     2734     12\n6        RegionEstrie Multi Polygon        1      5\n7          sdr_Estrie Multi Polygon       89      6\n8        DREstrie2021 Multi Polygon        7      6\n9 DivisionsRecens2021 Multi Polygon       98      6\n                           crs_name\n1 NAD83 / Statistics Canada Lambert\n2 NAD83 / Statistics Canada Lambert\n3 NAD83 / Statistics Canada Lambert\n4 NAD83 / Statistics Canada Lambert\n5 NAD83 / Statistics Canada Lambert\n6 NAD83 / Statistics Canada Lambert\n7 NAD83 / Statistics Canada Lambert\n8 NAD83 / Statistics Canada Lambert\n9 NAD83 / Statistics Canada Lambert\n\n## Importation d'une couche\nSR.RMRSherb &lt;- st_read(dsn = fichierGPKG, \n                       layer = \"SherbSR\", quiet=TRUE)\n## Affichage des deux premiers enregistrements\nhead(SR.RMRSherb, n=2)\n\nSimple feature collection with 2 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 7762066 ymin: 1271201 xmax: 7765357 ymax: 1274082\nProjected CRS: NAD83 / Statistics Canada Lambert\n                IDUGD      SRIDU   SRNOM SUPTERRE PRIDU SRpop_2021 SRtlog_2021\n1 2021S05074330001.00 4330001.00 0001.00   3.1882    24       5637        2918\n2 2021S05074330002.00 4330002.00 0002.00   0.8727    24       1868        1169\n  SRrhlog_2021 RMRcode   HabKm2                           geom\n1         2756     433 1768.082 MULTIPOLYGON (((7764998 127...\n2         1063     433 2140.484 MULTIPOLYGON (((7763361 127...\n\n## Visualisation rapide des secteurs avec ggplot\nggplot()+ geom_sf(data = SR.RMRSherb, lwd = .5)\n\n\n\n\n\n1.1.1.3 Importation d’une couche dans une geodatabase d’ESRI\nLa logique est la même qu’avec un GeoPackage, nous spécifions le chemin de la geodatabase et la couche avec les paramètres dsn et layer.\n\nAffectDuTerritoire &lt;- st_read(dsn = \"data/chap01/geodatabase/Sherbrooke.gdb\", \n                              layer = \"AffectationsDuTerritoire\", quiet=TRUE)\n## Visualisation des affectations du sol avec ggplot\nggplot()+ geom_sf(data = AffectDuTerritoire, aes(fill = TYPE), lwd = .2)\n\n\n\n\n\n1.1.1.4 Importation de données GPS\nEn géomatique appliquée, il est fréquent de collecter des données sur le terrain avec un appareil GPS. Les données ainsi collectées peuvent être enregistrées dans différents formats de données dépendamment de l’appareil GPS utilisé : GPS eXchange Format (GPX), Garmin’s Flexible and Interoperable Data Transfer (FIT), Training Center XML (TCX), etc.\n\n1.1.1.4.1 Importation de coordonnées GPS longitude/latitude au format csv\n\nUne personne ayant collecté des données sur le terrain pourrait aussi vous les transmettre dans un fichier csv (fichier texte délimité par des virgules). Il convient d’importer le fichier de coordonnées GPS dans R dans un DataFrame (avec la fonction read.csv). Une fois importé, nous constatons qu’il comprend trois champs :\n\nid : un champ identifiant avec des valeurs uniques.\nlon : longitude.\nlat : latitude.\n\nLes points sont projetés en longitude/latitude (WGS84 long/lat, EPSG : 4326).\n\n## Importation du fichier csv\nPointsGPS &lt;- read.csv(\"data/chap01/gps/pointsGPS.csv\")\nhead(PointsGPS)\n\n  id       lon      lat\n1  1 -71.99985 45.36010\n2  2 -71.99096 45.37535\n3  3 -71.98444 45.46964\n4  4 -72.09873 45.37126\n5  5 -72.04880 45.41035\n6  6 -71.95000 45.32570\n\n\nPour convertir le DataFrame en un objet sf, nous utilisons la fonction st_as_sf en spécifiant les champs pour les coordonnées et la projection cartographique.\n\n## Importation du fichier csv\nPointsGPS &lt;- st_as_sf(PointsGPS, coords = c(\"lon\",\"lat\"), crs = 4326)\n\nLes points ainsi créés sont localisés dans la ville de Sherbrooke.\n\n## Affichage des points avec le package tmap\ntmap_mode(\"view\") ## Mode actif de tmap\ntm_shape(PointsGPS)+\n  tm_dots(size = 0.05, shape = 21, col = \"red\")\n\n\n\n\n\n\n\n1.1.1.4.2 Importation de coordonnées GPS au format GPX\n\nLe format GPX est certainement le format de stockage et d’échange de coordonnées GPS le plus utilisé. Les informations géographiques (x,y) et temporelles (date et heure) sont respectivement enregistrées en degrés longitude/latitude (projection WSG) (WGS84, EPSG : 4326) et en temps universel coordonné (UTC, format ISO 8601).\nPour importer un fichier GPX, nous utilisons le package gpx. S’il n’est pas installé sur votre ordinateur, lancez la commande install.packages(\"gpx\") dans la console de R; n’oubliez pas de le charger avec library(\"gpx\")! Ensuite, importez le fichier GPX avec la fonction read_gpx, enregistrez la trace GPS dans un DataFrame et convertissez-la en objet sf.\n\nlibrary(\"gpx\")\n## Importation du fichier GPX\nTraceGPS &lt;- read_gpx(\"data/chap01/gps/TraceGPS.gpx\")\n## Cette trace GPS comprend trois listes : routes, tracks et waypoints\n## Les points sont stockés dans tracks\nnames(TraceGPS)\n\n[1] \"routes\"    \"tracks\"    \"waypoints\"\n\n## Pour visualiser les données, il suffit de lancer la ligne\n## ci-dessous (mise en commentaire car le résultat est un peu long...)\n# head(TraceGPS)\nTraceGPS &lt;- TraceGPS$tracks$`ID1_PA_2021-12-03_TRAJET01.gpx`\n## Conversion du DataFrame en objet sf\nTraceGPS &lt;- st_as_sf(TraceGPS, coords = c(\"Longitude\",\"Latitude\"), crs = 4326)\n## Visualisation des premiers enregistrements\nhead(TraceGPS, n=2)\n\nSimple feature collection with 2 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -4.022827 ymin: 5.327383 xmax: -4.022825 ymax: 5.327387\nGeodetic CRS:  WGS 84\n  Elevation                Time extensions Segment ID\n1      40.8 2021-12-03 08:38:49         NA          1\n2      40.6 2021-12-03 08:38:50         NA          1\n                    geometry\n1 POINT (-4.022827 5.327387)\n2 POINT (-4.022825 5.327383)\n\n\nLa trace GPS correspond à un trajet réalisé à vélo à Abidjan (Côte d’Ivoire) le 3 décembre 2021. Cette trace a été obtenue avec une montre Garmin et comprend un point chaque seconde.\n\ntmap_mode(\"view\")\ntm_basemap(leaflet::providers$OpenStreetMap)+\ntm_shape(TraceGPS)+\n  tm_dots(size = 0.001, shape = 21, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nLa structure de la classe sf\n\n\nLa classe sf est composée de trois éléments (figure 1.1) :\n\nL’objet simple feature geometry (sfg) est la géométrie d’une observation. Tel que vu plus haut, elle est une géométrie simple (point, linestring, polygon), multiple (multipoint, multilinestring, multipolygon) ou une collection de géométries différentes (Geometrycollection). Pour définir chacune de ces géométries, nous utilisons les méthodes st_point(), st_linestring(), st_polygon(), st_multipoint(), st_multilinestring(), st_multipolygon() et Geometrycollection().\nL’objet simple feature column (sfc) est simplement une liste de simple feature geometry (sfg). Elle représente la colonne geometry d’une couche vectorielle sf.\nL’objet data.frame correspond à la table attributaire.\n\nUne simple feature correspond ainsi à une observation (ligne) d’un objet sf, soit une entité spatiale comprenant l’information sémantique (attributs) et l’information spatiale (géométrie).\n\n\nFigure 1.1: Structure de la classe sf\n\nVoyons un exemple concret : créons une couche sf comprenant les trois entités spatiales décrites dans la figure 1.1.\n\n## Création des géométries : simple feature geometry (sfg)\npoint1 = st_point(c(-8001939, 5686891))\npoint2 = st_point(c(-8009681, 5696536))\npoint3 = st_point(c(-7998695, 5689743))\n## Création d'une liste de géométries : simple feature geometry (sfc)\n## avec la projection cartographique EPSG 3857\npoints_sfc = st_sfc(point1, point2, point3, crs = 3857)\n## Création de la table attributaire : objet data.frame\ntable_attr = data.frame(TYPE = c(\"Aréna\", \"Aréna\",\"Aréna\"),\n                       NOM = c(\"Aréna Eugène-Lalonde\", \n                               \"Aréna Philippe-Bergeron\",\n                               \"Centre Julien-Ducharme\"),\n                       OBJECTID = c(1, 2, 3))\n## Création de l'objet sf\nArena_sf = st_sf(table_attr, geometry = points_sfc)\n## Le résultat est bien identique à celui de la figure ci-dessus\nhead(Arena_sf)\n\nSimple feature collection with 3 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8009681 ymin: 5686891 xmax: -7998695 ymax: 5696536\nProjected CRS: WGS 84 / Pseudo-Mercator\n   TYPE                     NOM OBJECTID                 geometry\n1 Aréna    Aréna Eugène-Lalonde        1 POINT (-8001939 5686891)\n2 Aréna Aréna Philippe-Bergeron        2 POINT (-8009681 5696536)\n3 Aréna  Centre Julien-Ducharme        3 POINT (-7998695 5689743)\n\n\n\n\n\n1.1.2 Importation de données raster\n\nLa fonction terra::rast permet d’importer des images de différents formats (GeoTiff, ESRI, ENVI, ERDAS, BIN, GRID, etc.). Nous importons ci-dessous quatre feuillets de modèles numériques d’altitude (MNA) à l’échelle du 1/20000 couvrant la ville de Sherbrooke. La figure 1.2 présente l’un d’entre eux.\n\n## Liste des fichiers GeoTIFF dans le dossier\nlist.files(path=\"data/chap01/raster\", pattern = \".tif\")\n\n [1] \"f21e05_101.tif\"         \"f21e05_101.tif.aux.xml\" \"f21e05_201.tif\"        \n [4] \"f21e05_201.tif.aux.xml\" \"f21e12_101.tif\"         \"f21e12_101.tif.aux.xml\"\n [7] \"f31h08_102.tif\"         \"f31h08_102.tif.aux.xml\" \"f31h08_202.tif\"        \n[10] \"f31h08_202.tif.aux.xml\"\n\n## Importation des fichiers\nf21e05_101 &lt;- terra::rast(\"data/chap01/raster/f21e05_101.tif\")\nf21e05_201 &lt;- terra::rast(\"data/chap01/raster/f21e05_201.tif\")\nf31h08_102 &lt;- terra::rast(\"data/chap01/raster/f31h08_102.tif\")\nf31h08_202 &lt;- terra::rast(\"data/chap01/raster/f31h08_202.tif\")\nf21e12_101 &lt;- terra::rast(\"data/chap01/raster/f21e12_101.tif\")\n## Visualisation des informations sur l'image f21e05_101\nf21e05_101\n\nclass       : SpatRaster \ndimensions  : 1409, 2798, 1  (nrow, ncol, nlyr)\nresolution  : 9e-05, 9e-05  (x, y)\nextent      : -72.00095, -71.74913, 45.24907, 45.37588  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource      : f21e05_101.tif \nname        : f21e05_101 \nmin value   :   143.4273 \nmax value   :   423.5806 \n\n# Visualisation de l'image\nterra::plot(f21e05_101, \n     main=\"Modèle numérique d'altitude à l’échelle de 1/20 000 (f21e05_101)\")\n\n\n\nFigure 1.2: Modèle numérique d’élévation au 1/20000 (feuillet f21e05_101)"
  },
  {
    "objectID": "01-ManipulationDonneesSpatiales.html#sec-012",
    "href": "01-ManipulationDonneesSpatiales.html#sec-012",
    "title": "1  Manipulation des données spatiales dans R",
    "section": "\n1.2 Manipulation de données vectorielles",
    "text": "1.2 Manipulation de données vectorielles\n\n\n\n\n\nPackage sf et opérations géométriques\n\n\nLe package sf est une librairie extrêmement complète permettant de réaliser une multitude d’opérations géométriques sur des couches vectorielles comme dans un système d’information géographique (SIG). Notre objectif n’est pas de toutes les décrire, mais d’aborder les principales. Au fil de vos projets avec sf, vous apprendrez d’autres fonctions. Pour ce faire, n’hésitez pas à consulter :\n\nUne belle Cheatsheet sur sf. Allez y jeter un œil, cela vaut la peine!\nSur le site CRAN de sf, vous trouverez plusieurs vignettes explicatives (exemples de code documentés).\nLa documentation complète en PDF.\n\nLa syntaxe methods(class = 'sfc') renvoie la liste des méthodes implémentées dans le package sf. Pour accéder à l’aide en ligne de l’une d’entre elles, écrivez simplement ?Nom de la fonction (ex. : ?st_buffer).\n\nmethods(class = 'sfc')\n\n [1] [                            [&lt;-                         \n [3] as.data.frame                c                           \n [5] coerce                       format                      \n [7] fortify                      identify                    \n [9] initialize                   ms_clip                     \n[11] ms_dissolve                  ms_erase                    \n[13] ms_explode                   ms_filter_islands           \n[15] ms_innerlines                ms_lines                    \n[17] ms_points                    ms_simplify                 \n[19] Ops                          print                       \n[21] rep                          scale_type                  \n[23] show                         slotsFromS3                 \n[25] st_area                      st_as_binary                \n[27] st_as_grob                   st_as_s2                    \n[29] st_as_sf                     st_as_text                  \n[31] st_bbox                      st_boundary                 \n[33] st_break_antimeridian        st_buffer                   \n[35] st_cast                      st_centroid                 \n[37] st_collection_extract        st_concave_hull             \n[39] st_convex_hull               st_coordinates              \n[41] st_crop                      st_crs                      \n[43] st_crs&lt;-                     st_difference               \n[45] st_geometry                  st_inscribed_circle         \n[47] st_intersection              st_intersects               \n[49] st_is                        st_is_valid                 \n[51] st_line_merge                st_m_range                  \n[53] st_make_valid                st_minimum_rotated_rectangle\n[55] st_nearest_points            st_node                     \n[57] st_normalize                 st_point_on_surface         \n[59] st_polygonize                st_precision                \n[61] st_reverse                   st_sample                   \n[63] st_segmentize                st_set_precision            \n[65] st_shift_longitude           st_simplify                 \n[67] st_snap                      st_sym_difference           \n[69] st_transform                 st_triangulate              \n[71] st_triangulate_constrained   st_union                    \n[73] st_voronoi                   st_wrap_dateline            \n[75] st_write                     st_z_range                  \n[77] st_zm                        str                         \n[79] summary                      vec_cast.sfc                \n[81] vec_ptype2.sfc               vect                        \nsee '?methods' for accessing help and source code\n\n\n\n\n\n1.2.1 Fonctions relatives à la projection cartographique\nLes trois principales fonctions relatives à la projection cartographique des couches vectorielles sont :\n\nst_crs(x) pour connaître la projection géographique d’un objet sf.\nst_transform(x, cr) pour modifier la projection cartographique.\nst_is_longlat(x) pour vérifier si les coordonnées sont en degrés longitude/latitude.\n\n\n## Importation d'un shapefile pour la province de Québec\nProvinceQc &lt;- st_read(\"data/chap01/shp/Quebec.shp\", quiet = TRUE)\n## La projection est EPSG:3347 - NAD83 / Statistics Canada Lambert,\n## soit la projection conique conforme de Lambert\nst_crs(ProvinceQc)\n\nCoordinate Reference System:\n  User input: NAD83 / Statistics Canada Lambert \n  wkt:\nPROJCRS[\"NAD83 / Statistics Canada Lambert\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Statistics Canada Lambert\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",63.390675,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-91.8666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",77,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",6200000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",3000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Topographic mapping (small scale).\"],\n        AREA[\"Canada - onshore and offshore - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon.\"],\n        BBOX[38.21,-141.01,86.46,-40.73]],\n    ID[\"EPSG\",3347]]\n\n## Reprojection de la couche en WGS84 long/lat (EPSG:4326)\nProvinceQc.4326 &lt;- st_transform(ProvinceQc, crs = 4326)\n## longitude/latitude?\nst_is_longlat(ProvinceQc)\n\n[1] FALSE\n\nst_is_longlat(ProvinceQc.4326)\n\n[1] TRUE\n\n\nLa figure 1.3 démontre bien que les deux couches sont projetées différemment.\n\nMap1 &lt;- ggplot()+geom_sf(data = ProvinceQc)+coord_sf(crs = st_crs(ProvinceQc))+\n        labs(subtitle = \"Conique conforme de Lambert (EPSG : 3347)\")\n\nMap2 &lt;- ggplot()+geom_sf(data = ProvinceQc.4326)+coord_sf(crs = st_crs(ProvinceQc.4326))+\n        labs(subtitle = \"WGS84 long/lat (EPSG : 4326)\")\n\ncomp_plot &lt;- ggarrange(Map1, Map2, ncol = 2, nrow = 1)\nannotate_figure(comp_plot,\n                top = text_grob(\"Province de Québec\",\n                                face = \"bold\", size = 12, just = \"center\")\n                )\n\n\n\nFigure 1.3: Deux projections cartographiques\n\n\n\n\n1.2.2 Fonctions d’opérations géométriques sur une couche\nIl existe une quinzaine de fonctions d’opérations géométriques sur une couche dans le package sf dont le résultat renvoie de nouvelles géométries (voir la documentation suivante). Nous décrivons ici uniquement celles qui nous semblent les plus utilisées :\n\nst_bbox(x) renvoie les coordonnées minimales et maximales des géométries d’un objet sf. Pour créer l’enveloppe d’un objet sf, il suffit donc d’écrire st_as_sfc(st_bbox(x)).\nst_boundary(x) renvoie les limites (contours) des géométries d’un objet sf.\nst_convex_hull(x) crée l’enveloppe convexe des géométries d’un objet sf.\nst_combine(x) regroupe les géométries d’un objet sf en une seule géométrie, sans les réunir ni résoudre les limites internes.\nst_union(x) fusionne les géométries d’un objet sf en une seule géométrie.\nst_buffer(x, dist, endCapStyle = c(\"ROUND\", \"FLAT\", \"SQUARE\"), joinStyle = c(\"ROUND\", \"MITRE\", \"BEVEL\")) crée des zones tampons d’une distance définie avec le paramètre dist. Cette fonction s’applique à des points, à des lignes et à des polygones.\nst_centroid(x) crée des points au centre de chaque géométrie d’un objet sf. Elle s’applique donc à des lignes et à des polygones.\nst_point_on_surface(x) crée un point au centre de chaque polygone d’un objet sf .\nst_simplify(x, dTolerance) simplifie les contours de géométries (lignes ou polygones) avec une tolérance exprimée en mètres (paramètre dTolerance) d’un objet sf .\nst_voronoi(x, bOnlyEdges = TRUE) crée des polygones de Thiessen, appelés aussi polygones de Voronoï pour des points. Attention, le paramètre bOnlyEdges = TRUE renvoie des lignes tandis que bOnlyEdges = FALSE renvoie des polygones.\n\n\n1.2.2.1 Enveloppe et union d’une couche\nLe code ci-dessous crée une enveloppe (en bleu) et un polygone fusionné (en rouge) pour les arrondissements de la ville de Sherbrooke (figure 1.4). La couche résultante de l’opération st_as_sfc(st_bbox(x)) est ainsi l’équivalent des outils Emprise de QGIS et Minimum Bounding Geometry (Geometry Type = Envelope) d’ArcGIS Pro.\n\n## Enveloppe sur les arrondissements de la ville de Sherbrooke\nArrond.Enveloppe &lt;- st_as_sfc(st_bbox(Arrondissements))\n## Fusionne les géométries en une seule en résolvant les limites internes\nArrond.Union &lt;- st_union(Arrondissements)\n\n\ntmap_mode(\"plot\")\ntm_shape(Arrond.Enveloppe) + tm_borders(col = \"blue\", lwd=2)+\n  tm_shape(Arrond.Union) + tm_borders(col = \"red\", lwd=2)+\n  tm_layout(frame = FALSE)+\n  tm_scale_bar(c(0,5,10))\n\n\n\nFigure 1.4: Enveloppe sur une couche\n\n\n\n\n1.2.2.2 Enveloppe orientée\nLa fonction st_bbox de sf produit des rectangles englobant des géométries qui sont orientées nord-sud. Il est possible de générer des rectangles orientés autour de géométries pour minimiser leur emprise et ainsi mieux représenter l’orientation de la géométrie initiale. Il n’existe pas de fonction dans sf pour le faire, mais le package foot offre une implémentation facile d’utilisation. Notez que foot n’est pas déposé sur CRAN et doit être téléchargé depuis Github avec la ligne de code ci-dessous.\n\ndevtools::install_github(\"wpgp/foot\", build_vignettes = FALSE)\n\nLa couche résultante de l’opération fs_mbr(x, returnShape = TRUE) (figure 1.5, b) est ainsi l’équivalent des outils Emprise orientée minimale (OMBB) de QGIS et Minimum Bounding Geometry (Geometry Type = Rectangle by area) d’ArcGIS Pro.\n\nlibrary(foot)\n## Rectangles (enveloppes) orientés \nrectangles_oriented &lt;- fs_mbr(Arrondissements, returnShape = TRUE)\nrectangles_oriented &lt;- st_as_sf(rectangles_oriented,\n                                crs = st_crs(Arrondissements))\nrectangles_oriented$NOM &lt;- Arrondissements$NOM\n## Rectangles non orientés (nord-sud)\nst_bbox_by_feature = function(x) {\n  x = st_geometry(x)\n  f &lt;- function(y) st_as_sfc(st_bbox(y))\n  do.call(\"c\", lapply(x, f))\n}\nrectangles &lt;- st_as_sf(st_bbox_by_feature(Arrondissements),\n                       crs = st_crs(Arrondissements))\nrectangles$NOM &lt;- Arrondissements$NOM\n\n\n\n\n\nFigure 1.5: Enveloppes classiques et orientées\n\n\n\n\n1.2.2.3 Centroïdes et centre de surface\nLe code ci-dessous extrait les centres géométriques, c’est-à-dire les centroïdes (en bleu) et les points à l’intérieur des polygones (en rouge) pour les arrondissements de la ville de Sherbrooke (figure 1.6). Ces deux opérations correspondent aux outils centroïdes et Point dans la surface de QGIS et Feature to Point (avec l'option Inside) d’ArcGIS Pro.\n\n## Centroïdes et points dans les polygones sur les arrondissements\nArrond.centroide &lt;- st_centroid(Arrondissements)\nArrond.pointpoly &lt;- st_point_on_surface(Arrondissements)\n\n\n\n\n\nFigure 1.6: Centroïdes et points à l’intérieur des polygones\n\n\n\n\n1.2.2.4 Zone tampon (buffer)\nUne simple ligne de code permet de créer des zones tampons (équivalent des outils Analyse vectorielle/Tampon dans QGIS et Buffer dans ArcGIS Pro). Une fois les zones créées, utilisez la fonction st_union pour fusionner les tampons en un polygone (figure 1.7).\n\n## Zones tampons de 1000 mètres autour des installations sportives et récréatives\nInstSports.buffer &lt;- st_buffer(InstallationSport, dist = 1000)\n## Si vous souhaitez fusionner les zones tampons, utilisez la fonction st_union\nInstSports.bufferUnion &lt;- st_union(InstSports.buffer)\n## Zones tampons de 500 mètres autour des lignes\nPistesCyclables.buffer &lt;- st_buffer(PistesCyclables, dist = 500)\nPistesCyclables.bufferUnion &lt;- st_union(PistesCyclables.buffer)\n\n\n\n\n\nFigure 1.7: Zones tampons\n\n\n\nNotez que pour des polygones, il est possible de créer des polygones intérieurs comme suit : st_buffer(x, dist = - Valeur). Par exemple, le code ci-dessous crée des polygones de 200 mètres autour et à l’intérieur du parc du Mont-Bellevue de la ville de Sherbrooke (figure 1.8).\n\n## Importation de la couche des aires aménagées de la ville de Sherbrooke\nAiresAmenag &lt;- st_read(dsn = \"data/chap01/geodatabase/Sherbrooke.gdb\",\n                       layer = \"AiresAmenagees\", quiet = TRUE)\n## Sélection du parc du Mont-Bellevue\nMontBellevue &lt;- subset(AiresAmenag, NOM == \"Parc du Mont-Bellevue\")\n## Création d'une zone tampon autour du parc\nMontBellevue.ZTA500 &lt;- st_buffer(MontBellevue, dist = 200)\n## Création d'une zone tampon à l'intérieur du parc\nMontBellevue.ZTI500 &lt;- st_buffer(MontBellevue, dist = -200)\n\n\n\n\n\nFigure 1.8: Zone tampon intérieure et zone tampon extérieure\n\n\n\n\n1.2.2.5 Simplification de géométries\nLa simplification ou généralisation d’une couche de lignes ou de polygones permet de supprimer des sommets tout en gardant le même nombre de géométries dans la couche résultante. Cette opération peut être réalisée dans QGIS avec l’outil simplifier et dans ArcGIS Pro avec l’outil Generalize. Deux raisons principales peuvent motiver le recours à cette opération :\n\nLa réduction de la taille du fichier, surtout si la couche est utilisée pour de la cartographie interactive sur Internet avec des formats vectoriels comme le SVG (Scalable Vector Graphics), le KML ou le GeoJSON.\nL’utilisation de la couche à une plus petite échelle cartographique nécessitant la suppression de détails.\n\nLe code suivant permet de simplifier les contours des arrondissements de la ville de Sherbrooke avec des tolérances de 250, 500, 1000 et 2000 mètres. Plus la valeur de la tolérance est élevée, plus les contours sont simplifiés (figure 1.9). Notez que l’algorithme de Douglas-Peucker (Douglas et Peucker 1973) a été implémenté dans la fonction st_simplify. Bien qu’intéressant, cet algorithme ne conserve pas les frontières entre les polygones.\n\n## Simplification des contours avec différentes distances de tolérance\nArrond.simplify250m &lt;- st_simplify(Arrondissements, \n                                   preserveTopology = TRUE, \n                                   dTolerance = 250)\nArrond.simplify500m &lt;- st_simplify(Arrondissements, \n                                   preserveTopology = TRUE, \n                                   dTolerance = 500)\nArrond.simplify1000m &lt;- st_simplify(Arrondissements, \n                                    preserveTopology = TRUE, \n                                    dTolerance = 1000)\nArrond.simplify2000m &lt;- st_simplify(Arrondissements, \n                                    preserveTopology = TRUE, \n                                    dTolerance = 2000)\n\n\n\n\n\nFigure 1.9: Simplification des contours de géométries\n\n\n\nPour remédier au problème des frontières non conservées, utilisez l’algorithme de Visvalingam et Whyatt (1993) avec la fonction ms_simplify du package rmapshaper (figure 1.10), tel qu’illustré dans le code ci-dessous. À titre de rappel, pour l’installer et le charger sur votre ordinateur, tapez dans la console : install.packages(\"rmapshaper\") et library(\"rmapshaper\"). Le paramètre keep permet de définir la proportion de points à retenir : plus sa valeur est faible, plus la simplification est importante.\n\n\n\n\nFigure 1.10: Simplification des contours avec l’algorithme de Visvalingam–Whyatt\n\n\n\n\n1.2.2.6 Enveloppe convexe (convex hull)\nLe code ci-dessous permet de créer l’enveloppe convexe pour des points (figure 1.11). Notez que cette fonction peut également s’appliquer à des lignes et à des polygones. Elle correspond aux outils Enveloppe convexe de QGIS et Feature to Point (avec l'option Convex hull) d’ArcGIS Pro.\n\n## Enveloppe convexe autour des points GPS\nPointsGPS.Convexhull &lt;- st_convex_hull(st_union(PointsGPS))\n\n\n\n\n\nFigure 1.11: Enveloppe convexe autour de points\n\n\n\n\n1.2.2.7 Enveloppe concave (concave hull)\nUne extension possible du polygone convexe est le polygone concave qui a une superficie plus réduite. Il n’existe pas de fonction dans sf qui l’implémente. Il faut donc installer un package supplémentaire, soit concaveman.\n\nlibrary(concaveman)\n## Convex hull autour des points GPS\nPointsGPS.Concavhull &lt;- concaveman(PointsGPS)\n\n\n\n\n\nFigure 1.12: Enveloppe concave autour de points\n\n\n\nNotez que comparativement au polygone convexe (figure 1.12), le polygone concave peut auvoir plus d’une solution possible (lire l’article suivant). Plus spécifiquement, il faut choisir son degré de concavité. Dans la fonction concaveman::concaveman, le paramètre concavity prend une valeur numérique, qui, si elle tend vers l’infini, produit un polygone convexe (figure 1.13).\n\nlibrary(ggpubr)\n# test avec plusieurs valeur de concavité\nconcav_values &lt;- c(1,1.5,3,8)\nplots &lt;- lapply(concav_values, function(i){\n  Concavhull &lt;- concaveman(PointsGPS, concavity = i)\n  this_plot &lt;- ggplot()+\n    geom_sf(data = Concavhull)+\n    geom_sf(data = PointsGPS)+\n    labs(subtitle = paste0(\"Concavité : \",i))+\n      theme(axis.text.x = element_blank(),\n            axis.text.y = element_blank(),\n            axis.ticks = element_blank())\n  return(this_plot)\n})\nggarrange(plotlist = plots)\n\n\n\nFigure 1.13: Enveloppe concave autour de points\n\n\n\nDans QGIS, il existe plusieurs plugins permettant de générer des enveloppes concaves, ainsi qu’une fonction installée de base avec GRASS (v.concave.hull).\n\n1.2.3 Fonctions d’opérations géométriques entre deux couches\nLes opérations entre deux couches sont bien connues et largement utilisées dans les SIG. Bien entendu, plusieurs fonctions de ce type sont disponibles dans sf et renvoient une nouvelle couche géographique sf :\n\nst_intersection(x, y) génère l’intersection entre les géométries de deux couches. À ne pas confondre avec la fonction st_intersects(x, y) qui permet de construire une requête spatiale.\nst_union(x, y) génère l’union entre les géométries de deux couches.\nst_difference(x, y) crée une géométrie à partir de x qui n’est pas en intersection avec y.\nst_sym_difference(x, y) crée une géométrie représentant les portions des géométries x et y qui ne s’intersectent pas.\nst_crop(x, y, xmin, ymin, xmax, ymax) extrait les géométries de x comprises dans un rectangle.\n\nEn guise de comparaison, toutes ces fonctions sont disponibles dans la boîte à outils de traitement de QGIS (dans le groupe recouvrement de vecteur) et les outils de la catégorie Overlay du Geoprocessing d’ArcGIS Pro. Le code ci-dessous illustre comment réaliser des intersections et des unions entre deux couches polygonales.\n\n## Importation des deux couches\npolysX &lt;- st_read(\"data/chap01/shp/PolyX.shp\", quiet = TRUE)\npolysY &lt;- st_read(\"data/chap01/shp/PolyY.shp\", quiet = TRUE)\n## Intersection des deux couches\n## Les géométries récupèrent les attributs des deux couches\nInter.XY &lt;- st_intersection(polysX, polysY)\nhead(Inter.XY)\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -8006904 ymin: 5684822 xmax: -8006602 ymax: 5685184\nProjected CRS: WGS 84 / Pseudo-Mercator\n  X_id Y_id                       geometry\n1  X.a  Y.d POLYGON ((-8006753 5684838,...\n2  Y.b  Y.d POLYGON ((-8006788 5684908,...\n\n## Intersection entre deux couches préalablement fusionnées : \n## Le résutat est une seule géométrie\nInter.XYUnion &lt;- st_intersection(st_union(polysX), st_union(polysY))\n## Union des deux couches\nUnion.XY &lt;- st_union(st_union(polysX), st_union(polysY))\n\n\n\n\n\n\nLa fonction st_intersection peut aussi être utilisée comme la méthode clip dans un SIG (ArcGIS Pro ou QGIS). En guise d’exemple, dans le code ci-dessous, nous extrayons les points GPS localisés sur le territoire de la ville de Sherbrooke (figure 1.14).\n\n# Nous nous assurons que les deux couches ont la même projection\nPointsGPS &lt;- st_transform(PointsGPS, st_crs(Arrond.Union))\n# Extraction des points\nPointsGPS.Sherb &lt;- st_intersection(PointsGPS, Arrond.Union)\n# Visualisation avant et après\nCarte1 &lt;- ggplot()+geom_sf(data = Arrond.Union)+geom_sf(data = PointsGPS)+\n          labs(subtitle = \"Avant l'intersection\")+\n          theme(axis.text.x = element_blank(),\n                axis.text.y = element_blank(),\n                axis.ticks = element_blank())\nCarte2 &lt;- ggplot()+geom_sf(data = Arrond.Union)+geom_sf(data = PointsGPS.Sherb)+\n          labs(subtitle = \"Après l'intersection\")+\n          theme(axis.text.x = element_blank(),\n                axis.text.y = element_blank(),\n                axis.ticks = element_blank())\nggarrange(Carte1, Carte2, ncol = 2, nrow = 1)\n\n\n\nFigure 1.14: Fonction st_intersection() équivalente à la méthode clip dans un SIG\n\n\n\nQuelques lignes de code suffisent pour générer les différences de superposition entre les géométries de couches géographiques (figure 1.15).\n\n## Différences entre deux couches\nDiff.XY &lt;- st_difference(st_union(polysX), st_union(polysY))\nDiff.YX &lt;- st_difference(st_union(polysY), st_union(polysX))\nDiff.symXY &lt;- st_sym_difference(st_union(polysY), st_union(polysX))\n\n\ntmap_mode(\"plot\")\nMapInterU &lt;- tm_shape(Zone)+tm_fill(col = NA, alpha = 0, border.col = NA, lwd = 0)+\ntm_shape(Inter.XYUnion)+tm_polygons(col = \"tomato4\", alpha = .5, \n                            border.col =\"black\", lwd = 1)\n\nMapDiffXY &lt;- tm_shape(Zone)+tm_fill(col = NA, alpha = 0, border.col = NA, lwd = 0)+\n             tm_shape(Diff.XY)+\n             tm_polygons(col = \"red\", alpha = .5,border.col =\"black\", lwd = 1)+\n             tm_layout(main.title = \"st_difference(st_union(polysX), st_union(polysY))\", \n                       main.title.size = .8)\n\nMapDiffYX &lt;- tm_shape(Zone)+tm_fill(col = NA, alpha = 0, border.col = NA, lwd = 0)+\n             tm_shape(Diff.YX)+\n             tm_polygons(col = \"yellow\", alpha = .5,border.col =\"black\", lwd = 1)+\n             tm_layout(main.title = \"st_difference(st_union(polysY), st_union(polysX))\", \n                       main.title.size = .8)\n\nMapSymDiffYX &lt;- tm_shape(Zone)+tm_fill(col = NA, alpha = 0, border.col = NA, lwd = 0)+\n             tm_shape(Diff.symXY)+\n             tm_polygons(col = \"green\", alpha = .5,border.col =\"black\", lwd = 1)+\n             tm_layout(main.title = \"st_sym_difference(st_union(polysY), st_union(polysX))\",\n                       main.title.size = .8)\n\ntmap_arrange(MapZone, MapDiffXY, MapDiffYX, MapSymDiffYX, ncol=2, nrow=2)\n\n\n\nFigure 1.15: Différences de superposition entre des géométries de différentes couches\n\n\n\n\n1.2.4 Fonctions de mesures géométriques et de récupération des coordonnées géographiques\nLes principales fonctions de mesures géométriques et de coordonnées géographiques sont :\n\nst_area(x) calcule la superficie des polygones ou des multipolygones d’une couche sf .\nst_length(x) calcule la longueur des lignes ou des polylignes d’une couche sf .\nst_distance(x, y) calcule la distance 2D entre deux objets sf, exprimée dans le système de coordonnées de référence.\nst_coordinates(x) renvoie les coordonnées géographiques de géométries.\n\nCi-dessous, nous affichons les superficies des quatre arrondissements, puis nous enregistrons les superficies en m2 et en km2 dans deux nouveaux champs dénommés SupM2 et SupKm2.\n\n## Superficie des polygones des arrondissements\nst_area(Arrondissements)\n\nUnits: [m^2]\n[1] 477791738 119343215  58289370  87034244\n\n## Ajout de champs de superficie dans la table attributaire\nArrondissements$SupM2 &lt;- as.numeric(st_area(st_transform(Arrondissements, crs = 2949)))\nArrondissements$SupKm2 &lt;- as.numeric(st_area(st_transform(Arrondissements, crs = 2949)))/1000000\nhead(Arrondissements, n=2)\n\nSimple feature collection with 2 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -8027109 ymin: 5668860 xmax: -8000502 ymax: 5704391\nProjected CRS: WGS 84 / Pseudo-Mercator\n  NUMERO                                                         NOM\n1      1 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville\n2      4                                  Arrondissement des Nations\n                        geometry     SupM2    SupKm2\n1 POLYGON ((-8005013 5702777,... 235580454 235.58045\n2 POLYGON ((-8005680 5690860,...  58861606  58.86161\n\n\nDe manière très semblable, calculons la longueur de géométries étant des lignes ou des multilignes.\n\n## Longueurs en mètres\nPistesCyclables$longMetre &lt;- as.numeric(st_length(st_transform(PistesCyclables, crs = 2949)))\nPistesCyclables$longKm &lt;- as.numeric(st_length(st_transform(PistesCyclables, crs = 2949)))/10000\nhead(PistesCyclables, n=2)\n\nSimple feature collection with 2 features and 5 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -8010969 ymin: 5666202 xmax: -7997972 ymax: 5697954\nProjected CRS: WGS 84 / Pseudo-Mercator\n                       NOM OBJECTID SHAPE__Len                       geometry\n1     Axe de la Massawippi        1   13944.09 MULTILINESTRING ((-8010969 ...\n2 Axe de la Saint-François        2   19394.28 MULTILINESTRING ((-8001909 ...\n  longMetre    longKm\n1  9807.769 0.9807769\n2 13602.324 1.3602324\n\n\nPour calculer la longueur d’un périmètre, il faut préalablement récupérer son contour avec la méthode st_boundary, puis calculer la longueur avec st_length.\n\n## Conversion des polygones en lignes\nArrond.Contour &lt;- st_boundary(Arrondissements)\n## Calcul de la longueur et enregistrement dans deux nouveaux champs\nArrondissements$PerimetreMetre &lt;- as.numeric(st_length(Arrond.Contour))\nArrondissements$PerimetreKm &lt;- as.numeric(st_length(Arrond.Contour)) / 1000\nhead(Arrondissements)\n\nSimple feature collection with 4 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -8027109 ymin: 5668860 xmax: -7993013 ymax: 5704391\nProjected CRS: WGS 84 / Pseudo-Mercator\n  NUMERO                                                         NOM\n1      1 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville\n2      4                                  Arrondissement des Nations\n3      3                               Arrondissement de Lennoxville\n4      2                                Arrondissement de Fleurimont\n                        geometry     SupM2    SupKm2 PerimetreMetre PerimetreKm\n1 POLYGON ((-8005013 5702777,... 235580454 235.58045      143771.63   143.77163\n2 POLYGON ((-8005680 5690860,...  58861606  58.86161       50476.65    50.47665\n3 POLYGON ((-7993443 5684778,...  28776861  28.77686       43531.03    43.53103\n4 POLYGON ((-7999483 5693167,...  42882506  42.88251       44172.25    44.17225\n\n\nCalculons désormais la distance 2D (euclidienne) entre les centres des arrondissements. Nous utilisons donc la fonction st_distance(x), puisque nous avons une seule couche (x = Arrond.pointpoly).\n\n## Longueurs en mètres\nst_distance(Arrond.pointpoly)\n\nUnits: [m]\n         1         2         3         4\n1     0.00 10458.989 21787.479 18047.846\n2 10458.99     0.000 11555.203  8627.962\n3 21787.48 11555.203     0.000  9622.735\n4 18047.85  8627.962  9622.735     0.000\n\n\nAdmettons que nous souhaitons calculer la distance entre les centres des quatre arrondissements et l’hôtel de ville de Sherbrooke dont les coordonnées en degrés (WGS84, EPSG : 4326) sont les suivantes : -71.89306, 45.40417. Nous utilisons alors la fonction st_distance(x, y) dans laquelle les paramètres x et y sont les arrondissements et l’hôtel de ville. Quelques lignes de code suffisent à créer une couche pour l’hôtel de ville, à calculer les distances et à les stocker dans un nouveau champ attributaire de la couche arrondissement.\n\n## Création d'un objet sf pour l'hôtel de ville\nHotelVille &lt;- data.frame(ID = 1,\n                         Nom = \"Hôtel de ville\",\n                         lon = -71.89306,\n                         lat = 45.40417)\nHotelVille &lt;- st_as_sf(HotelVille, coords = c(\"lon\",\"lat\"), crs = 4326)\nhead(HotelVille)\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -71.89306 ymin: 45.40417 xmax: -71.89306 ymax: 45.40417\nGeodetic CRS:  WGS 84\n  ID            Nom                   geometry\n1  1 Hôtel de ville POINT (-71.89306 45.40417)\n\n## Nous nous assurons que les deux couches ont la même projection\nHotelVille &lt;- st_transform(HotelVille, st_crs(Arrond.pointpoly))\n## Calcul des distances\nArrondissements$DistHVMetre &lt;- as.numeric(st_distance(Arrond.pointpoly,HotelVille))\nArrondissements$DistHVKm &lt;- as.numeric(st_distance(Arrond.pointpoly,\n                                                   HotelVille)) / 1000\nhead(Arrondissements)\n\nSimple feature collection with 4 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -8027109 ymin: 5668860 xmax: -7993013 ymax: 5704391\nProjected CRS: WGS 84 / Pseudo-Mercator\n  NUMERO                                                         NOM\n1      1 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville\n2      4                                  Arrondissement des Nations\n3      3                               Arrondissement de Lennoxville\n4      2                                Arrondissement de Fleurimont\n                        geometry     SupM2    SupKm2 PerimetreMetre PerimetreKm\n1 POLYGON ((-8005013 5702777,... 235580454 235.58045      143771.63   143.77163\n2 POLYGON ((-8005680 5690860,...  58861606  58.86161       50476.65    50.47665\n3 POLYGON ((-7993443 5684778,...  28776861  28.77686       43531.03    43.53103\n4 POLYGON ((-7999483 5693167,...  42882506  42.88251       44172.25    44.17225\n  DistHVMetre  DistHVKm\n1   14661.518 14.661518\n2    4662.164  4.662164\n3    9058.677  9.058677\n4    4050.374  4.050374\n\n\nIl est fréquent de vouloir enregistrer les coordonnées géographiques dans des champs attributaires. Dans le code ci-dessous, nous créons deux champs (x et y) dans lesquels nous enregistrons les coordonnées géographiques des points au centre de la surface de chaque arrondissement. Pour ce faire, nous utilisons la méthode st_coordinates .\n\n## Coordonnées des centres de la surface des polygones\nxy &lt;- st_coordinates(st_point_on_surface(Arrondissements))\nhead(xy)\n\n            X       Y\n[1,] -8017707 5686628\n[2,] -8007570 5684053\n[3,] -7997637 5678149\n[4,] -7999683 5687552\n\n## Enregistrement dans la couche Arrondissements. Notez que :\n## xy[,1] signale de récupérer toutes les valeurs de la première colonne, soit X\n## xy[,2] signale de récupérer toutes les valeurs de la deuxième colonne, soit Y\nArrondissements$X &lt;- xy[,1] \nArrondissements$Y &lt;- xy[,2]\n\n\n1.2.5 Jointures spatiales\nEn géomatique, il est fréquent de réaliser des jointures spatiales, soit une opération qui consiste à joindre les attributs d’une couche géographique à une autre à partir d’une relation spatiale. Prenons deux exemples construits avec les installations sportives et récréatives (couche InstallationSport) et les arrondissements de la ville de Sherbrooke (Arrondissements).\nPremièrement, pour les installations sportives et récréatives (couche InstallationSport), nous souhaitons ajouter dans la table attributaire les champs NUMERO et NOM issus de la couche des arrondissements de la ville de Sherbrooke (Arrondissements). Grâce à ces deux champs, nous pouvons connaître dans quel arrondissement chaque installation sportive est située.\n\n## Jointure spatiale avec le paramètre st_intersects\nInstallS.join &lt;- st_join(InstallationSport, Arrondissements, join = st_intersects)\n## Visualisation des deux premiers enregistrements\nhead(InstallS.join, n=2)\n\nSimple feature collection with 2 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8009681 ymin: 5686891 xmax: -8001939 ymax: 5696536\nProjected CRS: WGS 84 / Pseudo-Mercator\n   TYPE DETAIL                   NOM.x SURFACE ECLAIRAGE OBJECTID NUMERO\n1 Aréna   &lt;NA&gt;    Aréna Eugène-Lalonde    &lt;NA&gt;      &lt;NA&gt;        1      2\n2 Aréna   &lt;NA&gt; Aréna Philippe-Bergeron    &lt;NA&gt;      &lt;NA&gt;        2      1\n                                                        NOM.y     SupM2\n1                                Arrondissement de Fleurimont  42882506\n2 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville 235580454\n     SupKm2 PerimetreMetre PerimetreKm DistHVMetre  DistHVKm        X       Y\n1  42.88251       44172.25    44.17225    4050.374  4.050374 -7999683 5687552\n2 235.58045      143771.63   143.77163   14661.518 14.661518 -8017707 5686628\n                  geometry\n1 POINT (-8001939 5686891)\n2 POINT (-8009681 5696536)\n\n## Suppression des champs utiles\nInstallS.join[c(\"SupM2\", \"SupKm2\", \"PerimetreMetre\", \n                       \"PerimetreKm\", \"DistHVMetre\",  \"DistHVKm\")] &lt;- list(NULL)\n## Modification des noms de champs : NOM.x et NOM.y\nnames(InstallS.join)[names(InstallS.join) == \"NOM.x\"] &lt;- \"NomInstallation\"\nnames(InstallS.join)[names(InstallS.join) == \"NOM.y\"] &lt;- \"NomArrondissement\"\nhead(InstallS.join, n=2)\n\nSimple feature collection with 2 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8009681 ymin: 5686891 xmax: -8001939 ymax: 5696536\nProjected CRS: WGS 84 / Pseudo-Mercator\n   TYPE DETAIL         NomInstallation SURFACE ECLAIRAGE OBJECTID NUMERO\n1 Aréna   &lt;NA&gt;    Aréna Eugène-Lalonde    &lt;NA&gt;      &lt;NA&gt;        1      2\n2 Aréna   &lt;NA&gt; Aréna Philippe-Bergeron    &lt;NA&gt;      &lt;NA&gt;        2      1\n                                            NomArrondissement        X       Y\n1                                Arrondissement de Fleurimont -7999683 5687552\n2 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville -8017707 5686628\n                  geometry\n1 POINT (-8001939 5686891)\n2 POINT (-8009681 5696536)\n\n\nDeuxièmement, une autre jointure classique consiste à dénombrer les points compris dans des polygones, soit une opération SIG communément appelée POINT-IN-POLYGON.\n\n## Sélection des points dans les polygones des arrondissements\n## Notez que la relation spatiale pour la jointure est st_contains\n## Nous aurions pu aussi utiliser st_intersects\nArrondissements$NbInstall = lengths(st_contains(Arrondissements, InstallationSport))\nhead(Arrondissements$NbInstall)\n\n[1] 125 166  29 116\n\n\n\n\n\n\n\nAutres relations spatiales à appliquer lors de la jointure spatiale\n\n\nAvec le paramètre join de la méthode st_join, il est possible de spécifier la jointure spatiale avec différentes méthodes : st_contains_properly, st_contains, st_covered_by, st_covers, st_crosses, st_disjoint, st_equals_exact, st_equals, st_is_within_distance, st_nearest_feature,st_overlaps, st_touches et st_within.\nN’hésitez pas à consulter la documentation de la fonction en tapant?st_join dans la console R.\n\n\n\n1.2.6 Requêtes spatiales\nDans un logiciel SIG, la sélection d’entités spatiales par localisation est une opération courante, équivalente à Select By Location dans ArcGis Pro ou Sélection par localisation dans QGIS.\nLe package sf permet de réaliser des requêtes spatiales avec notamment les méthodes suivantes :\n\nst_contains(x, y) renvoie les géométries de x qui contiennent celles de y. Cette fonction est donc l’inverse de st_within.\nst_disjoint(x, y) renvoie les géométries de x qui ne partagent aucune portion de celles de y. Cette fonction est donc l’inverse de st_intersects(x, y).\nst_equals(x, y) renvoie les géométries de x qui sont identiques à celles de y.\nst_intersects(x, y) renvoie les géométries de x qui partagent au moins une partie de celles de y. Elle est donc l’inverse de st_disjoints(x, y).\nst_nearest_feature(x, y) renvoie, pour chaque géométrie x, la géométrie de y qui est la plus proche.\nst_overlaps(x, y) cette fonction est très semblable à st_intersects(x, y). Toutefois, les types de géométries de x et de y doivent être identiques, c’est-à-dire deux couches de lignes ou de couches de polygones. Aussi, une géométrie ne peut pas contenir complètement l’autre comme avec st_within(x, y) et st_contains(x, y).\nst_touches(x, y) renvoie les géométries de x qui sont tangentes à celles de x sans qu’elles se chevauchent. Par exemple, deux arrondissements peuvent se toucher, c’est-à-dire qu’ils partagent une frontière commune sans que l’un chevauche l’autre.\nst_within(x, y) renvoie les géométries de x qui sont comprises intégralement dans celles de y. Cette fonction est donc l’inverse de st_contains(x, y).\nst_within_distance(x, y, dist =) renvoie les géométries de x qui sont situées à une certaine distance euclidienne de celles de y.\n\n\n\n\n\n\nModification de l’affichage du résultat de la requête spatiale : le paramètre sparse\n\n\nPar défaut, le résultat d’une requête spatiale renvoie une liste d’indices pour les géométries x et y. Il est aussi possible de renvoyer la matrice complète entre x et y, avec les valeurs TRUE quand la relation spatiale est vérifiée et FALSE pour une situation inverse.\nPrenons deux exemples pour illustrer le tout.\nLa figure ci-dessous représente les quatre arrondissements de la ville de Sherbrooke. Notez que les numéros correspondent aux indices des géométries.\n\n\n  NUMERO                                                         NOM\n1      1 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville\n2      4                                  Arrondissement des Nations\n3      3                               Arrondissement de Lennoxville\n4      2                                Arrondissement de Fleurimont\n\n\n\n\n\nAppliquons une requête spatiale entre les arrondissements avec st_intersects et sparse = TRUE. Pour chaque arrondissement, nous obtenons une liste des arrondissements qui l’intersectent.\n\nst_intersects(Arrondissements, Arrondissements, sparse = TRUE)\n\nSparse geometry binary predicate list of length 4, where the predicate\nwas `intersects'\n 1: 1, 2, 4\n 2: 1, 2, 3, 4\n 3: 2, 3, 4\n 4: 1, 2, 3, 4\n\n\nAvec sparse = FALSE, nous obtenons une matrice complète de dimension 4 X 4 arrondissements. Nous constatons que l’arrondissement 1 intersecte lui-même (évidemment!) et les arrondissements 2 et 4, mais il n’intersecte pas le 3.\n\nst_intersects(Arrondissements, Arrondissements, sparse = FALSE)\n\n      [,1] [,2]  [,3] [,4]\n[1,]  TRUE TRUE FALSE TRUE\n[2,]  TRUE TRUE  TRUE TRUE\n[3,] FALSE TRUE  TRUE TRUE\n[4,]  TRUE TRUE  TRUE TRUE\n\n\n\n\nConstruisons des requêtes plus complexes comprenant deux couches.\nPremièrement, écrivons une requête spatiale pour sélectionner les segments des pistes cyclables qui intersectent le parc du Mont-Bellevue. Pour ce faire, nous utilisons la fonction st_intersects avec l’argument sparse = FALSE et enregistrons le résultat dans un nouveau champ dénommé ParcMB.intersect qui prendra les valeurs TRUE ou FALSE.\n\n\n\n\n\n\n## Intersection\nRequeteSpatiale &lt;- st_intersects(PistesCyclables, MontBellevue, sparse = FALSE)\nhead(RequeteSpatiale)\n\n      [,1]\n[1,] FALSE\n[2,] FALSE\n[3,] FALSE\n[4,] FALSE\n[5,] FALSE\n[6,] FALSE\n\n## Création d'un nouveau champ\nPistesCyclables$ParcMB.intersect &lt;- RequeteSpatiale[, 1]\nhead(PistesCyclables)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -8010969 ymin: 5666202 xmax: -7997216 ymax: 5697954\nProjected CRS: WGS 84 / Pseudo-Mercator\n                       NOM OBJECTID  SHAPE__Len                       geometry\n1     Axe de la Massawippi        1 13944.08678 MULTILINESTRING ((-8010969 ...\n2 Axe de la Saint-François        2 19394.27693 MULTILINESTRING ((-8001909 ...\n3   Axe du Ruisseau-Dorman        3 16337.23985 MULTILINESTRING ((-7999121 ...\n4        Réseau utilitaire        4   467.23254 MULTILINESTRING ((-8000179 ...\n5        Réseau utilitaire        5    15.57987 MULTILINESTRING ((-8004036 ...\n6        Réseau utilitaire        6   823.83428 MULTILINESTRING ((-8003649 ...\n    longMetre      longKm ParcMB.intersect\n1  9807.76890 0.980776890            FALSE\n2 13602.32404 1.360232404            FALSE\n3 11469.13476 1.146913476            FALSE\n4   327.46928 0.032746928            FALSE\n5    10.95083 0.001095083            FALSE\n6   578.59143 0.057859143            FALSE\n\n## Nous constatons qu'un seul segment intersecte le parc\ntable(PistesCyclables$ParcMB.intersect)\n\n\nFALSE  TRUE \n  272     1 \n\n## Création d'une nouvelle couche pour la sélection\nPistesCyclables.Selection &lt;- PistesCyclables[PistesCyclables$ParcMB.intersect== TRUE, ]\n## Visualisation\ntmap_mode(\"view\")\ntm_shape(MontBellevue) + tm_fill(col=\"lightgreen\")+ tm_borders(col = \"black\", lwd=2)+\ntm_shape(PistesCyclables.Selection)+tm_lines(col=\"red\", lwd=1)\n\n\n\n\n\n\nCréons une deuxième requête spatiale pour sélectionner les points GPS situés à moins de cinq kilomètres de l’hôtel de ville de Sherbrooke avec la méthode st_is_within_distance.\n\n\n\n\n\n\n\n\n## Requête spatiale\nRequeteSpatiale &lt;- st_is_within_distance(PointsGPS, HotelVille, \n                                         5000, sparse = FALSE)\n## Ajout d'un champ pour la requête\nPointsGPS$HotelVille2km &lt;- RequeteSpatiale[, 1]\n## Nous constatons que 17 points GPS sont à moins de 5 km\ntable(PointsGPS$HotelVille2km)\n\n\nFALSE  TRUE \n   72    17 \n\n## Création d'une nouvelle couche pour la sélection\nPointsGPS.selection &lt;- PointsGPS[PointsGPS$HotelVille2km== TRUE, ]\n## Visualisation\ntm_shape(PointsGPS.selection) + tm_dots(col=\"red\", size = .05)+\ntm_shape(HotelVille)+tm_dots(col=\"black\", size = .25)\n\n\n\n\n\n\nFinalement, avec la méthode st_within, nous constatons que seuls deux points GPS sont situés dans le parc du Mont-Bellevue.\n\n## Requête spatiale\nRequeteSpatiale &lt;- st_within(st_transform(PointsGPS, st_crs(MontBellevue)),\n                             MontBellevue, sparse = FALSE)\ntable(RequeteSpatiale[,1])\n\n\nFALSE  TRUE \n   87     2 \n\n\n\n1.2.7 Manipulation des données attributaires\nDans cette section, nous verrons comment importer une table attributaire, puis la joindre à une couche géographique, ajouter et calculer de nouveaux champs et réaliser des requêtes attributaires.\n\n1.2.7.1 Importation d’une table attributaire\n\n\n\n\n\nJoindre les attributs d’une table externe à une couche vectorielle sf\n\n\nDans un SIG, joindre une table à une couche géographique vectorielle est une opération courante. Par exemple, il est fréquent de joindre des données socioéconomiques issues d’un recensement à une couche géographique (divisions de recensement, subdivisions de recensement, secteurs de recensement, aires de diffusion, etc.).\nPour ce faire, vous devez importer les données dans un DataFrame de R. Ces données peuvent être stockées dans différents formats de fichiers (texte délimité par des virgules (extension csv), dBase (dbf), Excel (xlsx)) ou dans des fichiers provenant de logiciels statistiques commerciaux comme Stata, SAS et SPSS (dta, sas7bdat, sav).\nDans cette section, nous voyons seulement l’importation de fichiers texte délimités par des virgules, de fichiers Excel et dBase. Concernant ce dernier type de fichier, notez que la table attributaire d’une couche Esri Shapefile est stockée dans un fichier dBase! Il peut être intéressant d’importer la table sans les géométries.\nPour une description détaillée de l’importation d’autres fichiers (entre autres Stata, SAS et SPSS), consultez la section intitulée Manipulation d’un DataFrame (Apparicio et Gelb 2022).\n\n\nDans le code ci-dessous, nous voyons comment importer trois types de fichiers :\n\nread.csv(file) pour importer un fichier délimité par des virgules. Cette fonction est de base avec R, ce qui signifie qu’elle ne nécessite pas l’installation d’un package.\nread.dbf(file) pour importer un fichier dBase. Cette fonction est rattachée au package foreign que vous devez installer si ce n’est pas déjà fait (commande install.packages(\"foreign\")) et le charger (commande library(\"foreign\")).\nread.xlsx(file) pour importer un fichier Excel. Cette fonction est rattachée au package xlsx que vous devez installer si ce n’est pas déjà fait (commande install.packages(\"xlsx\")) et le charger (commande library(\"xlsx\")).\n\n\nlibrary(\"xlsx\")      # package pour importer des fichiers Excel\nlibrary(\"foreign\")   # package pour importer des fichiers dBase\n## Importation du fichier csv\nt1 &lt;- Sys.time()\ndfCSV &lt;- read.csv(file = \"data/chap01/tables/SRQC2021.csv\",\n                    header = TRUE,\n                    dec = \".\",    # séparateur de décimales qui peut être remplacé par ,\n                    sep = \",\"     # séparateur des champs qui peut être remplacé par ;\n                    )\nt2 &lt;- Sys.time()\ncat(\"temps de traitement (CSV) : \", \n    as.numeric(difftime(t2,t1,units=\"secs\")),\n    \" secondes\")\n\ntemps de traitement (CSV) :  0.01712108  secondes\n\n## Importation d'un fichier Excel avec le nom de fichier et de la feuille Excel\n## sheetIndex = 1 signale l'importation de la première feuille Excel\nt1 &lt;- Sys.time()\ndfExcel &lt;- read.xlsx(file = \"data/chap01/tables/ADSRQC2021.xlsx\",\n                      sheetIndex = 2)\nt2 &lt;- Sys.time()\ncat(\"temps de traitement (Excel) : \", \n    as.numeric(difftime(t2,t1,units=\"secs\")),\n    \" secondes\")\n\ntemps de traitement (Excel) :  7.323151  secondes\n\n## Importation du fichier dBase\nt1 &lt;- Sys.time()\ndfDbf &lt;- read.dbf(file = \"data/chap01/tables/ADQC2021.dbf\")\nt2 &lt;- Sys.time()\ncat(\"temps de traitement (dBase) : \", \n    as.numeric(difftime(t2,t1,units=\"secs\")),\n    \" secondes\")\n\ntemps de traitement (dBase) :  0.134568  secondes\n\n\n\n\n\n\n\nExportation du fichier Excel dans un fichier texte\n\n\nLe temps nécessaire pour importer un fichier Excel est bien plus long que pour des fichiers texte et dBase! Par conséquent, si vous travaillez avec Excel, il est vivement conseillé de l’exporter vers un fichier texte (dans Excel, Fichier/Enregistrer sous/type de fichier CSV).\n\n\nQuelques lignes suffisent pour explorer la structure des données importées avec les fonctions nrow, ncol, colnames (respectivement le nombre de lignes, le nombre de colonnes et les noms des colonnes du dataframe).\n\n## Nombre de lignes et de colonnes\nnrow(dfCSV)\n\n[1] 2245\n\nncol(dfCSV)\n\n[1] 40\n\ncat(\"le DataFrame dfCSV a\", nrow(dfCSV), \"lignes (observations)\",\n    'et', ncol(dfCSV), \"colonnes\\n\")\n\nle DataFrame dfCSV a 2245 lignes (observations) et 40 colonnes\n\n## Noms des champs\ncolnames(dfCSV)\n\n [1] \"SRIDU\"                     \"PopTotAge\"                \n [3] \"Pop0_14\"                   \"Pop15_64\"                 \n [5] \"Pop65plus\"                 \"TotalLog\"                 \n [7] \"MaisonIndiv\"               \"MaisonJumulee\"            \n [9] \"MaisonRangee\"              \"AppartDuplex\"             \n[11] \"AppartMoins5E\"             \"Appart5EtPlus\"            \n[13] \"AutreMaisonIndivAttenante\" \"LogementMobile\"           \n[15] \"TotalMenag\"                \"Menage1pers\"              \n[17] \"Menage2pers\"               \"Menage3pers\"              \n[19] \"Menage4pers\"               \"Menage5pPlus\"             \n[21] \"RevMedMenage\"              \"PopTotMFRApI\"             \n[23] \"PopTotMFR\"                 \"PopTotMFRPct\"             \n[25] \"TotalMenag2\"               \"Proprietaire\"             \n[27] \"Locataire\"                 \"TotalLog2\"                \n[29] \"Log1960ouAv\"               \"Log1961_80\"               \n[31] \"Log1981_90\"                \"Log1991_00\"               \n[33] \"Log2001_05\"                \"Log2006_10\"               \n[35] \"Log2011_15\"                \"Log2016_21\"               \n[37] \"ValeurMedLog\"              \"ValeurMoyLog\"             \n[39] \"LoyerMedian\"               \"LoyerMoyen\"               \n\n## Affichage des deux premières observations\nhead(dfCSV, n=2)\n\n                                                      SRIDU PopTotAge Pop0_14\n1 4470001.01 (SR), Drummondville (RMR) (4470001.01) (00000)      5080     810\n2 4470001.02 (SR), Drummondville (RMR) (4470001.02) (00000)      3400     175\n  Pop15_64 Pop65plus TotalLog MaisonIndiv MaisonJumulee MaisonRangee\n1     3285       985     2280        1290           185          210\n2     1305      1920     1815         155            75           85\n  AppartDuplex AppartMoins5E Appart5EtPlus AutreMaisonIndivAttenante\n1           70           415             0                        15\n2           15          1485             0                         5\n  LogementMobile TotalMenag Menage1pers Menage2pers Menage3pers Menage4pers\n1            100       2285         705         895         325         225\n2              0       1810         985         670         100          45\n  Menage5pPlus RevMedMenage PopTotMFRApI PopTotMFR PopTotMFRPct TotalMenag2\n1          130        69000         5085       520         10.2        2295\n2           15        47600         2885       545         18.9        1820\n  Proprietaire Locataire TotalLog2 Log1960ouAv Log1961_80 Log1981_90 Log1991_00\n1         1445       850      2295         115        590        535        485\n2          485      1335      1820          50        310        375        405\n  Log2001_05 Log2006_10 Log2011_15 Log2016_21 ValeurMedLog ValeurMoyLog\n1        155         70         75        265       250000       250200\n2        215        200        120        140       250000       305000\n  LoyerMedian LoyerMoyen\n1         695        742\n2         740        737\n\n\n\n1.2.7.2 Jointure attributaire avec la couche géographique sf\n\nLes données importées dans la table attributive proviennent du recensement de Statistique Canada de 2021 et sont ancrées au niveau des secteurs de recensement (SR) des régions métropolitaines de recensement (RMR) et des agglomérations de recensement (AR) du Québec. Pour les SR de la RMR de Sherbrooke, les données de la couche géométrique sont importées à partir d’un fichier shapefile. Aussi, nous constatons que les deux sources de données ont un champ commun SRIDU, soit l’identifiant unique des SR, mais que l’information y est présentée différemment :\n\nDans la couche géographique SR.RMRSherb (objet sf), nous avons une observation avec la valeur 4470001.01, soit un champ avec dix caractères.\nDans la table attributaire dfCSV (DataFrame), nous avons une observation avec la valeur 4470001.01 (SR), Drummondville (RMR) (4470001.01) (00000).\n\nPar conséquent, avant d’appliquer une jointure, nous modifions le champ SRIDU de ce DataFrame afin qu’il ait aussi dix caractères avec la ligne de code suivante : dfCSV$SRIDU &lt;- substr(dfCSV$SRIDU, 1, 10). De la sorte, nous récupérons uniquement les dix premiers caractères.\nFinalement, la jointure est réalisée avec la fonction merge avec laquelle nous spécifions le résultat de la jointure (SR.RMRSherbDonnees), la couche géographique (SR.RMRSherb), la table attributaire (dfCSV) et le champ commun aux deux avec l’option (by =\"SRIDU\") :\nSR.RMRSherbDonnees &lt;- merge(SR.RMRSherb, dfCSV, by = \"SRIDU\").\n\n## Importation des SR de la RMR de Sherbrooke\nSR.RMRSherb &lt;- st_read(dsn = \"data/chap01/gpkg/Recen2021Sherbrooke.gpkg\", \n                       layer = \"SherbSR\", quiet=TRUE)\n## Visualisation des premiers enregistrements\nhead(as.data.frame(SR.RMRSherb), n=2)\n\n                IDUGD      SRIDU   SRNOM SUPTERRE PRIDU SRpop_2021 SRtlog_2021\n1 2021S05074330001.00 4330001.00 0001.00   3.1882    24       5637        2918\n2 2021S05074330002.00 4330002.00 0002.00   0.8727    24       1868        1169\n  SRrhlog_2021 RMRcode   HabKm2                           geom\n1         2756     433 1768.082 MULTIPOLYGON (((7764998 127...\n2         1063     433 2140.484 MULTIPOLYGON (((7763361 127...\n\nhead(dfCSV, n=2)\n\n                                                      SRIDU PopTotAge Pop0_14\n1 4470001.01 (SR), Drummondville (RMR) (4470001.01) (00000)      5080     810\n2 4470001.02 (SR), Drummondville (RMR) (4470001.02) (00000)      3400     175\n  Pop15_64 Pop65plus TotalLog MaisonIndiv MaisonJumulee MaisonRangee\n1     3285       985     2280        1290           185          210\n2     1305      1920     1815         155            75           85\n  AppartDuplex AppartMoins5E Appart5EtPlus AutreMaisonIndivAttenante\n1           70           415             0                        15\n2           15          1485             0                         5\n  LogementMobile TotalMenag Menage1pers Menage2pers Menage3pers Menage4pers\n1            100       2285         705         895         325         225\n2              0       1810         985         670         100          45\n  Menage5pPlus RevMedMenage PopTotMFRApI PopTotMFR PopTotMFRPct TotalMenag2\n1          130        69000         5085       520         10.2        2295\n2           15        47600         2885       545         18.9        1820\n  Proprietaire Locataire TotalLog2 Log1960ouAv Log1961_80 Log1981_90 Log1991_00\n1         1445       850      2295         115        590        535        485\n2          485      1335      1820          50        310        375        405\n  Log2001_05 Log2006_10 Log2011_15 Log2016_21 ValeurMedLog ValeurMoyLog\n1        155         70         75        265       250000       250200\n2        215        200        120        140       250000       305000\n  LoyerMedian LoyerMoyen\n1         695        742\n2         740        737\n\n## Modification du champ SRIDU du DataFrame dfCSV\ndfCSV$SRIDU &lt;- substr(dfCSV$SRIDU, 1, 10)\n## Jointure attributaire avec la fonction merge\nSR.RMRSherbDonnees &lt;- merge(SR.RMRSherb, \n                           dfCSV, \n                           by = \"SRIDU\")\n\n\n\n\n\n\nJointure avec deux champs ayant des noms différents\n\n\nEn résumé, une jointure attributaire s’écrit :\nNouvelObjetSf &lt;- merge(X, Y, by = \"Nom du champ commun pour la jointure\")\navec X et Y étant respectivement l’objet sf (couche géographique) et la table attributaire à joindre. Si les champs pour la jointure ont des noms différents, il est possible d’écrire :\nNouvelObjetSf &lt;- merge(X, Y, by.x = \"Champ X pour la jointure\", by.y = \"Champ Y pour la jointure\")\nCe type de jointure conserve uniquement les observations qui sont communes à la couche géographique et à la table attributaire. Concrètement, si une couche comprend 100 entités spatiales et la table attributaire uniquement 80 observations, la couche résultante (NouvelObjetSf) aura uniquement 80 entités spatiales (bien entendu, quand les valeurs concordent…).\nLorsque vous souhaitez quand même conserver toutes les entités spatiales de la couche géographique de départ, écrivez :\nNouvelObjetSf &lt;- merge(X, Y, by = \"Nom du champ commun pour la jointure\", all.x = TRUE)\nDans la nouvelle couche Sf, les entités spatiales de X qui n’ont pas été appariées avec les observations de la table attributaire Y auront des valeurs nulles (NA) pour les champs de X ajoutés au NouvelObjetSf.\nPour obtenir plus d’informations sur les différentes variantes d’une jointure, tapez ?merge dans la console.\n\n\n\n1.2.7.3 Ajout et calcul de champs\nDans la section 1.2.4, nous avons vu comment ajouter des champs relatifs à la géométrie (aire, longueur, distance, coordonnées de centroïdes). Dans un SIG, il est courant de calculer de nouveaux champs à partir de champs existants dans la table attributaire (par exemple, avec les outils Calculatrice de champ dans QGIS ou Calculate Field dans ArcGIS Pro).\nCe type de traitement est aussi très simple à réaliser dans R. Pour ce faire, nous utilisons des opérateurs mathématiques, relationnels et logiques comme dans n’importe quel logiciel de SIG. En guise d’exemple, nous calculons ci-dessous les pourcentages d’enfants de moins de 15 ans et de locataires. Ces pourcentages sont arrondis à deux décimales avec la fonction round.\n\n## Création et cacul de nouveau champs\nSR.RMRSherbDonnees$PctPop0_14 &lt;- round(SR.RMRSherbDonnees$Pop0_14 / \n                                     SR.RMRSherbDonnees$PopTotAge * 100, 2)\n\nSR.RMRSherbDonnees$PctLocataire &lt;- round(SR.RMRSherbDonnees$Locataire / \n                                     SR.RMRSherbDonnees$TotalMenag2 * 100, 2)\n\n\n1.2.7.4 Requêtes attributaires\nDans un SIG, il est fréquent de réaliser une requête attributaire pour explorer les données (par exemple, avec les outils Select By Attributes dans ArcGIS Pro et Sélection avec expression dans QGIS) et exporter le résultat de la requête dans une nouvelle couche (Export Features dans ArcGIS Pro et Sauvegarder les entités sélectionnées sous…).\nDans le code ci-dessous, vous trouverez plusieurs exemples de requêtes attributaires. Remarquez que les résultats des requêtes sont enregistrés dans de nouveaux objets sf (couches géographiques) dénommés Requete1 à Requete5.\n\n## Sélection de l'axe cyclable de la Magog\n#############################################\n# Affichage des valeurs uniques pour le champ NOM de la couche PistesCyclables\nunique(PistesCyclables$NOM)\n\n [1] \"Axe de la Massawippi\"         \"Axe de la Saint-François\"    \n [3] \"Axe du Ruisseau-Dorman\"       \"Réseau utilitaire\"           \n [5] \"Tronçon fermé temporairement\" \"Détour\"                      \n [7] \"Axe de la Magog\"              \"Axe du Ruisseau-Kee\"         \n [9] \"Axe de la Magog Sud\"          NA                            \n[11] \"Axe du Sommet\"               \n\n## Requête attributaire et enregistrement du résultat dans un nouvel objet sf\nRequete1 &lt;- subset(PistesCyclables, NOM == \"Axe de la Magog\")\ncat(nrow(Requete1), \"enregistrements sélectionnés sur\", nrow(PistesCyclables))\n\n15 enregistrements sélectionnés sur 273\n\n## Si vous souhaitez connaître uniquement le nombre d'enregistrements sélectionnés\n## sans créer un nouvel objet sf, il suffit d'écrire :\nnrow(subset(PistesCyclables, NOM == \"Axe de la Magog\"))\n\n[1] 15\n\n## Sélection des SR dont la moitié ou plus des logements sont en location\n##########################################################################\n## Sommaire statistique sur le champ pourcentage de locataires\nsummary(SR.RMRSherbDonnees$PctLocataire)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   6.92   18.03   41.63   43.81   65.95   93.83 \n\n## Requête attributaire et enregistrement du résultat dans un nouvel objet sf\nRequete2 &lt;- subset(SR.RMRSherbDonnees, PctLocataire &gt;= 50)\ncat(nrow(Requete2), \"enregistrements sélectionnés sur\", nrow(SR.RMRSherbDonnees))\n\n20 enregistrements sélectionnés sur 50\n\n## Sélection des installations sportives avec un éclairage dans \n## l'arrondissement des Nations (deux critères dans la requête)\n##########################################################################\nunique(InstallS.join$NomArrondissement)\n\n[1] \"Arrondissement de Fleurimont\"                               \n[2] \"Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville\"\n[3] \"Arrondissement des Nations\"                                 \n[4] \"Arrondissement de Lennoxville\"                              \n\ntable(InstallS.join$ECLAIRAGE)\n\n\nNon Oui \n 46  85 \n\n## Requête attributaire avec un opérateur AND (&)\nRequete3 &lt;- subset(InstallS.join,\n                   NomArrondissement == \"Arrondissement des Nations\" & \n                   ECLAIRAGE == \"Oui\")\ncat(nrow(Requete3), \"enregistrements sélectionnés sur\", nrow(InstallS.join))\n\n30 enregistrements sélectionnés sur 436\n\n## Sélection des SR avec deux critères et un opérateur OR (|)\n##########################################################################\n## Sommaires statistiques sur deux champs\nsummary(SR.RMRSherbDonnees$LoyerMoyen)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  570.0   679.5   729.0   768.4   847.5  1200.0 \n\nsummary(SR.RMRSherbDonnees$ValeurMedLog)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 200000  235500  250000  272520  300000  476000 \n\n## Requête attributaire avec un opérateur OR \nRequete4 &lt;- subset(SR.RMRSherbDonnees,\n                   LoyerMoyen &lt; 700 | ValeurMedLog &lt; 250000)\ncat(nrow(Requete4), \"enregistrements sélectionnés sur\", nrow(SR.RMRSherbDonnees))\n\n27 enregistrements sélectionnés sur 50\n\n## Sélection de différents types d'installations sportives\n##########################################################################\nunique(InstallS.join$TYPE)\n\n [1] \"Aréna\"                          \"Tir à l'arc\"                   \n [3] \"Pétanque\"                       \"Jeu de galets\"                 \n [5] \"Planche à roulettes\"            \"Préau et plancher de danse\"    \n [7] \"Patinoire à bandes mobiles\"     \"Surface, anneau ou étang glacé\"\n [9] \"Patinoire à bandes fixes\"       \"Plage\"                         \n[11] \"Jeu d'eau\"                      \"Piste multifonctionnelle\"      \n[13] \"Glissade sur tube\"              \"Jeu de fers\"                   \n[15] \"Piscine\"                        \"Tennis\"                        \n[17] \"Baseball\"                       \"Basketball\"                    \n[19] \"Football\"                       \"Volleyball\"                    \n[21] \"Ultimate frisbee\"               \"Pickleball\"                    \n[23] \"Soccer\"                         \"Jeu modulaire\"                 \n\n## Requête attributaire avec un opérateur %in%\nRequete5 &lt;- subset(InstallS.join,\n                             TYPE %in% c(\"Aréna\", \"Piscine\", \"Jeu d'eau\"))\ncat(nrow(Requete5), \"enregistrements sélectionnés sur\", nrow(InstallS.join))\n\n26 enregistrements sélectionnés sur 436"
  },
  {
    "objectID": "01-ManipulationDonneesSpatiales.html#sec-013",
    "href": "01-ManipulationDonneesSpatiales.html#sec-013",
    "title": "1  Manipulation des données spatiales dans R",
    "section": "\n1.3 Manipulation de données matricielles (raster)",
    "text": "1.3 Manipulation de données matricielles (raster)\nEn géomatique, les données matricielles (raster) sont une représentation de l’information spatiale sous forme d’une grille rectangulaire composée de cellules élémentaires de taille identique appelées pixels, soit une image. Chaque pixel a une valeur pour une caractéristique spécifique, comme l’altitude, la température, l’utilisation du sol, etc. Les données matricielles sont couramment employées dans les systèmes d’information géographique (SIG) pour la cartographie, l’analyse et la prise de décision en géomatique.\nDans cette section, nous abordons uniquement des fonctions simples de manipulation de données matricielles, notamment le mosaïquage et découpage d’images, et les requêtes attributaires sur des images.\n\n1.3.1 Mosaïquage et découpage d’images\nUne fois plusieurs images importées, il est fréquent de vouloir les fusionner. Pour ce faire, nous utilisons deux méthodes du package terra :\n\nterra::merge fusionne plusieurs images (objets de type SpatRasters) pour former un nouvel objet SpatRasters dont l’étendue est recalculée en fonction des images fusionnées. Par contre, quand les images se chevauchent, les valeurs des pixels dans les zones de chevauchement seront prises dans le même ordre que les images.\nterra::mosaic fusionne aussi plusieurs images. Toutefois, dans les zones de chevauchement, les moyennes des pixels sont calculées. Selon la documentation de terra, cette méthode serait plus rapide que la précédente. Dans le code ci-dessous, nous fusionnons les quatre feuillets de modèles numériques d’altitude (MNA) importés dans la section 1.1.2.\n\n\n## Les GeoTIFF importés avec terra sont bien des SpatRaster\nclass(f21e05_101)\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n## Création d'une liste pour les cinq feuillets SpatRaster\nrlist &lt;- list(f21e05_101, f21e05_201, f31h08_102,\n              f31h08_202, f21e12_101)\nrsrc &lt;- sprc(rlist)\n## Création de la mosaïque\nMosaicSherb &lt;- mosaic(rsrc)\nMosaicSherb\n\nclass       : SpatRaster \ndimensions  : 4187, 5575, 1  (nrow, ncol, nlyr)\nresolution  : 9e-05, 9e-05  (x, y)\nextent      : -72.25087, -71.74913, 45.24907, 45.6259  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource(s)   : memory\nname        : f21e05_101 \nmin value   :   123.7184 \nmax value   :   845.0474 \n\n\nVous constatez ci-dessus que la projection des images est lon/lat NAD83 (EPSG:4269).\nD’autres fonctions permettent de découper une image en fonction d’une autre image (objet SpatRaster de terra)) ou d’un objet terra vectoriel (SpatVector) :\n\ncrop(x, y) découpe une image x en prenant l’étendue de y.\nmask(x, y) découpe une image x en prenant la zone (pixels avec des valeurs non nulles ou objets vectoriels) de y. Les pixels en dehors de cette zone auront nulle comme valeur (NA dans R).\n\nEn guise d’exemple, découpons la mosaïque avec le polygone de la ville de Sherbrooke en utilisant la méthode mask. Attention, les deux sources de données doivent avoir la même projection et il faut préalablement convertir l’objet sf en objet terra.\n\n## Changement de projection pour le polygone de la ville de Sherbrooke\n## Application de la même projection que celle de la mosaïque\nVilleSherb.EPSG4269 &lt;- st_transform(Arrond.Union, crs(MosaicSherb))\n# Convertir l'objet sf en un objet SpatVector de terra\nVilleSherb.SpatVector = vect(VilleSherb.EPSG4269)\n## Découpage de la mosaïque avec le polygone de la ville de Sherbrooke\nMosaicSherbCrop &lt;- terra::mask(MosaicSherb, VilleSherb.SpatVector)\nMosaicSherbCrop\n\nclass       : SpatRaster \ndimensions  : 4187, 5575, 1  (nrow, ncol, nlyr)\nresolution  : 9e-05, 9e-05  (x, y)\nextent      : -72.25087, -71.74913, 45.24907, 45.6259  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource(s)   : memory\nname        : f21e05_101 \nmin value   :        126 \nmax value   :        390 \n\n## Constastez ci-dessus que le nom de l'image est f21e05_101.\n## Pour le changer, utilisez la fonction names()\nnames(MosaicSherbCrop) = \"Elevation\"\nMosaicSherbCrop\n\nclass       : SpatRaster \ndimensions  : 4187, 5575, 1  (nrow, ncol, nlyr)\nresolution  : 9e-05, 9e-05  (x, y)\nextent      : -72.25087, -71.74913, 45.24907, 45.6259  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource(s)   : memory\nname        : Elevation \nmin value   :       126 \nmax value   :       390 \n\n## Visualisation du résultat\nplot(MosaicSherbCrop)\n\n\n\n\n\n1.3.2 Requêtes attributaires sur des images\nAvant d’effectuer une requête, il est judicieux d’explorer les valeurs des pixels de l’image avec un histogramme et la fonction summary(Nom de l'image) (valeurs minimales, maximales, quartiles, moyenne et valeurs nulles – NA).\n\n## Sommaire statistique des valeurs\nsummary(MosaicSherbCrop)\n\n   Elevation    \n Min.   :126.0  \n 1st Qu.:197.7  \n Median :227.6  \n Mean   :225.4  \n 3rd Qu.:251.4  \n Max.   :389.9  \n NA's   :78040  \n\n## Histogramme\nhist(MosaicSherbCrop,\n     main = \"Mosaïque du MNA pour la ville de Sherbrooke\",\n     xlab = \"Élévation (mètres)\", ylab = \"Fréquence\",\n     col = \"lightgreen\")\n\n\n\n## Histogramme en barre de 125 à 400 avec un saut de 25 mètres \nhist(MosaicSherbCrop,\n     main = \"Mosaïque du MNA pour la ville de Sherbrooke\",\n     xlab = \"Élévation (mètres)\", ylab = \"Fréquence\",\n     breaks = seq(from = 125, to = 400, by = 25),\n     col = \"lightgreen\")\n\n\n\n\n\n## Sélection des pixels avec une élévation d'au moins 300 mètres\nMosaicSherbCrop300 = clamp(MosaicSherbCrop, lower = 300)\nplot(MosaicSherbCrop300, \n     main = \"Pixels avec une élévation d'au moins 300 mètres\")\n\n\n\n## Sélection des pixels avec une élévation de 200 à 300 mètres\nMosaicSherbCrop200_300 = clamp(MosaicSherbCrop, lower = 200, upper = 300)\nplot(MosaicSherbCrop200_300,\n     main = \"Pixels avec une élévation de 200 à 300 mètres\")"
  },
  {
    "objectID": "01-ManipulationDonneesSpatiales.html#sec-014",
    "href": "01-ManipulationDonneesSpatiales.html#sec-014",
    "title": "1  Manipulation des données spatiales dans R",
    "section": "\n1.4 Exportation de données spatiales de R vers des formats géographiques",
    "text": "1.4 Exportation de données spatiales de R vers des formats géographiques\n\n1.4.1 Exportation de données vectorielles sf\n\n\n\n\n\n\nPourquoi exporter des objets sf vers différents formats géographiques?\n\n\nPlusieurs méthodes d’analyse de données spatiales ne sont pas implémentées dans les logiciels de SIG comme ArcGIS Pro ou QGIS d’où l’intérêt de recourir à R ou à Python. La démarche méthodologique classique comprend alors trois étapes :\n\nImporter des données géographiques.\nRéaliser des analyses avancées dans R ou Python.\nExporter les résultats finaux vers différents formats géographiques (shapefile, GeoPackage, geodatabase d’ESRI, etc.).\n\nTrois raisons majeures motivent l’exportation des données :\n\nCartographier les résultats finaux dans votre logiciel SIG préféré.\nPartager les données avec des personnes n’utilisant pas R.\nRéaliser éventuellement d’autres analyses dans votre logiciel de SIG préféré.\n\n\n\nDans la section 1.1, nous avons vu que la fonction st_read() du package sf permet d’importer une multitude de formats géographiques. Pour exporter avec sf, utilisez simplement la fonction st_write(). Le code ci-dessous illustre comment exporter des objets sf aux formats shapefile (shp), GeoPackage (GPKG), Keyhole Markup Language (kml) et GeoJSON. Par défaut, st_write() n’écrase pas un fichier existant; pour l’écraser, ajoutez le paramètre append = FALSE.\n\n## Exportation au format shapefile\nst_write(PointsGPS, # couche sf\n         \"data/chap01/export/PointsGPS.shp\",  # chemin et nom du fichier\n         append = FALSE, # pour écraser le fichier s'il existe\n         driver = \"ESRI Shapefile\")\n\nDeleting layer `PointsGPS' using driver `ESRI Shapefile'\nWriting layer `PointsGPS' to data source \n  `data/chap01/export/PointsGPS.shp' using driver `ESRI Shapefile'\nWriting 89 features with 2 fields and geometry type Point.\n\n## Exportation dans une couche dans GPKG\nst_write(PointsGPS, \n         dsn = \"data/chap01/export/Data.gpkg\", \n         layer = \"PointsGPS\",\n         append = FALSE, \n         driver = \"GPKG\")\n\nDeleting layer `PointsGPS' using driver `GPKG'\nWriting layer `PointsGPS' to data source \n  `data/chap01/export/Data.gpkg' using driver `GPKG'\nWriting 89 features with 2 fields and geometry type Point.\n\n## Exportation vers un fichier KML\nst_write(PointsGPS, \n         dsn = \"data/chap01/export/PointsGPS.kml\", \n         append = FALSE,\n         driver=\"KML\")\n\nWriting layer `PointsGPS' to data source \n  `data/chap01/export/PointsGPS.kml' using driver `KML'\nWriting 89 features with 2 fields and geometry type Point.\n\n## Exportation vers un fichier GeoJSON\nst_write(PointsGPS, \n         dsn = \"data/chap01/export/PointsGPS.geojson\", \n         append = FALSE,\n         driver=\"GeoJSON\")\n\nDeleting layer not supported by driver `GeoJSON'\nDeleting layer `PointsGPS' failed\nWriting layer `PointsGPS' to data source \n  `data/chap01/export/PointsGPS.geojson' using driver `GeoJSON'\nUpdating existing layer PointsGPS\nWriting 89 features with 2 fields and geometry type Point.\n\n\nLe paramètre driver de la fonction st_write permet de spécifier le format du fichier. Pour obtenir la liste des formats qu’il est possible d’importer et d’exporter, tapez dans la console ?st_drivers ou consultez le tableau 1.1.\n\n\n\n\nTableau 1.1: Liste des formats avec le package sf (st_drivers)\n\n\n\n\n\n\n\n\n\n\nNom\nDescription\nÉcriture\nSi vecteur\nSi raster\n\n\n\nESRIC\nESRIC\nEsri Compact Cache\nFALSE\nTRUE\nTRUE\n\n\nnetCDF\nnetCDF\nNetwork Common Data Format\nTRUE\nTRUE\nTRUE\n\n\nPDS4\nPDS4\nNASA Planetary Data System 4\nTRUE\nTRUE\nTRUE\n\n\nVICAR\nVICAR\nMIPL VICAR file\nTRUE\nTRUE\nTRUE\n\n\nJP2OpenJPEG\nJP2OpenJPEG\nJPEG-2000 driver based on OpenJPEG library\nFALSE\nTRUE\nTRUE\n\n\nPDF\nPDF\nGeospatial PDF\nTRUE\nTRUE\nTRUE\n\n\nMBTiles\nMBTiles\nMBTiles\nTRUE\nTRUE\nTRUE\n\n\nBAG\nBAG\nBathymetry Attributed Grid\nTRUE\nTRUE\nTRUE\n\n\nEEDA\nEEDA\nEarth Engine Data API\nFALSE\nFALSE\nTRUE\n\n\nOGCAPI\nOGCAPI\nOGCAPI\nFALSE\nTRUE\nTRUE\n\n\nESRI Shapefile\nESRI Shapefile\nESRI Shapefile\nTRUE\nFALSE\nTRUE\n\n\nMapInfo File\nMapInfo File\nMapInfo File\nTRUE\nFALSE\nTRUE\n\n\nUK .NTF\nUK .NTF\nUK .NTF\nFALSE\nFALSE\nTRUE\n\n\nLVBAG\nLVBAG\nKadaster LV BAG Extract 2.0\nFALSE\nFALSE\nTRUE\n\n\nOGR_SDTS\nOGR_SDTS\nSDTS\nFALSE\nFALSE\nTRUE\n\n\nS57\nS57\nIHO S-57 (ENC)\nTRUE\nFALSE\nTRUE\n\n\nDGN\nDGN\nMicrostation DGN\nTRUE\nFALSE\nTRUE\n\n\nOGR_VRT\nOGR_VRT\nVRT - Virtual Datasource\nFALSE\nFALSE\nTRUE\n\n\nMemory\nMemory\nMemory\nTRUE\nFALSE\nTRUE\n\n\nCSV\nCSV\nComma Separated Value (.csv)\nTRUE\nFALSE\nTRUE\n\n\nGML\nGML\nGeography Markup Language (GML)\nTRUE\nFALSE\nTRUE\n\n\nGPX\nGPX\nGPX\nTRUE\nFALSE\nTRUE\n\n\nKML\nKML\nKeyhole Markup Language (KML)\nTRUE\nFALSE\nTRUE\n\n\nGeoJSON\nGeoJSON\nGeoJSON\nTRUE\nFALSE\nTRUE\n\n\nGeoJSONSeq\nGeoJSONSeq\nGeoJSON Sequence\nTRUE\nFALSE\nTRUE\n\n\nESRIJSON\nESRIJSON\nESRIJSON\nFALSE\nFALSE\nTRUE\n\n\nTopoJSON\nTopoJSON\nTopoJSON\nFALSE\nFALSE\nTRUE\n\n\nOGR_GMT\nOGR_GMT\nGMT ASCII Vectors (.gmt)\nTRUE\nFALSE\nTRUE\n\n\nGPKG\nGPKG\nGeoPackage\nTRUE\nTRUE\nTRUE\n\n\nSQLite\nSQLite\nSQLite / Spatialite\nTRUE\nFALSE\nTRUE\n\n\nWAsP\nWAsP\nWAsP .map format\nTRUE\nFALSE\nTRUE\n\n\nMySQL\nMySQL\nMySQL\nTRUE\nFALSE\nTRUE\n\n\nOpenFileGDB\nOpenFileGDB\nESRI FileGDB\nFALSE\nFALSE\nTRUE\n\n\nDXF\nDXF\nAutoCAD DXF\nTRUE\nFALSE\nTRUE\n\n\nCAD\nCAD\nAutoCAD Driver\nFALSE\nTRUE\nTRUE\n\n\nFlatGeobuf\nFlatGeobuf\nFlatGeobuf\nTRUE\nFALSE\nTRUE\n\n\nGeoconcept\nGeoconcept\nGeoconcept\nTRUE\nFALSE\nTRUE\n\n\nGeoRSS\nGeoRSS\nGeoRSS\nTRUE\nFALSE\nTRUE\n\n\nVFK\nVFK\nCzech Cadastral Exchange Data Format\nFALSE\nFALSE\nTRUE\n\n\nPGDUMP\nPGDUMP\nPostgreSQL SQL dump\nTRUE\nFALSE\nTRUE\n\n\nOSM\nOSM\nOpenStreetMap XML and PBF\nFALSE\nFALSE\nTRUE\n\n\nGPSBabel\nGPSBabel\nGPSBabel\nTRUE\nFALSE\nTRUE\n\n\nOGR_PDS\nOGR_PDS\nPlanetary Data Systems TABLE\nFALSE\nFALSE\nTRUE\n\n\nWFS\nWFS\nOGC WFS (Web Feature Service)\nFALSE\nFALSE\nTRUE\n\n\nOAPIF\nOAPIF\nOGC API - Features\nFALSE\nFALSE\nTRUE\n\n\nEDIGEO\nEDIGEO\nFrench EDIGEO exchange format\nFALSE\nFALSE\nTRUE\n\n\nSVG\nSVG\nScalable Vector Graphics\nFALSE\nFALSE\nTRUE\n\n\nIdrisi\nIdrisi\nIdrisi Vector (.vct)\nFALSE\nFALSE\nTRUE\n\n\nXLS\nXLS\nMS Excel format\nFALSE\nFALSE\nTRUE\n\n\nODS\nODS\nOpen Document/ LibreOffice / OpenOffice Spreadsheet\nTRUE\nFALSE\nTRUE\n\n\nXLSX\nXLSX\nMS Office Open XML spreadsheet\nTRUE\nFALSE\nTRUE\n\n\nElasticsearch\nElasticsearch\nElastic Search\nTRUE\nFALSE\nTRUE\n\n\nCarto\nCarto\nCarto\nTRUE\nFALSE\nTRUE\n\n\nAmigoCloud\nAmigoCloud\nAmigoCloud\nTRUE\nFALSE\nTRUE\n\n\nSXF\nSXF\nStorage and eXchange Format\nFALSE\nFALSE\nTRUE\n\n\nSelafin\nSelafin\nSelafin\nTRUE\nFALSE\nTRUE\n\n\nJML\nJML\nOpenJUMP JML\nTRUE\nFALSE\nTRUE\n\n\nPLSCENES\nPLSCENES\nPlanet Labs Scenes API\nFALSE\nTRUE\nTRUE\n\n\nCSW\nCSW\nOGC CSW (Catalog Service for the Web)\nFALSE\nFALSE\nTRUE\n\n\nVDV\nVDV\nVDV-451/VDV-452/INTREST Data Format\nTRUE\nFALSE\nTRUE\n\n\nMVT\nMVT\nMapbox Vector Tiles\nTRUE\nFALSE\nTRUE\n\n\nNGW\nNGW\nNextGIS Web\nTRUE\nTRUE\nTRUE\n\n\nMapML\nMapML\nMapML\nTRUE\nFALSE\nTRUE\n\n\nTIGER\nTIGER\nU.S. Census TIGER/Line\nFALSE\nFALSE\nTRUE\n\n\nAVCBin\nAVCBin\nArc/Info Binary Coverage\nFALSE\nFALSE\nTRUE\n\n\nAVCE00\nAVCE00\nArc/Info E00 (ASCII) Coverage\nFALSE\nFALSE\nTRUE\n\n\nHTTP\nHTTP\nHTTP Fetching Wrapper\nFALSE\nTRUE\nTRUE\n\n\n\n\n\n\n\n1.4.2 Exportation de données raster\n\nL’exportation d’objets SpatRasters de terra est très simple avec la méthode terra::writeRaster. En guise d’exemple, le code ci-dessous exporte la mosaïque de MNA dans un fichier GeoTIFF. Notez que le paramètre filetype permet de spécifier d’autres formats d’images de la liste qui est disponible au lien suivant : https://gdal.org/drivers/raster/index.html. En guise d’exemple, les paramètres EIR, ENVI, RST, ERS et GRASS permettent d’exporter vers les logiciels de télédétection ERDAS, ENVI, Idrisi, ERMapper et GRASS, tandis que le paramètre GPKG permet d’exporter vers un GeoPackage raster.\n\nterra::writeRaster(MosaicSherbCrop, \"data/chap01/export/MosaicSherb.tif\", \n                   filetype = \"GTiff\", \n                   overwrite = TRUE)"
  },
  {
    "objectID": "01-ManipulationDonneesSpatiales.html#sec-015",
    "href": "01-ManipulationDonneesSpatiales.html#sec-015",
    "title": "1  Manipulation des données spatiales dans R",
    "section": "\n1.5 Cartographie avec R",
    "text": "1.5 Cartographie avec R\n\n\n\n\n\nPourquoi cartographier des données dans R?\n\n\nVous avez certainement un logiciel de SIG préféré pour construire une carte thématique (QGIS ou ArcGIS Pro par exemple). Puisqu’en quelques clics de souris, il est facile de réaliser une carte dans un SIG, quel est donc l’intérêt d’écrire des lignes de code pour afficher une carte dans R? Autrement dit, pourquoi devriez-vous vous compliquer la vie à apprendre de la syntaxe R pour produire une simple carte? Savoir cartographier dans R a plusieurs avantages :\n\nCartographier rapidement les résultats d’une analyse dans R permet d’éviter des allers-retours (exportation et importation de données) entre R et un logiciel de SIG. Or, la cartographie fait partie intégrante d’une démarche méthodologique d’analyse ou de modélisation spatiale. Vous restez ainsi dans le même environnement de travail (R) jusqu’à l’obtention de vos résultats finaux. Une fois ces derniers obtenus, vous pouvez les exporter et construire une carte très élaborée dans un logiciel de SIG.\nLa syntaxe R n’est pas si compliquée. Quelques lignes de code écrites pour une première analyse peuvent être réutilisées, modifiées et bonifiées pour une autre analyse. Au fil de vos projets, vous construirez des cartes de plus en plus élaborées. Autrement dit, après quelques heures d’investissement, vous deviendrez une personne experte en cartographie dans R!\n\n\n\n\n\n\n\n\nQuels packages utiliser pour la cartographie dans R?\n\n\nIl existe plusieurs packages R pour la cartographie, notamment :\n\nggplot2 est certainement le meilleur package R pour réaliser des graphiques (Wickham 2016). Il permet désormais de construire des cartes.\ncartography permet de construire efficacement des cartes thématiques (Giraud et Lambert 2016). Pour avoir une idée de son potentiel, consultez cette Cheatsheet.\ntmap (Tennekes 2018) est actuellement l’un des packages les plus complets et les plus utilisés pour construire des cartes thématiques.\nDes packages spécifiques permettent de créer des cartes interactives sur Internet, notamment mapview, mapdeck et leaflet. Ce dernier est basé sur la librairie JavaScript, largement utilisée dans le domaine de la cartographie sur Internet.\n\nDans cette section, nous utilisons uniquement tmap dont plusieurs ressources sont disponibles sur Internet :\n\nSur le site CRAN de tmap, une excellente vignette intitulée tmap: get started!\nUn article dans Journal of Statistical Software de Martijn Tennekes, créateur du package tmap.\nLa documentation complète en PDF.\n\n\n\n\n1.5.1 Manipulation des couches géométriques\n\n1.5.1.1 Principales fonctions de représentation de couches vectorielles et matricielles\nIl existe trois catégories de fonctions pour paramétrer l’affichage de couches géographiques (tableau 1.2).\n\n\n\n\nTableau 1.2: Principales fonctions pour manipuler des couches vectorielles et matricielles\n\n\n\n\n\n\n\n\n\nFonction\nDescription\nPoints\nLignes\nPolyg.\nRaster\n\n\n\ntm_shape\nCrée un élément tmap à partir d’une couche géographique vectorielle (sf) ou matricielle (raster)\nX\nX\nX\nX\n\n\ntm_polygons\nDessine des polygones (couleur et contour)\n\n\nX\n\n\n\ntm_symbols\nDessine des symboles\nX\nX\nX\n\n\n\ntm_lines\nDessine des lignes\n\nX\n\n\n\n\ntm_text\nDessine des étiquettes à partir d’un champ\nX\nX\nX\n\n\n\ntm_raster\nAffiche un raster\n\n\n\nX\n\n\ntm_fill\nDessine l’intérieur de polygones\n\n\nX\n\n\n\ntm_border\nDessine les contours\n\n\nX\n\n\n\ntm_bubbles\nDessine des cercles (notamment proportionnels)\nX\nX\nX\n\n\n\ntm_squares\nDessine des carrés (notamment proportionnels)\nX\nX\nX\n\n\n\ntm_dots\nDessine des points\nX\nX\nX\n\n\n\ntm_markers\nDessine des icones avec étiquettes\nX\nX\nX\n\n\n\n\n\n\n\nConstruction d’une carte simple avec une couche vectorielle et une couche matricielle\nLe code ci-dessous permet d’afficher deux couches avec la fonction tm_shape : l’une vectorielle, l’autre matricielle (figure 1.16).\n\ntmap_mode(\"plot\")\n# 1er objet tmap pour une couche raster\ntm_shape(MosaicSherbCrop)+\n  tm_raster(palette = terrain.colors(10))+\n# 1er objet tmap pour une couche vectorielle\ntm_shape(Arrondissements)+\n  tm_borders(col = \"black\", lwd = 3)+ # contour noir avec une épaisseur de trois points\n  tm_text(\"NUMERO\") # Étiquettes identifiant l'arrondissement\n\n\n\nFigure 1.16: Exemple de carte construite avec le package tmap avec une couche polygonale et une image\n\n\n\n\n\n\nOrdre et hiérarchie des couches avec tmap.\n\n\nVous avez compris qu’une couche est affichée avec la fonction tm_shape et que le + permet d’ajouter une ou plusieurs fonctions d’habillage à cette couche (tm_polygons, tm_lines, tm_text, tm_raster, etc.).\nIl est possible d’en superposer en utilisant plusieurs tm_shape comme suit :\ntm_shape(Nom de la première couche)+ ... paramètres de la couche + tm_shape(Nom de la seconde couche)+ ... paramètres de la couche\nNotez que la première couche est celle avec laquelle la projection et l’étendue de la carte sont définies. Il est toutefois possible de changer le tout en utilisant l’argument is.master = TRUE dans le tm_shape d’une couche donnée.\n\n\nConstruction d’une carte avec plusieurs couches vectorielles\nLes lignes de code suivantes permettent de construire la figure 1.17 avec trois couches sf.\n\ntmap_mode(\"plot\")\n## Polygones\ntm_shape(Arrondissements)+\n  tm_text(\"NUMERO\")+ # Étiquettes identifiant l'arrondissement\n  tm_polygons(col=\"wheat\", border.col = \"black\", lwd = 3)+\n## Lignes\ntm_shape(Rues)+\n  tm_lines(col= \"gray\", lwd = 1)+\n## Points\ntm_shape(PointsGPS.Sherb)+\n  tm_dots(shape=21, col=\"blue\", size=.3)\n\n\n\nFigure 1.17: Exemple de carte construite avec le package tmap avec plusieurs couches vectorielles (polygones, lignes, points)\n\n\n\nLa figure 1.18 illustre la différence entre les fonctions tm_dots et tm_markers.\n\n## Points avec tm_dots()\nCartePoints &lt;- \n  tm_shape(Arrondissements) + tm_polygons(col=\"wheat\", border.col = \"black\") +\n  tm_shape(PointsGPS.Sherb) + tm_dots(shape=21, col=\"blue\", size=.3)\n## Icones avec tm_markers()\nCarteMarkers &lt;- \n  tm_shape(Arrondissements) + tm_polygons(col=\"wheat\", border.col = \"black\") +\n  tm_shape(PointsGPS.Sherb) + tm_markers(size = 0.2, border.col = rgb(0,0,0,0))\n## Combinaison des deux cartes\ntmap_arrange(CartePoints, CarteMarkers, ncol=2, nrow=1)\n\n\n\nFigure 1.18: Exemple de carte tmap avec tm_dots et tm_markers\n\n\n\n\n1.5.1.2 Couleurs uniques et palette de couleurs dans tmap\n\nVous avez remarqué plus haut que plusieurs fonctions comprennent l’argument col pour spécifier une couleur. Pour connaître les trois manières de spécifier une couleur dans R – nom de la couleur R (lightblue par exemple), code hexadécimal (#f03b20 par exemple) ou notation RVBA (rgb(0.2, 0.4, 0.4, 0) par exemple) –, consultez la section suivante (Apparicio et Gelb 2022).\nPour spécifier une palette de couleurs sur un champ dans différentes fonctions (entre autres, tm_polygons, tm_lines, tm_fill, tm_dots), il suffit d’utiliser deux arguments dans la fonction, soit col=\"Nom du champ\" et palette=\"nom de la palette de couleurs\".\nLe package tmap intègre les palettes de deux autres packages : viridisLite (Garnier et al. 2021) et RColorBrewer (Neuwirth 2022). Le premier propose cinq palettes de couleurs : viridis, magma, plasma, inferno, cividis. Le second intègre une série de palettes de couleurs proposées par la géographe et cartographe Cynthia Brewer et ses collègues (Harrower et Brewer 2003; Brewer, Hatchard et Harrower 2003). Vous avez probablement déjà exploré leur site Internet où il est possible de sélectionner une palette en fonction du nombre de classes, de la nature des données et de la codification des couleurs (HEX, RGB, CMYK). Succinctement, RColorBrewer propose plusieurs palettes regroupées selon trois catégories :\n\nPalettes qualitatives à appliquer à une variable qualitative nominale comme son nom l’indique (figure 1.19). Pour afficher les palettes et connaître leurs noms, tapez display.brewer.all(type=\"qual\") dans la console.\nPalettes séquentielles pour une variable continue avec des valeurs faibles à fortes (figure 1.20). Tapez display.brewer.all(type=\"seq\") dans la console.\nPalettes divergentes à appliquer à une variable continue dont les valeurs aux deux extrémités s’opposent (figure 1.21). Tapez display.brewer.all(type=\"div\") dans la console.\n\n\n\nFigure 1.19: Palettes de couleurs qualitatives du package RColorBrewer\n\n\n\nFigure 1.20: Palettes de couleurs séquentielles du package RColorBrewer\n\n\n\nFigure 1.21: Palettes de couleurs divergentes du package RColorBrewer\n\n\n\n\n\n\nComparaison de palettes avec un nombre de classes défini\n\n\nSi vous connaissez le nombre de classes, mais que vous hésitez à choisir telle ou telle palette de couleurs, tapez dans la console :\n\ndisplay.brewer.all(n=5, type=\"seq\", exact.n=TRUE)\ndisplay.brewer.all(n=5, type=\"div\", exact.n=TRUE)\ndisplay.brewer.all(n=5, type=\"qual\", exact.n=TRUE)\n\nD’autres arguments peuvent être ajoutés comme colorblindFriendly=TRUE qui renvoie uniquement des palettes de couleurs adaptées aux personnes daltoniennes. En guise d’exemple, avec cinq classes, il est possible de comparer neuf palettes divergentes et six autres adaptées aux personnes daltoniennes (figure 1.22).\n\n\nFigure 1.22: Palettes de couleurs divergentes du package RColorBrewer avec cinq classes\n\nVous hésitez encore à choisir une palette de couleurs? Tapez la syntaxe ci-dessous dans la console pour afficher l’ensemble des palettes des packages RColorBrewer et viridisLite.\ntmaptools::palette_explorer()\nPour inverser les couleurs d’une palette, vous devez précéder le nom de la palette par un signe moins (exemple : -Greens).\n\n\n\n1.5.1.3 Cartographie d’une variable qualitative : valeurs uniques\nApplication à une couche de points\nLe code ci-dessous illustre comment construire une carte thématique avec des couleurs appliquées à une variable qualitative nominale (champ TYPE de la couche InstallationSport) d’une couche de points (figure 1.23).\n\n## Carte\ntm_shape(Arrondissements)+\n  tm_borders()+\ntm_shape(InstallationSport)+\n  tm_dots(shape = 21,\n          size=.3,\n          col= \"TYPE\", \n          palette = \"Set1\", \n          title =\"Type d'installation\")+\ntm_layout(main.title = \"Installations sportives\",\n          frame=FALSE,\n          legend.position = c(\"left\", \"top\"),\n          legend.outside=TRUE)\n\n\n\nFigure 1.23: Exemple de cartographie d’une variable qualitative sur des points\n\n\n\nApplication à une couche de lignes\nLe code ci-dessous illustre comment construire une carte thématique avec des couleurs appliquées à une variable qualitative nominale (champ TYPESEGMEN) d’une couche de lignes (figure 1.24).\n\n## Listes des valeurs uniques\ntable(Rues$TYPESEGMEN)\n\n\n      Artère    Autoroute Chemin privé  Collectrice       Locale \n        1459          380          467          579         4844 \n\n## Lignes\ntmap_mode(\"plot\")\ntm_shape(Rues)+\n  tm_lines(col= \"TYPESEGMEN\",\n           palette = c(\"red\", \"brown4\", \"cornsilk1\", \"lightpink\", \"gainsboro\"),\n           lwd = 2 \n           )\n\n\n\nFigure 1.24: Exemple de cartographie d’une variable qualitative sur des lignes\n\n\n\nApplication à une couche de polygones\nLe code ci-dessous illustre comment construire une carte thématique avec des couleurs appliquées à une variable qualitative nominale (champ SDRNOM de la couche AD2021) d’une couche de polygones (figure 1.25).\n\n## Importation de la couche des aires de diffusion de 2021 pour la RMR de Sherbrooke\nAD2021 &lt;- st_read(dsn = \"data/chap01/gpkg/Recen2021Sherbrooke.gpkg\", \n                  layer = \"SherbAD\", \n                  quiet = TRUE)\n## Carte\ntmap_mode(\"plot\")\ntm_shape(AD2021)+\n  tm_fill(col= \"SDRNOM\", \n          palette = \"Set2\", \n          lwd = 1, \n          title =\"Municipalité\")+\n  tm_borders(col=\"black\")+\ntm_layout(main.title = \"Aires de diffusion de 2021\",\n          frame =FALSE,\n          legend.position = c(\"left\", \"top\"),\n          legend.outside=TRUE)\n\n\n\nFigure 1.25: Exemple de cartographie d’une variable qualitative sur des polygones\n\n\n\n\n1.5.1.4 Cartographie d’une variable discrète : cercles proportionnels\nLa syntaxe ci-dessous permet de créer une carte avec des cercles proportionnels pour les municipalités de la région administrative de l’Estrie (figure 1.26).\n\n## Importation des municipalités (subdivisions de recensements - SDR) de l'Estrie\nSDR.Estrie &lt;- st_read(dsn = \"data/chap01/gpkg/Recen2021Sherbrooke.gpkg\", \n                  layer = \"sdr_Estrie\", quiet = TRUE)\n## Importation des MRC (divisions de recensements - DR) de l'Estrie\nDR.Estrie &lt;- st_read(dsn = \"data/chap01/gpkg/Recen2021Sherbrooke.gpkg\", \n                  layer = \"DREstrie2021\",  quiet = TRUE)\n## Importation des données sur la population\nPopSDR &lt;- read.csv(\"data/chap01/tables/SDR_Estrie.csv\")\nPopSDR$SDRidu &lt;- as.character(PopSDR$SDRidu)\n## Fusion des données\nSDR.Estrie &lt;- merge(SDR.Estrie, PopSDR, by.x = \"SDRIDU\", by.y = \"SDRidu\")\n## Construction de la carte\ntmap_mode(\"plot\")\ntm_shape(SDR.Estrie)+\n  tm_polygons(col=\"whitesmoke\", border.col = \"grey30\", lwd = 1)+\n  tm_bubbles(size = \"SDRpop_2021\",\n             border.col = \"black\",\n             col = \"tomato1\",\n             title.size = \"Population\",\n             scale = 3)+ # facteur multiplicateur pour la taille du cercle\ntm_shape(DR.Estrie)+\n  tm_borders(col=\"black\", lwd = 2)\n\n\n\nFigure 1.26: Exemple de carte avec des cercles proportionnels\n\n\n\n\n1.5.1.5 Cartographie d’une variable continue : cartes choroplèthes et méthodes de discrétisation\nL’argument style, qui est commun à plusieurs fonctions (tm_polygons, tm_fill, tm_lines, tm_dots, etc.), permet de choisir une méthode de discrétisation dont les principales sont :\n\nfixed: intervalles fixés par l’analyste.\nequal: intervalles égaux.\npretty: intervalles arrondis aux nombres entiers.\nquantile: selon les quantiles (même nombre d’observations dans chaque classe).\njenks: selon la méthode de Jenks.\nsd: selon l’écart-type.\n\nD’autres méthodes peuvent être utilisées comme kmeans, hclust, bclust, fisher, dpih, headtails et log10_pretty. En guise d’exemple, la figure 1.28 présente une discrétisation en cinq classes selon la méthode des quantiles. Notez aussi qu’il est possible de réaliser une carte avec un dégradé continu avec style = \"cont\" tel qu’illustré ci-dessous (figure 1.27).\n\n## Sélection des aires de diffusion de Sherbrooke\nAD2021.sherb &lt;- subset(AD2021, SDRNOM == \"Sherbrooke\")\n## Carte\ntmap_mode(\"plot\")\ntm_shape(AD2021.sherb)+\n  tm_fill(col= \"HabKm2\", \n          palette = \"Reds\",  \n          style = \"cont\",\n          title =\"Hab./km2\")+\n  tm_borders(col=\"black\")\n\n\n\nFigure 1.27: Exemple de carte choroplèthe avec une palette continue\n\n\n\nLa figure 1.28 utilise une discrétisation selon la méthode de quantiles avec cinq classes. Autrement dit, chaque classe comprend 20 % des aires de diffusion de la ville de Sherbrooke.\n\ntmap_mode(\"plot\")\ntm_shape(AD2021.sherb)+\n  tm_fill(col= \"HabKm2\",\n          palette = \"Reds\",  \n          n = 5, # nombre de classes\n          style = \"quantile\",\n          legend.format = list(text.separator = \"à\"),\n          title =\"Hab./km2\")+\n  tm_borders(col=\"black\", lwd = .5)\n\n\n\nFigure 1.28: Exemple de carte choroplèthe avec une discrétisation selon les quantiles\n\n\n\nLa figure 1.29 présente quatre méthodes de discrétisation différentes appliquées au revenu médian des ménages par secteur de recensement dans la région métropolitaine de recensement de Sherbrooke en 2021.\n\n\nFigure 1.29: Différentes méthodes de discrétisation\n\n\n1.5.2 Cartes interactives\nAvec la fonction tmap_mode, il est possible de choisir l’un des deux modes de visualisation suivants :\n\nstatique avec tmap_mode(\"plot\").\ninteractif avec tmap_mode(\"view\").\n\nVous constaterez ci-dessous que par défaut, trois fonds de carte sont disponibles dans la carte interactive, soit dans l’ordre Esri.WorldGrayCanvas, OpenStreetMap et Esri.WorldTopoMap.\n\n## Mode active tmap\ntmap_mode(\"view\")\n## Importation des couches\nArrond.sf = read_sf(\"data/chap01/shp/Arrondissements.shp\")\nInstallSR.sf = read_sf(\"data/chap01/shp/Installations_sportives_et_recreatives.shp\")\n## Carte\ntm_shape(InstallSR.sf)+ tm_dots(size = 0.05, shape = 21, col = \"red\")+\ntm_shape(Arrond.sf)+ tm_borders(col=\"black\", lwd= .5)\n\n\n\n\n\n\nIl est aussi possible de changer les fonds de carte avec la fonction tm_basemap tandis que la fonction tm_tiles permet de superposer une tuile (pour la toponymie par exemple) (tableau 1.3).\n\n\n\n\nTableau 1.3: Fonctions pour des cartes interactives\n\nFonction\nDescription\n\n\n\ntmap_mode\nChoisir le mode statique ou interactive\n\n\ntm_basemap\nSpécifier un fond de carte\n\n\ntm_tiles\nSpécifier une tuile de fond\n\n\n\n\n\n\nDans le code ci-dessous, nous utilisons uniquement deux fonds de carte. Remarquez les lignes avec l’argument popup.vars qui permet de définir les champs visibles dans la fenêtre surgissante (pop-up). Cliquez sur une installation sportive pour activer la fenêtre surgissante.\n\n## Carte\ntm_basemap(c(\"OpenStreetMap\", \"Esri.WorldTopoMap\"))+\ntm_shape(InstallSR.sf)+ \n  tm_dots(size = 0.05, shape = 21, col = \"red\",\n          # définition pour le pop-up (clic sur une installation)\n          popup.vars=c(\"Nom : \"=\"NOM\",\n                       \"Type : \" = \"TYPE\",\n                       \"Éclairage : \" = \"ECLAIRAGE\",\n                       \"Éclairage : \" = \"SURFACE\"),\n          id = \"OBJECTID\")+\ntm_shape(Arrond.sf)+ tm_borders(col=\"black\", lwd= .5)\n\n\n\n\n\n\n\n\n\n\n\nOù trouver des fonds de carte?\n\n\nUne liste des fonds de carte Leaflet est disponible au lien suivant.\n\n\n\n1.5.3 Mise en page d’une carte\nLes principales fonctions de mise en page d’une carte sont présentées au tableau 1.4.\n\n\n\n\nTableau 1.4: Fonctions d’habillage d’une carte\n\n\n\n\n\n\nFonction\nDescription\nPrincipaux arguments\n\n\n\ntm_facets\nCréer un élément tmap avec plusieurs vignettes\n\nby: groupé par colonne. nrow et ncol: nombres de lignes et de colonnes\n\n\ntmap_arrange\nFusionner plusieurs cartes dans une mise en page\n\nnrow et ncol: nombre de lignes et de colonnes\n\n\ntm_grid\nAjouter une grille de lignes de coordonnées (ex. long/lat)\n\nx et y: vecteurs pour les coordonnées\n\n\ntm_credits\nCréer un texte pour spéficier l’auteur.e ou la source de la carte\n\ntext: texte. size: taille du texte. fontfamily: police du texte\n\n\ntm_scale_bar\nCréer une échelle\n\nbreak: vecteur numérique pour l’échelle. position: position de l’échelle avec les valeurs left, center, right, bottom, top. Par exemple c(‘left’, ‘bottom’)\n\n\ntm_compass\nCréer une flèche du nord\n\ntype: type de flèche du Nord (‘arrow’, ‘4star’, ‘8star’, ‘radar’, ‘rose’)\n\n\ntm_logo\nAjouter un logo à une carte\n\nfile: chemin et nom du fichier ou URL\n\n\ntm_xlab\nAjouter un titre sur l’axe des X de la carte\n\ntext: nom de l’axe\n\n\ntm_ylab\nAjouter un titre sur l’axe des Y de la carte\n\ntext: nom de l’axe\n\n\ntm_layout\nSpécifier des éléments de mise en page de la carte\n\ntitle: titre de la carte\n\n\ntm_legend\nParamétrer la légende de la carte\n\nposition: position de la légende avec les valeurs left, center, right, bottom, top\n\n\n\ntmap_options\nParamétrer et conserver plusieurs options sur la carte\n\nunit: unités de mesures (‘imperial’, ‘km’, ‘m’, ‘mi’, and ‘ft’)\n\n\n\n\n\n\n\n1.5.3.1 Combinaison de plusieurs cartes\nTel que décrit dans le tableau 1.4, il existe deux fonctions pour combiner deux cartes : tmap_arrange et tm_facets.\nPour ceux et celles réalisant régulièrement des graphiques dans R avec ggplot2, tmap_arrange est très similaire à la fonction ggarrange du package ggpubr qui permet de fusionner plusieurs graphiques. Globalement, le principe est le suivant : vous réalisez deux cartes ou plus que vous combinez dans une même sortie avec tmap_arrange. Vous trouverez ci-dessous un exemple avec deux cartes (figure 1.30).\n\ntmap_mode(\"plot\")\n## Carte 1\nCarte1 =  tm_shape(SDR.Estrie)+\n            tm_polygons(col=\"whitesmoke\", border.col = \"grey30\", lwd = 1)+\n            tm_bubbles(size = \"SDRpop_2021\",\n                       border.col = \"black\",\n                       col = \"tomato1\",\n                       title.size = \"Population\",\n                       scale = 3)+ # facteur multiplicateur pour la taille du cercle\n          tm_shape(DR.Estrie)+ tm_borders(col=\"black\", lwd = 2)\n## Calcul de la densité de population\nSDR.Estrie$HabKm2 &lt;- as.numeric(SDR.Estrie$SDRpop_2021 / (st_area(SDR.Estrie) / 1000000))\n## Carte 2\nCarte2 =  tm_shape(SDR.Estrie)+\n              tm_fill(col= \"HabKm2\", \n                    palette = \"Reds\",  \n                    style = \"quantile\", n = 4,\n                    title =\"Hab./km2\",\n                    legend.format = list(text.separator = \"à\"))+\n              tm_borders(col=\"black\")+\n          tm_shape(DR.Estrie)+ tm_borders(col=\"black\", lwd = 2)\n## Combinaison des deux cartes\ntmap_arrange(Carte1, Carte2, ncol = 2, nrow = 1)\n\n\n\nFigure 1.30: Exemple de combinaisons de carte avec tmap_arrange\n\n\n\nQuant à la fonction tm_facets, elle permet de créer plusieurs cartes avec l’argument by. Prenons un exemple concret : vous disposez d’une couche géographique des municipalités du Québec et vous souhaitez réaliser une carte pour chaque région administrative. L’argument by = \"Region\" vous permet alors d’avoir une vignette par région. Dans l’exemple ci-dessous, nous avons cartographié la même variable (densité de population) pour différentes zones de la région métropolitaine de Sherbrooke (figure 1.31).\n\ntmap_mode(\"plot\")\n## Création d'une variable zone basée sur les noms des municipalités\nAD2021$Zone &lt;- ifelse(AD2021$SDRNOM == \"Sherbrooke\", \"A. Sherbrooke\", \"\") \nAD2021$Zone &lt;- ifelse(AD2021$SDRNOM %in% c(\"Compton\", \"Waterville\", \"Hatley\", \"North Hatley\"), \n                      \"B. Sud\", AD2021$Zone) \nAD2021$Zone &lt;- ifelse(AD2021$SDRNOM %in% c(\"Orford\", \"Magog\", \"Saint-Denis-de-Brompton\"), \n                      \"C. Est\", AD2021$Zone) \nAD2021$Zone &lt;- ifelse(AD2021$SDRNOM %in% c(\"Ascot Corner\", \"Val-Joli\", \"Stoke\"), \n                      \"C. Nord\", AD2021$Zone) \n## Création des cartes avec tm_facets\ntmap_mode(\"plot\")\ntm_shape(AD2021)+\n  tm_fill(col= \"HabKm2\",\n          palette = \"Reds\",  \n          n = 5, # nombre de classes\n          style = \"quantile\",\n          title =\"Hab./km2\",\n          legend.format = list(text.separator = \"à\"))+\n  tm_borders(col=\"black\", lwd = .5)+\n  tm_facets(by = \"Zone\")\n\n\n\nFigure 1.31: Premier exemple de combinaison de cartes avec tm_facets\n\n\n\nL’utilisation de tm_facets peut être également très utile pour comparer les distributions spatiales de points à différentes années (figure 1.32).\n\ntmap_mode(\"plot\")\n## Importation des incidents\nIncidents &lt;- st_read(\"data/chap01/shp/IncidentsSecuritePublique.shp\", quiet = TRUE)\n## Création des cartes avec tm_facets\ntmap_mode(\"plot\")\ntm_shape(Arrondissements) + \n  tm_polygons(col=\"wheat\", border.col = \"black\") +\ntm_shape(Incidents) +\n  tm_dots(shape=21, col=\"blue\", size=.2) +\ntm_facets(by = \"ANNEE\")\n\n\n\nFigure 1.32: Deuxième exemple de combinaisons de carte avec tm_facets\n\n\n\n\n1.5.3.2 Mise en page d’une carte\nNous reprenons la figure 1.33 et l’habillons en ajoutant une échelle (tm_scale_bar), une flèche du Nord (tm_compass), la source et l’auteur (tm_credits) et un titre (tm_layout) (figure 1.33).\n\n## Carte 1\ntmap_mode(\"plot\")\ntm_shape(SDR.Estrie)+\n            tm_fill(col= \"HabKm2\", palette = \"Greens\",  \n                    style = \"quantile\", n = 4,\n                    title =\"Hab./km2\",\n                    legend.format = list(text.separator = \"à\"))+\n            tm_bubbles(size = \"SDRpop_2021\", border.col = \"black\", col = \"tomato1\", scale = 3,\n                       title.size = \"Population\")+ \n            tm_borders(col=\"black\")+\n## Ajout de de la flèche du Nord\ntm_compass(position = c(\"right\", \"bottom\"), \n           size = 2)+\n## Ajout de l'échelle\ntm_scale_bar(breaks  = c(0, 25, 50),\n             position = c(\"right\", \"bottom\"))+\n## Ajout de la source\ntm_credits(\"Source : recensement de 2021, Statistique Canada\\nAuteur : Jéremy Lacartemplace.\", \n           position = c(\"right\", \"bottom\"),\n           size = 0.7,\n           align = \"right\") +\n## Légende  \ntm_legend(position = c(\"left\", \"top\"), \n          frame = FALSE, bg.color = \"white\")+\n## Modification de la mise en page\ntm_layout(main.title = \"Municipalités de l'Estrie\",\n          legend.outside = TRUE,\n          frame = FALSE)\n\n\n\nFigure 1.33: Habillage d’une carte\n\n\n\n\n\n\n\n\nAller plus loin avec tmap?\n\n\nNous avons abordé uniquement les principales fonctions et arguments pour l’habillage d’une carte. Plusieurs exemples de très belles cartes créées avec tmap sont disponibles aux ressources suivantes :\n\nL’excellente vignette intitulée tmap: get started!\nVisualizing Spatial Data in R with tmap.\nMaking Maps with R.\nLe chapitre Making maps with R du livre Geocomputation with R.\n\n\n\n\n1.5.4 Exportation d’une carte\nUne fois la carte finalisée, il est possible de l’exporter dans différents formats avec la fonction tmap_save :\n\nEn mode image (png, jpg, bmp, tiff) pour l’insérer dans un logiciel de traitement de texte (Word ou OpenOffice Writer) ou dans un éditeur LaTeX (Overleaf par exemple).\nEn mode vectoriel (PDF ou SVG) pour finaliser l’édition de la carte dans un logiciel de création graphique vectorielle (Illustrator par exemple).\nEn HTML dans lequel la carte sera intégrée selon le mode de visualisation interactive, sous la forme d’un widget Leaflet.\n\n\n## Transformation en long/lat\n## Carte 1\ntmap_mode(\"plot\")\nCarte1 &lt;- tm_shape(SDR.Estrie)+\n  tm_fill(col= \"HabKm2\", palette = \"Greens\", style = \"quantile\", n = 4, title =\"Hab./km2\")+\n  tm_bubbles(size = \"SDRpop_2021\", border.col = \"black\", col = \"tomato1\", scale = 3,\n                       title.size = \"Population\")+ \n  tm_borders(col=\"black\")+\n  tm_compass(position = c(\"right\", \"bottom\"), size = 2)+\n  tm_scale_bar(breaks  = c(0, 25, 50), position = c(\"right\", \"bottom\"))+\n  tm_credits(\"Source : recensement de 2021, Statistique Canada\\nAuteur : Jéremy Lacartemplace.\", \n           position = c(\"right\", \"bottom\"), size = 0.7, align = \"right\") +\n  tm_legend(position = c(\"left\", \"top\"), frame = FALSE, bg.color = \"white\")+\n  tm_layout(main.title = \"Municipalités de l'Estrie\", legend.outside = TRUE, frame = FALSE)\n\n## Exportation de la Carte1 au format png\ntmap_save(Carte1, filename = \"data/chap01/export/Carte1.png\", dpi = 600)\n## Exportation de la Carte1 au format PDF\ntmap_save(Carte1, filename = \"data/chap01/export/Carte1.pdf\")\n## Exportation de la Carte1 au format HTML\ntmap_save(Carte1, filename = \"data/chap01/export/Carte1.html\")"
  },
  {
    "objectID": "01-ManipulationDonneesSpatiales.html#sec-016",
    "href": "01-ManipulationDonneesSpatiales.html#sec-016",
    "title": "1  Manipulation des données spatiales dans R",
    "section": "\n1.6 Quiz de révision du chapitre",
    "text": "1.6 Quiz de révision du chapitre\n\n\n\n\n\nLa classe sf est composée de trois éléments :\n\n\nRelisez au besoin la section 1.1.1.4.1.\n\n\n\n\n\n\nsimple feature geometry (sfg) : géométrie d’une observation\n\n\n\n\n\n\n\nsimple feature column (sfc) : liste toutes les géométries d’une couche\n\n\n\n\n\n\n\ndata.fame : données attributaires\n\n\n\n\n\n\n\nraster : données images\n\n\n\n\n\n\n\n\n\n\nLaquelle de ces fonctions permet de changer la projection cartographique d’une couche géographique?\n\n\nRelisez au besoin le début de la section 1.2.1.\n\n\n\n\n\n\nst_crs(x)\n\n\n\n\n\n\n\nst_transform(x, crs)\n\n\n\n\n\n\n\nst_is_longlat(x)\n\n\n\n\n\n\n\n\n\n\nLaquelle de ces fonctions n’est pas une fonction géométrique sur une couche?\n\n\nRelisez au besoin la section 1.2.2.\n\n\n\n\n\n\nst_bbox(x)\n\n\n\n\n\n\n\nst_union(x)\n\n\n\n\n\n\n\nst_point_on_surface(x)\n\n\n\n\n\n\n\nst_crop(x, y, xmin, ymin, xmax, ymax)\n\n\n\n\n\n\n\nst_centroid(x)\n\n\n\n\n\n\n\n\n\n\nComparativement à l’algorithme de Douglas et Peucker, que permet l’algorithme de Visvalingam lors de la simplification des contours?\n\n\nRelisez au besoin la section 1.2.2.4.\n\n\n\n\n\n\nIl est plus rapide.\n\n\n\n\n\n\n\nIl permet de conserver les frontières.\n\n\n\n\n\n\n\n\n\n\nTous les points sont compris dans leur enveloppe convexe.\n\n\nRelisez au besoin la section 1.2.2.5.\n\n\n\n\n\n\nVrai\n\n\n\n\n\n\n\nFaux\n\n\n\n\n\n\n\n\n\n\nQuelles sont les quatre fonctions de mesures géométriques et de récupération des coordonnées géographiques?\n\n\nRelisez au besoin la section 1.2.4.\n\n\n\n\n\n\nst_area(x)\n\n\n\n\n\n\n\nst_length(x)\n\n\n\n\n\n\n\nst_distance(x,y)\n\n\n\n\n\n\n\nst_coordinates(x)\n\n\n\n\n\n\n\nst_union(x)\n\n\n\n\n\n\n\n\n\n\nQuelle est la différence entre les fonctions st_intersects(x, y) et st_intersection(x, y)?\n\n\nRelisez le deuxième encadré à la section 1.2.6.\n\n\n\n\n\n\nElles génèrent le même résultat.\n\n\n\n\n\n\n\nLa première est une requête spatiale, la seconde renvoie l’intersection entre deux couches.\n\n\n\n\n\n\n\nLa première renvoie l’intersection entre deux couches, la seconde est une requête spatiale.\n\n\n\n\n\n\n\n\n\n\nLaquelle des fonctions sf permet d’exporter des données vectorielles?\n\n\nRelisez au besoin la section 1.4.1.\n\n\n\n\n\n\nst_read()\n\n\n\n\n\n\n\nst_write()\n\n\n\n\n\n\n\nwriteRaster()\n\n\n\n\n\n\n\n\n\nVérifier votre résultat"
  },
  {
    "objectID": "01-ManipulationDonneesSpatiales.html#sec-017",
    "href": "01-ManipulationDonneesSpatiales.html#sec-017",
    "title": "1  Manipulation des données spatiales dans R",
    "section": "\n1.7 Exercices de révision",
    "text": "1.7 Exercices de révision\n\n\n\n\n\nExercice 1. Découpage des rues de l’arrondissement des Nations de la ville de Sherbrooke\n\n\nComplétez le code ci-dessous avec les étapes suivantes :\n\nRequête attributaire pour créer un objet sf avec uniquement l’arrondissement des Nations à partir de la couche Arrondissements et le champ NOM (voir la section 1.2.7.4).\nDécoupez les rues (Rues) sur le nouvel objet sf (voir la section 1.2.3).\n\n\nlibrary(sf)\n## Importation des deux couches\nArrond &lt;- st_read(\"data/chap01/shp/Arrondissements.shp\", quiet = TRUE)\nRues &lt;- st_read(\"data/chap01/shp/Segments_de_rue.shp\", quiet = TRUE)\n## Requête attributaire : création d'un objet sf pour l'arrondissement des Nations\ntable(Arrond$NOM)\nArrond.DesNations &lt;- subset(À compléter)\n## Découper les rues avec le polygone de l'arrondissement des Nations\nRues.DesNations &lt;- À compléter\n\nCorrection à la section 9.1.1.\n\n\n\n\n\n\n\nExercice 2. Calcul d’un nouveau champ\n\n\nCalculez un nouveau champ (DistHVKM) dans la couche des aires de diffusion (AD) (AD.RMRSherb) qui représente la distance en kilomètres entre l’hôtel de ville de Sherbrooke et les points des AD. Puis, cartographiez le champ DistHVKM en quatre classes selon la méthode de discrétisation par quantiles. Complétez le code ci-dessous avec les étapes suivantes :\n\nAjoutez un champ pour la distance (DistHVKM) dans la couche AD.RMRSherb (voir la section 1.2.4).\nCartographiez le champ DistHVKM en quatre classes selon la méthode des quantiles (voir la section 1.5.1.5).\n\n\nlibrary(sf)\nlibrary(tmap)\n## Importation des deux couches\nAD.RMRSherb &lt;- st_read(dsn = \"data/chap01/gpkg/Recen2021Sherbrooke.gpkg\", \n                       layer = \"SherbAD\", quiet = TRUE)\nHotelVille &lt;- data.frame(ID = 1, Nom = \"Hôtel de ville\",\n                         lon = -71.89306, lat = 45.40417)\nHotelVille &lt;- st_as_sf(HotelVille, coords = c(\"lon\",\"lat\"), crs = 4326)\n## Changement de projection avant de s'assurer que les deux couches ont la même\nHotelVille &lt;- st_transform(HotelVille, st_crs(AD.RMRSherb))\n## Ajout d'un champ pour la distance en km à l'hôtel de ville pour les secteurs de recensement\nAD.RMRSherb$DistHVKM &lt;- À compléter\n## Cartographie en quatre classes selon les quantiles\ntmap_mode(\"plot\")\ntm_shape(À compléter)+\n  tm_fill(À compléter)+\n  tm_borders(col=\"black\")\n\nCorrection à la section 9.1.2.\n\n\n\n\n\n\n\nExercice 3. Importation d’une couche shapefile\n\n\nImportez une couche shapefile pour les divisions de recensement et calculez la densité de population (nombre d’habitants au km2). Complétez le code ci-dessous avec les étapes suivantes :\n\nFaites une jointure attributaire entre la couche DR.Qc et la table DR.Data (voir la section 1.2.7.2).\nCalculez le champ HabKm2, soit la division entre les champs DRpop_2021 et SUPTERRE (voir la section 1.5.1.5).\n\n\nlibrary(sf)\n## Importation de la couche des divisions de recensement du Québec\nDR.Qc &lt;- st_read(dsn = \"data/chap01/gpkg/Recen2021Sherbrooke.gpkg\", \n                 layer = \"DivisionsRecens2021\", quiet = TRUE)\n## Importation du fichier csv des divisions de recensement\nDR.Data &lt;- read.csv(\"data/chap01/tables/DRQC2021.csv\")\n## Jointure attributaire avec le champ IDUGD\nDR.Qc &lt;- A compléter\n## Il y a déjà deux champs dans la table pour calculer la densité de population :\n## SUPTERRE : superficie en km2\n## DRpop_2021 : population en 2021\nDR.Qc$HabKm2 &lt;- A compléter\nhead(DR.Qc, n=2)\nsummary(DR.Qc$HabKm2)\n\nCorrection à la section 9.1.3.\n\n\n\n\n\n\n\nExercice 4. Coordonnées géographiques\n\n\nVous recevez les coordonnées en degrés (WGS84, EPSG : 4326) : -71.91688, 45.37579. Créez un point pour cette localisation et calculez la distance la séparant du tronçon autoroutier le plus proche. Complétez le code ci-dessous avec les étapes suivantes :\n\nFaites une requête attributaire pour créer un objet sf avec uniquement les tronçons autoroutiers à partir de la couche Rues et le champ TYPESEGMEN (voir la section 1.2.7.4).\nTrouvez l’identifiant du tronçon le plus proche avec la fonction st_nearest_feature (voir la section 1.2.6).\n\n\nlibrary(sf)\n## Importation du réseau de rues\nRues &lt;- st_read(\"data/chap01/shp/Segments_de_rue.shp\", quiet=TRUE)\nunique(Rues$TYPESEGMEN)\n## Sélection des tronçons autoroutiers\nAutoroutes &lt;- À compléter\n## Création d'une couche sf pour le point avec les coordonnées\n## en degrés (WGS84, EPSG : 4326) : -71.91688, 45.37579\nPoint1_sf &lt;- À compléter\n## Changement de projection avant de s'assurer que les deux couches ont la même\nPoint1_sf &lt;- st_transform(Point1_sf, st_crs(Autoroutes))\n## Trouver le tronçon autoroutier le plus proche avec la fonction st_nearest_feature\nPlusProche &lt;- À compléter\nprint(PlusProche)\nPoint1_sf$AutoroutePlusProche &lt;- as.numeric(st_distance(Point1_sf,\n                                                        Autoroutes[PlusProche,]))\ncat(\"Distance à l'autoroute la plus proche :\", Point1_sf$AutoroutePlusProche, \"m.\")\n## Zone tampon\nZoneTampon &lt;- st_buffer(Point1_sf, Point1_sf$AutoroutePlusProche)\n## Cartographie\ntmap_mode(\"view\")\ntm_shape(ZoneTampon)+\n  tm_borders(col= \"black\")+\ntm_shape(Autoroutes)+\n  tm_lines(col=\"red\")+\ntm_shape(Point1_sf)+\n  tm_dots(col= \"blue\", shape=21, size = .2)\n\nCorrection à la section 9.1.4.\n\n\n\n\n\n\nApparicio, Philippe et Jérémy Gelb. 2022. Méthodes quantitatives en sciences sociales : un grand bol d’R. FabriqueREL, Licence CC BY-SA. https://laeq.github.io/LivreMethoQuantBolR/.\n\n\nBivand, Roger, Edzer Pebesma et Virgilio Gomez-Rubio. 2013. Applied spatial data analysis with R, Second edition. Springer, New York. https://asdar-book.org/.\n\n\nBrewer, Cynthia A, Geoffrey W Hatchard et Mark A Harrower. 2003. « ColorBrewer in print: a catalog of color schemes for maps. » Cartography and geographic information science 30 (1): 5‑32. https://doi.org/10.1559/152304003100010929.\n\n\nDouglas, David H et Thomas K Peucker. 1973. « Algorithms for the reduction of the number of points required to represent a digitized line or its caricature. » Cartographica: the international journal for geographic information and geovisualization 10 (2): 112‑122. https://doi.org/10.3138/FM57-6770-U75U-7727.\n\n\nGarnier, Simon, Noam Ross, Robert Rudis, Antônio Pedro Camargo, Marco Sciaini et Cédric Scherer. 2021. viridis - Colorblind-Friendly Color Maps for R. s.n. https://sjmgarnier.github.io/viridis/.\n\n\nGiraud, Timothée et Nicolas Lambert. 2016. « cartography: Create and Integrate Maps in your R Workflow. » JOSS 1 (4). The Open Journal. https://doi.org/10.21105/joss.00054.\n\n\nHarrower, Mark et Cynthia A Brewer. 2003. « ColorBrewer.org: an online tool for selecting colour schemes for maps. » The Cartographic Journal 40 (1): 27‑37. http://dx.doi.org/10.1179/000870403235002042.\n\n\nHijmans, Robert J. 2022a. raster: Geographic Data Analysis and Modeling. R package version 3.5-15. https://CRAN.R-project.org/package=raster.\n\n\n———. 2022b. terra: Spatial Data Analysis. s.n. https://CRAN.R-project.org/package=terra.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer Palettes. s.n. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nPebesma, Edzer. 2018. « Simple Features for R: Standardized Support for Spatial Vector Data. » The R Journal 10 (1): 439‑446. https://doi.org/10.32614/RJ-2018-009.\n\n\nPebesma, Edzer et Roger Bivand. 2005. « Classes and methods for spatial data in R. » R News 5 (2): 9‑13. https://CRAN.R-project.org/doc/Rnews/.\n\n\nTennekes, Martijn. 2018. « tmap: Thematic Maps in R. » Journal of Statistical Software 84 (6): 1‑39. https://doi.org/10.18637/jss.v084.i06.\n\n\nVisvalingam, Maheswari et James D Whyatt. 1993. « Line generalisation by repeated elimination of points. » The cartographic journal 30 (1): 46‑51. https://doi.org/10.3138/FM57-6770-U75U-7727.\n\n\nWickham, Hadley. 2016. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org."
  },
  {
    "objectID": "02-Autocorrelation.html#sec-021",
    "href": "02-Autocorrelation.html#sec-021",
    "title": "2  Autocorrélation spatiale",
    "section": "\n2.1 Notion d’autocorrélation spatiale",
    "text": "2.1 Notion d’autocorrélation spatiale\nComprendre la configuration spatiale d’un phénomène donné est une démarche fondamentale en analyse spatiale. Or, l’autocorrélation spatiale permet d’estimer la corrélation d’une variable par rapport à sa localisation dans l’espace, soit la dépendance spatiale. Autrement dit, elle permet de vérifier si les entités proches ou voisines ont tendance à être (dis)semblables en fonction d’un phénomène donné (soit une variable). Tel qu’illustré à la figure 2.1 (données fictives), on distingue trois formes d’autocorrélation spatiale :\n\n(a) Autocorrélation spatiale positive : lorsque les entités spatiales voisines ou proches se ressemblent davantage que celles non contiguës ou éloignées. Cela renvoie ainsi à la première loi de la géographie : « tout interagit avec tout, mais les objets proches ont plus de chance de le faire que les objets éloignés » (traduction libre) (Tobler 1970).\n(b) Autocorrélation spatiale négative : lorsque les entités spatiales voisines ou proches ont tendance à être dissemblables, comparativement à celles non contiguës ou éloignées.\n(c) Absence d’autocorrélation spatiale : lorsque les valeurs de la variable sont distribuées aléatoirement dans l’espace; autrement dit, lorsqu’il n’y a pas de relation entre le voisinage ou la proximité des entités spatiales et leur degré de ressemblance.\n\n\n\nFigure 2.1: Autocorrélation spatiale\n\n\n\n\n\n\nAnalyse de la figure 2.2\n\n\nQuelle est la variable pour laquelle le voisinage joue un rôle important dans sa distribution? L’autocorrélation pour cette variable est-elle positive ou négative? Pourquoi?\n\n\nFigure 2.2: Illustration de l’autocorrélation spatiale de deux variables pour les aires de diffusion de la ville de Sherbrooke\n\nRéponse : L’autocorrélation spatiale semble bien plus forte pour le pourcentage des logements construits avant 1960. Les aires de diffusion (AD) contiguës ou proches dans la partie centrale de la ville ont clairement des pourcentages élevés (rouge foncé) tandis que celles voisines ou proches dans les périphéries présentent des pourcentages faibles. Cela traduit donc une forte autocorrélation spatiale positive. Par contre, la distribution spatiale du pourcentage de personnes de 65 ans et plus semble plus aléatoire, traduisant ainsi une faible autocorrélation spatiale (dépendance spatiale).\n\n\nVous avez compris que la simple cartographie d’une variable vous donne une indication de l’autocorrélation spatiale. Pour contre, pour « chiffrer » l’intensité de l’autocorrélation spatiale, il convient de : 1) choisir une matrice de pondération spatiale (selon le voisinage ou la distance) (section 2.2), 2) calculer une mesure d’autocorrélation spatiale à partir de cette matrice (comme l’indice de Moran) (section 2.3)."
  },
  {
    "objectID": "02-Autocorrelation.html#sec-022",
    "href": "02-Autocorrelation.html#sec-022",
    "title": "2  Autocorrélation spatiale",
    "section": "\n2.2 Matrices de pondération spatiale",
    "text": "2.2 Matrices de pondération spatiale\nLes mesures d’autocorrélation spatiale visent à vérifier si les entités spatiales contiguës ou proches ont tendance à être semblables (autocorrélation positive) ou dissemblables (autocorrélation négative) en fonction d’un phénomène donné (en fonction d’une variable). Il convient donc avant tout de définir la manière de mesurer la relation d’adjacence ou de proximité entre deux entités spatiales.\nIl existe huit principales matrices de pondération spatiale regroupées en deux grandes catégories : celles de contiguïté (basées sur l’adjacence) et celles de proximité (basées sur la distance) (tableau 2.1). Lorsque la couche géographique est composée de points, seules les matrices de proximité peuvent être utilisées.\n\n\n\n\nTableau 2.1: Matrices de pondération spatiale selon la géométrie\n\n\n\n\n\n\n\n\nMatrice\nPoints\nLignes\nPolyg.\nRaster\n\n\n\nPartage d’un nœud (Queen)\n\nX\nX\nX\n\n\nPartage d’un segment (Rook)\n\nX\nX\nX\n\n\nPartage d’un nœud et ordre d’adjacence (Queen)\n\nX\nX\nX\n\n\nPartage d’un segment et ordre d’adjacence (Rook)\n\nX\nX\nX\n\n\nConnectivité selon la distance\nX\nX\nX\nX\n\n\nInverse de la distance\nX\nX\nX\nX\n\n\nInverse de la distance au carré\nX\nX\nX\nX\n\n\nNombre de plus proches voisins\nX\nX\nX\nX\n\n\n\n\n\n\n\n2.2.1 Matrices de contiguïté\nLa relation d’adjacence (de contiguïté) vise à déterminer si deux entités spatiales sont ou non voisines selon le partage soit d’un nœud, soit d’un segment (frontière commune). La contiguïté est liée à la notion de topologie qui prend en compte les relations de voisinage entre des entités spatiales, sans tenir compte de leurs tailles et de leurs formes géométriques. Elle peut être représentée à partir d’une matrice de contiguïté (avec une valeur de 1 quand deux entités sont voisines et de 0 pour une situation inverse) ou d’un graphe (formé de points représentant les entités spatiales et de lignes reliant les entités voisines) (figure 2.3).\n\n\nFigure 2.3: Relation topologique entre des entités spatiales polygonales\n\nTrois évaluations de la contiguïté sont représentées à la figure 2.4 :\n\nAdjacence selon le partage d’un segment, soit d’une frontière commune entre les polygones (A).\nAdjacence selon le partage d’un nœud (B).\n\nOrdre d’adjacence selon le partage d’un segment (C). L’ordre d’adjacence indique le nombre de frontières à traverser pour se rendre à l’entité spatiale contiguë, soit :\n\nOrdre 1 : une frontière à traverser pour se rendre dans l’entité spatiale adjacente.\nOrdre 2 : deux frontières à traverser pour atteindre les entités de la deuxième couronne.\nOrdre 3 : trois frontières à traverser pour atteindre les entités de la troisième couronne.\nEtc.\n\n\n\nBien entendu, les ordres d’adjacence peuvent être également définis selon le partage d’un nœud commun.\n\n\nFigure 2.4: Relations de voisinage et évaluation de la contiguïté\n\n\n\n\n\n\nApplicabilité des ordres d’adjacence\n\n\nLes matrices d’adjacence sont souvent utilisées dans les analyses de diffusion spatiale. Prenons un exemple concret : imaginons que le polygone en gris à la figure 2.4 est un parc. Nous pourrions évaluer le prix moyen des maisons dans les îlots qui font face au parc (ordre 1), toutes choses étant égales par ailleurs (superficie du terrain, superficie habitable, nombre de pièces, etc.). Puis, nous pourrions comparer ce prix moyen à ceux calculés pour les ordres suivants. Il est probable que le prix au premier ordre soit significativement plus élevé qu’au deuxième ordre, voire au troisième ordre. Autre exemple, nous pourrions réaliser un exercice similaire pour des maisons dans des îlots adjacents à un tronçon autoroutier. La relation est probablement inverse : un prix moyen plus bas à l’ordre 1 comparativement aux ordres suivants.\n\n\nHabituellement appelée \\(W\\), la matrice de contiguïté est binaire selon le partage tant d’un nœud (Queen en anglais) (équation 2.1) que d’un segment commun (Rook en anglais) (équation 2.2).\n\\[\nw_{ij} =\n\\begin{cases}\n   1 & \\text{si les entités spatiales }i \\text{ et }j \\text{ ont au moins un nœud commun; } i   \\ne j\\\\\n   0 & \\text{sinon}\n  \\end{cases}\n\\tag{2.1}\\]\n\\[\nw_{ij} =\n\\begin{cases}\n   1 & \\text{si les entités spatiales }i \\text{ et }j \\text{ partagent une frontière commune; } i   \\ne j\\\\\n   0 & \\text{sinon}\n  \\end{cases}\n\\tag{2.2}\\]\n\n\n\n\n\nExercice 1. Compléter des matrices de contiguïté\n\n\nPetit conseil pour la partie A : une matrice de contiguïté est symétrique c’est-à-dire que si le polygone A est voisin du polygone B, alors B est voisin de A! Par conséquent, pour gagner du temps, complétez une ligne et transposez-la en colonne.\n\n\nFigure 2.5: Exercice sur la contiguïté et les ordres d’adjacence\n\nCorrection à la section 9.2.1.\n\n\n\n2.2.2 Matrices de proximité spatiale\n\n2.2.2.1 Bref retour sur les différents types de distance\nPour calculer des mesures d’autocorrélation spatiale, nous pouvons aussi utiliser des matrices de pondération spatiale basées sur la proximité spatiale. Cette fois, nous ne cherchons pas à vérifier si les entités spatiales adjacentes se ressemblent, mais plutôt à vérifier si les entités spatiales proches les unes des autres se ressemblent. Pour ce faire, nous devons calculer les distances entre les entités spatiales.\nPour construire une matrice de pondération spatiale selon la proximité, nous pouvons utiliser plusieurs types de distance (Apparicio et al. 2017) : certaines sont cartésiennes, d’autres, dites réticulaires, sont calculées à partir d’un réseau de rues (figure 2.6).\nLes distances cartésiennes – euclidienne et de Manhattan (équation 2.3 et équation 2.4) – sont facilement calculables à partir des coordonnées géographiques (x,y) dans un SIG ou dans n’importe quel logiciel tableur, de statistique ou de gestion de base de données, etc. Pour cela, la couche géographique doit être dans un système de projection plane. La distance euclidienne représente ainsi la distance à vol d’oiseau entre deux points, tandis que la distance de Manhattan est la somme des deux côtés formant l’angle droit d’un triangle rectangle (l’hypoténuse, le plus grand des côtés du triangle, étant la distance euclidienne) (figure 2.6, a). Si la projection de la couche est sphérique (longitude/latitude), il convient d’utiliser la formule de haversine (basée sur la trigonométrie sphérique) pour obtenir la distance à vol d’oiseau (équation 2.5).\nPar contre, comme leurs noms l’indiquent, les distances réticulaires nécessitent un réseau de rues dans un SIG (notamment avec l’extension Network Analyst d’ArcGIS Pro) ou dans R (notamment avec le package R5R) pour calculer le chemin le plus rapide (chapitre 5).\n\\[\nd_{ij} = \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}\n\\tag{2.3}\\]\n\\[\nd_{ij} = \\lvert x_i-x_j \\rvert + \\lvert y_i-y_j \\rvert\n\\tag{2.4}\\]\n\\[\nd_{ij} = 2R \\cdot \\text{ arcsin} \\left( \\sqrt{\\text{sin}^2 \\left( \\frac{\\delta _i - \\delta _j}{2} \\right) + \\text{cos }\\delta _i \\cdot \\text{cos }\\delta _j \\cdot \\text{sin}^2 \\left( \\frac{\\phi _i - \\phi _j}{2} \\right)} \\right)\n\\tag{2.5}\\]\navec \\(R\\) étant le rayon de la terre; \\(\\delta _i\\) et \\(\\delta _j\\) les coordonnées de longitude pour les points \\(i\\) et \\(j\\); \\(\\phi _i\\) et \\(\\phi _j\\) les coordonnées de latitude pour les points \\(i\\) et \\(j\\).\n\n\nFigure 2.6: Les différents types de distance\n\n\n2.2.2.2 Matrice de distance binaire (de connectivité)\nÀ partir d’une matrice de distance entre les entités spatiales d’une couche géographique, il est possible de créer une matrice de pondération binaire (équation 2.6). Ce type de matrice est habituellement appelée matrice de connectivité. Il convient alors de fixer un seuil de distance maximal. Par exemple, avec un seuil de 500 mètres, \\(w_{ij}=1\\) si la distance entre les entités spatiales \\(i\\) et \\(j\\) est inférieure ou égale à 500 mètres; sinon \\(w_{ij}=0\\). Notez que pour des lignes et des polygones, la distance est habituellement calculée à partir de leurs centroïdes.\n\\[\nw_{ij} =\n\\begin{cases}\n   1 & \\text{si }d_{ij}\\leq{\\bar{d}}\\text{; } i \\ne j\\\\\n   0 & \\text{sinon}\n  \\end{cases}\n\\tag{2.6}\\]\navec \\(d_{ij}\\) étant la distance entre les entités spatiales \\(i\\) et \\(j\\), et \\(\\bar{d}\\) étant un seuil de distance maximal fixé par la personne utilisatrice (par exemple, 500 mètres).\nEn guise d’exemple, à la figure 2.7, seuls les polygones jaunes seraient considérés comme voisins du polygone bleu avec un seuil de distance maximal fixé à 2,5 kilomètres (valeur de 1); les roses se verraient affecter la valeur de 0.\n\n\nFigure 2.7: Illustration de la connectivité basée sur la distance\n\n\n2.2.2.3 Matrices basées sur la distance\nÀ partir d’une matrice de distance entre les entités spatiales, les pondérations peuvent être calculées avec l’inverse de la distance (\\(1/d_{ij}\\)) ou l’inverse de la distance au carré (\\(1/d_{ij^2}\\)) (équation 2.7). Analysons le graphique à la figure 2.8.\n\nPremièrement, nous constatons que plus la distance est grande, plus la valeur de la pondération est faible et inversement. De la sorte, nous accordons un rôle plus important aux entités spatiales proches les unes des autres que celles éloignées.\nDeuxièmement, les pondérations chutent beaucoup plus rapidement avec l’inverse de la distance au carré qu’avec l’inverse de la distance. Autrement dit, le recours à une matrice de pondération calculée avec l’inverse de la distance au carré a comme effet d’accorder un poids plus important aux entités géographiques très proches.\n\n\\[\nw_{ij} =\n\\begin{cases}\n  \\frac{1}{d_{ij}^{\\gamma}} &\\\\\n   0 & \\text{si } i=j\n  \\end{cases}\n\\tag{2.7}\\]\navec \\(\\gamma = 1\\) pour une matrice de l’inverse de la distance et \\(\\gamma = 2\\) pour l’inverse de la distance au carré.\n\n\n\n\nFigure 2.8: Comparaison des matrices inverse de la distance et inverse de la distance au carré\n\n\n\n\n\n\n\n\nPondération avec l’exponentielle inverse\n\n\nDans un excellent livre intitulé Économétrie spatiale appliquée des microdonnées, Jean Dubé et Diego Legros (2014) proposent de transformer la matrice des distances avec l’inverse de l’exponentielle (ou l’exponentielle négative de la distance) (équation 2.8). Comparativement à l’inverse de la distance au carré, cette opération fait chuter encore plus rapidement les pondérations.\n\\[\nw_{ij} =\n\\begin{cases}\n  \\frac{1}{e^{d_{ij}}} = e^{-d_{ij}} &\\\\\n   0 & \\text{si } i=j\n  \\end{cases}\n\\tag{2.8}\\]\n\n\nNotez que l’équation 2.7 peut être légèrement modifiée en introduisant un seuil maximal de la distance au-delà duquel les pondérations sont mises à 0 (équation 2.9). Autrement dit, cela permet de ne pas tenir compte des entités spatiales distantes à plus d’un seuil fixé par l’analyste, ce qui est particulièrement intéressant lorsque vous analysez un phénomène dont la diffusion (ou propagation) cesse ou est très minime au-delà d’une certaine distance. Par exemple, pour la propagation du bruit routier, le seuil de 300 mètres est souvent utilisé. Par conséquent, une mesure d’autocorrélation spatiale sur des mesures de bruit routier devrait probablement recourir à un seuil maximal de 300 mètres. Autre exemple, la superficie du territoire vital diffère selon les espèces animales (cerf, caribou, ours et loup, par exemple). Par conséquent, une ou un biologiste calculant des mesures d’autocorrélation spatiale risque aussi de fixer un seuil maximal différent selon l’espèce étudiée.\n\\[\nw_{ij} =\n\\begin{cases}\n   \\frac{1}{d_{ij}^{\\gamma}} & \\text{si }d_{ij}\\leq{\\bar{d}}\\\\\n   0 & \\text{si }d_{ij}&gt;{\\bar{d}}\\\\\n   0 & \\text{si } i=j\n  \\end{cases}\n\\tag{2.9}\\]\n\n\n\n\n\nCalculer le rayon maximal à partir d’une aire\n\n\nAdmettons que la superficie du territoire vital d’une espèce soit de 50 hectares, soit 0,5 km2 ou 500 000 m2. La formule bien connue pour calculer la superficie d’un cercle est \\(S = \\pi r^2\\) avec \\(S\\) et \\(r\\) étant respectivement la superficie et le rayon. Par conséquent, celle du rayon est \\(r = \\sqrt{\\frac{S}{\\pi}}\\). Pour trouver le rayon, vous devez taper sqrt(500000 / pi) dans la console de R et obtenir ainsi une distance de 398.9423 qui pourrait être arrondie à 400 mètres.\n\n\n\n2.2.2.4 Matrices selon le critère des plus proches voisins\nUne autre façon très utilisée pour définir une matrice de proximité à partir d’une matrice de distance consiste à retenir uniquement les n plus proches voisins. La matrice est aussi binaire avec les valeurs de 1 si les observations sont parmi les n plus proches de l’entité spatiale \\(i\\) et de 0 pour une situation inverse.\n\n2.2.3 Standardisation des matrices de pondération spatiale en ligne\nIl est recommandé de standardiser les matrices de pondération en ligne. La somme de la matrice de pondération sera alors égale au nombre d’entités spatiales de la couche géographique.\n\n\n\n\n\nQuel est l’intérêt de la standardisation?\n\n\nNous verrons dans les sections suivantes que ces matrices sont utilisées pour évaluer le degré d’autocorrélation spatiale globale et locale. Or, il est fréquent de comparer les valeurs des mesures d’autocorrélation spatiale obtenues avec différentes matrices d’adjacence et de proximité (contiguïté selon le partage d’un nœud, d’une frontière commune; inverse de la distance, inverse de la distance au carré, etc.). Autrement dit, la standardisation des matrices de pondération spatiale permet de vérifier si le degré de (dis)ressemblance des entités spatiales en fonction d’une variable donnée est plus fort avec une matrice de contiguïté, d’inverse de la distance ou encore d’inverse de la distance au carré, etc.\n\n\nPour illustrer comment réaliser une standardisation, nous utilisons une couche géographique comprenant peu d’entités spatiales, soit celle des quatre arrondissements de la ville de Sherbrooke (figure 2.9).\n\n\nFigure 2.9: Arrondissements de la ville de Sherbrooke\n\nAu tableau 2.2, différentes matrices de contiguïté et de distance ont été calculées, puis standardisées. Voici comment interpréter les différentes sections du tableau :\n\nContiguïté selon le partage d’une frontière commune. La valeur de 1 signale que deux arrondissements sont voisins, sinon la valeur est à 0. Tel qu’indiqué aux équation 2.1 et équation 2.2, un arrondissement ne peut être voisin de lui-même (ex.: valeur de 0 pour la cellule Bro. et Bro.). L’arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville (Bro.) a deux voisins, soit ceux des Nations et de Fleurimont (Nat. et Fle.), comme indiqué par la valeur 2 dans la colonne total. Par contre, les arrondissements des Nations et de Fleurimont sont voisins de tous les autres (valeur de 3 dans la colonne total).\nStandardisation de la matrice de contiguïté. Il suffit de diviser chaque valeur de la matrice de contiguïté par la somme de la ligne correspondante. De la sorte, la somme de chaque ligne est égale à 1 et la somme de l’ensemble des valeurs de la matrice est égale au nombre d’entités spatiales (ici 4).\nDistance (km). Nous avons calculé la distance euclidienne en kilomètres entre les centroïdes des arrondissements.\nInverse de la distance. Les valeurs sont obtenues avec la formule \\(1/_{dij}\\). Par exemple, entre Bro. et Nat., nous avons \\(1/7,9930 = 0,1251\\).\nInverse de la distance au carré. Les valeurs sont obtenues avec la formule \\(1/_{dij^2}\\). Par exemple, entre Bro. et Nat., nous avons \\(1/7,9930^2 = 0,0160\\).\nStandardisation de l’inverse de la distance. Comme précédemment, il suffit de diviser chaque valeur de la matrice par la somme de la ligne correspondante. Par exemple, pour Bro. et Nat., nous avons \\(0,1251 / 0,3241 = 0,3860\\). Remarquez que la somme des lignes est bien égale à 1.\nStandardisation de l’inverse de la distance au carré. Comme précédemment, il suffit de diviser chaque valeur de la matrice par la somme de la ligne correspondante. Par exemple, pour Bro. et Nat., nous avons \\(0,0160 / 0,0360 = 0,4440\\). Remarquez que la somme des lignes est bien égale à 1.\n\n\n\n\n\nTableau 2.2: Standardisation de matrices de pondération spatiale\n\nArrondissement\nBro.\nNat.\nLen.\nFle.\nSomme (lignes)\n\n\n\nMatrice de contiguïté selon le partage d'une frontière commune\n\n\nBro.\n0,0000\n1,0000\n0,0000\n1,0000\n2,0000\n\n\nNat.\n1,0000\n0,0000\n1,0000\n1,0000\n3,0000\n\n\nLen.\n0,0000\n1,0000\n0,0000\n1,0000\n2,0000\n\n\nFle.\n1,0000\n1,0000\n1,0000\n0,0000\n3,0000\n\n\nStandardisation de la matrice de contiguïté\n\n\nBro.\n0,0000\n0,5000\n0,0000\n0,5000\n1,0000\n\n\nNat.\n0,3330\n0,0000\n0,3330\n0,3330\n1,0000\n\n\nLen.\n0,0000\n0,5000\n0,0000\n0,5000\n1,0000\n\n\nFle.\n0,3330\n0,3330\n0,3330\n0,0000\n1,0000\n\n\nDistance (km)\n\n\nBro.\n0,0000\n7,9930\n18,9940\n16,1140\n\n\n\nNat.\n7,9930\n0,0000\n11,1190\n9,1650\n\n\n\nLen.\n18,9940\n11,1190\n0,0000\n9,2590\n\n\n\nFle.\n16,1140\n9,1650\n9,2590\n0,0000\n\n\n\nMatrice selon l'inverse de la distance\n\n\nBro.\n0,0000\n0,1251\n0,0526\n0,0621\n0,2398\n\n\nNat.\n0,1251\n0,0000\n0,0899\n0,1091\n0,3241\n\n\nLen.\n0,0526\n0,0899\n0,0000\n0,1080\n0,2505\n\n\nFle.\n0,0621\n0,1091\n0,1080\n0,0000\n0,2792\n\n\nMatrice selon l'inverse de la distance au carré\n\n\nBro.\n0,0000\n0,0160\n0,0030\n0,0040\n0,0230\n\n\nNat.\n0,0160\n0,0000\n0,0080\n0,0120\n0,0360\n\n\nLen.\n0,0030\n0,0080\n0,0000\n0,0120\n0,0230\n\n\nFle.\n0,0040\n0,0120\n0,0120\n0,0000\n0,0280\n\n\nStandardisation de l'inverse de la distance\n\n\nBro.\n0,0000\n0,5220\n0,2190\n0,2590\n1,0000\n\n\nNat.\n0,3860\n0,0000\n0,2770\n0,3370\n1,0000\n\n\nLen.\n0,2100\n0,3590\n0,0000\n0,4310\n1,0000\n\n\nFle.\n0,2220\n0,3910\n0,3870\n0,0000\n1,0000\n\n\nStandardisation de l'inverse de la distance au carré\n\n\nBro.\n0,0000\n0,6960\n0,1300\n0,1740\n1,0000\n\n\nNat.\n0,4440\n0,0000\n0,2220\n0,3330\n1,0000\n\n\nLen.\n0,1300\n0,3480\n0,0000\n0,5220\n1,0000\n\n\nFle.\n0,1430\n0,4290\n0,4290\n0,0000\n1,0000\n\n\n\n\n\n\n\n\n\n2.2.4 Construction de matrices de pondération spatiale dans R\n\n\n\n\n\nConstruction des matrices dans R  avec le packagespdep.\n\n\nLe package spdep dispose de différentes fonctions pour construire des matrices de contiguïté, de connectivité et de distance :\n\npoly2nb pour des matrices de contiguïté (section 2.2.4.1);\nnblag et nblag_cumul pour des matrices de contiguïté avec des ordres d’adjacence (section 2.2.4.2);\ndnearneigh pour des matrices de connectivité (section 2.2.4.3);\nas.matrix(dist(coords)) et mat2listw pour des matrices de distance (section 2.2.4.4);\nknn2nb pour des matrices selon le critère des plus proches voisins (section 2.2.4.5).\n\n\n\n\n2.2.4.1 Matrices de pondération spatiale selon la contiguïté\nPour créer des matrices de pondération spatiale selon la contiguïté décrites à la section 2.2.1, nous utilisons deux fonctions du package spdep :\n\npoly2nb(Nom de l'objet sf, queen=TRUE) crée une matrice de contiguïté sous la forme d’une classe nb (A neighbours list with class nb). Avec le paramètre queen=TRUE, la contiguïté est évaluée selon le partage d’un nœud; avec queen=FALSE, la contiguïté est évaluée selon le partage d’un segment (frontière). La matrice spatiale comprend une ligne par secteur de recensement avec les index des polygones adjacents. Par exemple, Queen[[1]] renvoie la liste des polygones voisins à la première entité spatiale, soit 2 14 15 16 23 32, c’est-à-dire six voisins.\nnb2listw(objet nb, zero.policy=TRUE, style = \"W\") crée une matrice de pondération spatiale à partir de n’importe quelle matrice spatiale (de contiguïté ou de distance). Le paramètre style = \"W\", qui est par défaut, permet de standardiser la matrice en ligne. Par exemple, W.Queen$weights[[1]] renvoie les valeurs des pondérations pour la première entité spatiale, soit 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 (0,1666667 = 1 / 6 voisin). Pour obtenir une matrice non standardisée, vous devez écrire style = \"B\", alors W.Queen$weights[[1]] renverra les valeurs de 1 1 1 1 1 1.\n\n\nlibrary(sf)    # pour importer des couches géographiques\nlibrary(spdep) # pour construire les matrices de pondération\n## Importation de la couche des secteurs de recensement de la ville de Sherbrooke\nSR &lt;- st_read(dsn = \"data/chap02/Recen2021Sherbrooke.gpkg\",\n              layer = \"DR_SherbSRDonnees2021\", quiet=TRUE)\n## Matrice selon le partage d'un nœud (Queen)\n# Création de la matrice spatiale\nQueen &lt;- poly2nb(SR, queen=TRUE)\n# Affichage de la première ligne de la matrice\nQueen[[1]]\n\n[1]  2 14 15 16 23 32\n\n# Création de la matrice de pondération avec une standardisation en ligne\nW.Queen &lt;- nb2listw(Queen, zero.policy=TRUE, style = \"W\")\n# Affichage de la première ligne des pondérations\nW.Queen$weights[[1]]\n\n[1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667\n\ncat(\"La somme de la première ligne de la matrice de pondération est égale à\",\n    sum(W.Queen$weights[[1]]))\n\nLa somme de la première ligne de la matrice de pondération est égale à 1\n\n## Matrice selon le partage d'un segment (Rook)\nRook &lt;- poly2nb(SR, queen=FALSE)\nW.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = \"W\")\n##  Comparaison des deux matrices de contiguïté\n# Résultat de la matrice de pondération (Queen)\nsummary(W.Queen)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 50 \nNumber of nonzero links: 272 \nPercentage nonzero weights: 10.88 \nAverage number of links: 5.44 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8 10 \n 1  2  2  9 13  9  8  5  1 \n1 least connected region:\n41 with 1 link\n1 most connected region:\n29 with 10 links\n\nWeights style: W \nWeights constants summary:\n   n   nn S0      S1       S2\nW 50 2500 50 20.3056 205.5251\n\n# Résultat de la matrice de pondération (Rook)\nsummary(W.Rook)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 50 \nNumber of nonzero links: 248 \nPercentage nonzero weights: 9.92 \nAverage number of links: 4.96 \nLink number distribution:\n\n 1  2  3  4  5  6  7  9 \n 1  2  4  9 17 11  5  1 \n1 least connected region:\n41 with 1 link\n1 most connected region:\n29 with 9 links\n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 50 2500 50 21.84674 205.0781\n\n\nLa syntaxe ci-dessous permet de visualiser et de comparer les graphes selon le partage d’un nœud (Queen) ou d’un segment commun (Rook).\n\npar(mfrow=c(1,2)) # permet d'avoir quatre graphiques (2x2)\ncoords &lt;- st_coordinates(st_centroid(SR))\n## Graphe selon le partage d'un nœud\nplot(st_geometry(SR), border=\"gray\", lwd=2, col=\"wheat\")\nplot(Queen, coords, add=TRUE, col=\"red\", lwd=2)\ntitle(main=\"Queen\", font.main= 1)\n## Graphe selon le partage d'une frontière commune\nplot(st_geometry(SR), border=\"gray\", lwd=2, col=\"wheat\")\nplot(Rook, coords, add=TRUE, col=\"red\", lwd=2)\ntitle(main=\"Rook\", font.main= 1)\n\n\n\n\n\n2.2.4.2 Matrices de pondération spatiale selon la contiguïté et un ordre d’adjacence\nPour décrire la construction des matrices de contiguïté avec un ordre d’adjacence (décrites à la section 2.2.1), nous utilisons une couche géographique comprenant peu d’entités spatiales, soit celle des quatre arrondissements de la ville de Sherbrooke (figure 1). Le code ci-dessous permet d’obtenir les résultats suivants :\n\nRook &lt;- poly2nb(Arrondissements, queen=FALSE): matrice d’ordre 1 selon le partage d’un segment.\nstr(Rook): pour chaque arrondissement, la liste des arrondissements adjacents d’ordre 1.\nRook.Ordre2 &lt;- nblag(Rook, 2): création d’une matrice d’ordre 2 avec la fonction nblag.\nstr(Rook.Ordre2[[1]]): liste des voisins d’ordre 1. Bien entendu, le résultat est identique à str(Rook).\nstr(Rook.Ordre2[[2]]): liste des voisins d’ordre 2.\nRook.Ordre2Cumule &lt;- nblag_cumul(Rook.Ordre2): fusion des deux listes en une seule avec la fonction nblag_cumul.\n\n\nArrondissements &lt;- st_read(\"data/chap02/Arrondissements.shp\", quiet=TRUE)\n## Matrice de contiguïté d'ordre 1 selon le partage d'un segment (Rook)\nRook &lt;- poly2nb(Arrondissements, queen=FALSE)\nstr(Rook)\n\nList of 4\n $ : int [1:2] 2 4\n $ : int [1:3] 1 3 4\n $ : int [1:2] 2 4\n $ : int [1:3] 1 2 3\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:4] \"1\" \"2\" \"3\" \"4\"\n - attr(*, \"call\")= language poly2nb(pl = Arrondissements, queen = FALSE)\n - attr(*, \"type\")= chr \"rook\"\n - attr(*, \"sym\")= logi TRUE\n\n## Matrice de contiguïté d'ordre 2 selon le partage d'un segment (Rook)\nRook.Ordre2 &lt;- nblag(Rook, 2)\n## Rook.Ordre2 comprend deux listes : l'une pour l'ordre 1 et l'autre pour l'autre 2.\nstr(Rook.Ordre2[[1]])\n\nList of 4\n $ : int [1:2] 2 4\n $ : int [1:3] 1 3 4\n $ : int [1:2] 2 4\n $ : int [1:3] 1 2 3\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:4] \"1\" \"2\" \"3\" \"4\"\n - attr(*, \"call\")= language poly2nb(pl = Arrondissements, queen = FALSE)\n - attr(*, \"type\")= chr \"rook\"\n - attr(*, \"sym\")= logi TRUE\n\nstr(Rook.Ordre2[[2]])\n\nList of 4\n $ : int 3\n $ : int 0\n $ : int 1\n $ : int 0\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:4] \"1\" \"2\" \"3\" \"4\"\n - attr(*, \"sym\")= logi TRUE\n\n## La fonction nblag_cumul permet de combiner les deux ordres dans une seule liste\nRook.Ordre2Cumule &lt;- nblag_cumul(Rook.Ordre2)\nstr(Rook.Ordre2Cumule)\n\nList of 4\n $ : int [1:3] 2 3 4\n $ : int [1:3] 1 3 4\n $ : int [1:3] 1 2 4\n $ : int [1:3] 1 2 3\n - attr(*, \"region.id\")= chr [1:4] \"1\" \"2\" \"3\" \"4\"\n - attr(*, \"call\")= language nblag_cumul(nblags = Rook.Ordre2)\n - attr(*, \"class\")= chr \"nb\"\n\n## Création de la matrice de pondération spatiale standardisée\nWRook.Ordre2Cumule &lt;- nb2listw(Rook.Ordre2Cumule, zero.policy=TRUE, style = \"W\")\n\nLa figure 2.10 permet de constater qu’au second ordre, chacun des arrondissements est relié aux trois autres.\n\n\nFigure 2.10: Adjacence de premier et de second ordre\n\nReprenons la couche des secteurs de recensement de la ville de Sherbrooke pour construire des matrices d’adjacence d’ordre 1 à 3.\n\n# Création des matrices d'ordre 1, 2 et 3\nQueen1 &lt;- poly2nb(SR, queen=TRUE)\nQueen2 &lt;- nblag_cumul(nblag(Queen1, 2))\nQueen3 &lt;- nblag_cumul(nblag(Queen1, 3))\n# Création des matrices\nW.Queen1 &lt;- nb2listw(Queen, zero.policy=TRUE, style = \"W\")\nW.Queen2 &lt;- nb2listw(Queen2, zero.policy=TRUE, style = \"W\")\nW.Queen3 &lt;- nb2listw(Queen3, zero.policy=TRUE, style = \"W\")\n\n\n2.2.4.3 Matrice de connectivité (matrice distance binaire)\nLa fonction dnearneigh(sf points, d1=, d2=) crée une matrice de connectivité (décrite à la section 2.2.2) à partir d’une couche de points. Les paramètres d1 et d2 permettent de spécifier le rayon de recherche (ex. : avec d1 = 0 et d2 = 2500, le seuil maximal de distance est de 2500 mètres).\nSi votre couche sf comprend des lignes ou des polygones, utilisez la fonction st_centroid ou st_point_on_surface() pour les convertir en points (section 1.2.2).\n\n## Conversion des polygones en points avec st_centroid\nSR.centroides &lt;- st_centroid(SR)\n## Matrice binaire avec un seuil de 2500 mètres\nConnect2500m &lt;- dnearneigh(SR.centroides, d1 = 0, d2 = 2500)\n## Matrice de pondération spatiale standardisée en ligne\nW.Connect2500m &lt;- nb2listw(Connect2500m, zero.policy=TRUE, style = \"W\")\n\n\n2.2.4.4 Matrices de pondération spatiale selon l’inverse de la distance et l’inverse de la distance au carré\nDans la section 2.2.3, nous avons présenté les matrices de l’inverse de la distance et de l’inverse de la distance au carré. Le code ci-dessous, qui permet de les créer, comprend les étapes suivantes :\n\nRécupération des coordonnées géographiques des entités spatiales.\nCréation de la matrice de distance euclidienne \\(n \\times n\\) (\\(n\\) étant le nombre d’entités spatiales de la couche).\nCalcul des matrices d’inverse de la distance et d’inverse de la distance au carré.\nStandardisation de ces deux matrices et transformation dans des objets listw avec la fonction mat2listw.\n\n\n## Coordonnées des centroïdes des entités spatiales\ncoords &lt;- st_coordinates(SR.centroides)\n## Création de la matrice de distance\ndistances &lt;- as.matrix(dist(coords, method = \"euclidean\"))\n# S'assurer que la diagonale de la matrice est à 0\ndiag(distances) &lt;- 0\n## Matrices inverse de la distance et inverse de la distance au carré\nInvDistances &lt;- ifelse(distances!=0, 1/distances, distances)\nInvDistances2 &lt;- ifelse(distances!=0, 1/distances^2, distances)\n## Matrices de pondération spatiale standardisées en ligne\nW_InvDistances  &lt;- mat2listw(InvDistances, style=\"W\")\nW_InvDistances2 &lt;- mat2listw(InvDistances2, style=\"W\")\n## Visualisation des valeurs des pondération pour la première entité spatiale\nround(W_InvDistances$weights[[1]],4)\n\n [1] 0.0688 0.0505 0.0377 0.0330 0.0220 0.0191 0.0152 0.0116 0.0155 0.0220\n[11] 0.0303 0.0382 0.0582 0.0677 0.0661 0.0366 0.0373 0.0316 0.0248 0.0123\n[21] 0.0178 0.0241 0.0055 0.0084 0.0083 0.0084 0.0106 0.0232 0.0125 0.0231\n[31] 0.0425 0.0192 0.0164 0.0049 0.0086 0.0071 0.0062 0.0061 0.0054 0.0049\n[41] 0.0078 0.0050 0.0032 0.0043 0.0036 0.0030 0.0035 0.0042 0.0037\n\n# La somme de la ligne est bien égale à 1\nsum(W_InvDistances$weights[[1]])\n\n[1] 1\n\n\n\n\n\n\n\nIntégration d’autres types de distance\n\n\nÀ la section 2.2.2.1, nous avons vu que plusieurs types de distances peuvent être utilisés : cartésiennes (euclidienne et de Manhattan) et réticulaires (chemin le plus rapide à pied, à vélo, en automobile et en transport en commun).\nPour construire une matrice de distance de Manhattan, vous devez changer la valeur du paramètre method de la fonction dist comme suit : as.matrix(dist(coords, method = \"manhattan\")).\nPour intégrer une distance réticulaire, vous devez la calculer, soit dans R (chapitre 5), soit dans un logiciel SIG (ArcGIS Pro avec l’extension Network Analyst par exemple) et l’importer dans R. Le reste du code sera alors identique.\n\n\nNous avons vu qu’il est possible d’utiliser une matrice de distance en fixant une distance maximale au-delà de laquelle les pondérations sont mises à 0 (équation 2.9 à la section 2.2.2.1). Le code ci-dessous permet de créer des matrices de pondération standardisées avec l’inverse de la distance et l’inverse de la distance au carré avec des seuils de 2500 et de 5000 mètres.\n\n## Coordonnées des centroïdes des entités spatiales\ncoords &lt;- st_coordinates(SR.centroides)\n## Création de la matrice de distance\ndistances &lt;- as.matrix(dist(coords, method = \"euclidean\"))\n## Création de différentes matrices avec différents seuils\nInvDistances.2500m  &lt;- ifelse(distances&lt;=2500 & distances!=0, 1/distances, 0)\nInvDistances.5000m  &lt;- ifelse(distances&lt;=5000 & distances!=0, 1/distances, 0)\nInvDistances2.2500m &lt;- ifelse(distances&lt;=2500 & distances!=0, 1/distances^2, 0)\nInvDistances2.5000m &lt;- ifelse(distances&lt;=5000 & distances!=0, 1/distances^2, 0)\n## Matrices de pondération spatiale standardisées en ligne\nW_InvDistances.2500 &lt;- mat2listw(InvDistances.2500m, style=\"W\")\nW_InvDistances.5000 &lt;- mat2listw(InvDistances.5000m, style=\"W\")\nW_InvDistances2.2500 &lt;- mat2listw(InvDistances2.2500m, style=\"W\")\nW_InvDistances2.5000 &lt;- mat2listw(InvDistances2.5000m, style=\"W\")\n\nSpécifier un seuil de distance peut toutefois être problématique. Par exemple, sur les 50 secteurs de recensement de la ville de Sherbrooke, 19 n’ont pas de voisins à 2500 mètres, indiqués par le résultat suivant :\n## 19 regions with no links:\n## 23 24 25 30 34 35 36 37 38 39 40 41 42 43 44 45 47 49 50\n\nsummary(W_InvDistances.2500, zero.policy=TRUE)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 50 \nNumber of nonzero links: 188 \nPercentage nonzero weights: 7.52 \nAverage number of links: 3.76 \n19 regions with no links:\n23 24 25 30 34 35 36 37 38 39 40 41 42 43 44 45 47 49 50\nLink number distribution:\n\n 0  1  2  3  4  6  7  8  9 10 11 12 13 \n19  6  2  2  4  2  2  3  4  1  2  1  2 \n6 least connected regions:\n21 26 31 33 46 48 with 1 link\n2 most connected regions:\n12 13 with 13 links\n\nWeights style: W \nWeights constants summary:\n   n  nn S0       S1       S2\nW 31 961 31 19.09079 128.6954\n\n\nMême avec un seuil de 5000 mètres, il reste encore 11 SR sans voisins.\n\nsummary(W_InvDistances.5000, zero.policy=TRUE)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 50 \nNumber of nonzero links: 532 \nPercentage nonzero weights: 21.28 \nAverage number of links: 10.64 \n11 regions with no links:\n35 36 37 39 40 41 42 43 47 49 50\nLink number distribution:\n\n 0  1  2  3  5  6  7  8  9 10 11 16 18 19 20 21 22 23 \n11  2  2  4  2  1  1  2  1  1  1  2  2  4  2  4  7  1 \n2 least connected regions:\n24 30 with 1 link\n1 most connected region:\n12 with 23 links\n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 39 1521 39 12.11283 162.9801\n\n\n\n\n\n\n\nRéduction de la taille des matrices de distance\n\n\nPlusieurs logiciels (notamment ArcGIS Pro et GeoDa) réduisent par défaut la taille des matrices de distance de la façon suivante : 1) construction d’une matrice de distance uniquement pour l’entité la plus proche (la matrice résultante est donc de dimension \\(n \\times 1\\)); 2) obtention de la distance maximale dans cette matrice, soit la distance la plus grande entre une entité spatiale et celle la plus proche; 3) construction de la matrice de distance finale avec comme seuil la distance maximale obtenue à l’étape précédente.\nCette réduction procure deux avantages importants :\n\nUne diminution considérable des temps de calcul, surtout pour les couches géographiques comprenant un nombre très élevé d’entités spatiales. Par exemple, avec une couche de 50 entités spatiales, la matrice des distances comprendra 2500 valeurs (50 \\(\\times\\) 50 = 2500) tandis qu’avec 1000 entités spatiales, elle en comprendra un million (1000 \\(\\times\\) 1000 = 1 000 000).\nComme décrit plus haut, il est préférable d’éviter d’avoir une matrice de distance avec des entités spatiales sans voisins, puisque cela a un impact négatif sur les mesures d’autocorrélation spatiale.\n\n\n\nLa syntaxe ci-dessous permet ainsi de construire des matrices de pondération (inverse de la distance et inverse de la distance au carré) à partir de la distance maximale et un SR et son voisin le plus proche.\n\n## Coordonnées des centroïdes des entités spatiales\ncoords &lt;- st_coordinates(SR.centroides)\n## Trouver le plus proche voisin\nk1 &lt;- knn2nb(knearneigh(coords))\n## Affichage des distances pour les 50 SR au le plus proche\nround(unlist(nbdists(k1,coords)),0)\n\n [1]  1352   563   563   659   833  1275  1275  1299  2136  1553  1171   833\n[13]   953   953  1024  1024   936   936  1149  1242  2378  1473  3863  4963\n[25]  2841  1755  1755  2294  1507  4002  1843  1710  2486  2977 10953  6965\n[37]  5331  4077  6717  6892  6892  6335  5639  3530  4202  1636  6662  1636\n[49] 10745 10034\n\n## Trouver la distance maximale       \nplusprochevoisin.max &lt;- max(unlist(nbdists(k1,coords)))\ncat(\"Distance maximale au plus proche voisin :\", round(plusprochevoisin.max,0), \"mètres\")\n\nDistance maximale au plus proche voisin : 10953 mètres\n\n## Matrice de distance avec la valeur maximale\n# les voisins les plus proches avec le seuil de distance maximal\nVoisins.DistMax &lt;- dnearneigh(coords, 0, plusprochevoisin.max)\n# Distances avec le seuil maximum\ndistances &lt;- nbdists(Voisins.DistMax, coords)\n# Inverse de la distance\nInvDistances &lt;- lapply(distances, function(x) (1/x))\n# Inverse de la distance au carré\nInvDistances2 &lt;- lapply(distances, function(x) (1/x^2))\n## Matrices de pondération spatiale standardisées en ligne\nW_InvDistances  &lt;- nb2listw(Voisins.DistMax, glist = InvDistances, style = \"W\")\nW_InvDistances2 &lt;- nb2listw(Voisins.DistMax, glist = InvDistances2, style = \"W\")\n\n\n2.2.4.5 Matrices de pondération spatiale selon le critère des plus proches voisins\nLa fonction knearneigh du package spdep crée des matrices de distance selon le critère des plus proches voisins (décrit à la section 2.2.2.4), dont le nombre est fixé avec le paramètre k.\n\n## Coordonnées géographiques des centroïdes des polygones\ncoords &lt;- st_coordinates(st_centroid(SR))\n## Matrices des plus proches voisins de 2 à 5\nk2 &lt;- knn2nb(knearneigh(coords, k = 2))\nk3 &lt;- knn2nb(knearneigh(coords, k = 3))\nk4 &lt;- knn2nb(knearneigh(coords, k = 4))\nk5 &lt;- knn2nb(knearneigh(coords, k = 5))\n## Matrices de pondération spatiale standardisées en ligne\nW.k2 &lt;-  nb2listw(k2, zero.policy=FALSE, style = \"W\")\nW.k3 &lt;-  nb2listw(k3, zero.policy=FALSE, style = \"W\")\nW.k4 &lt;-  nb2listw(k4, zero.policy=FALSE, style = \"W\")\nW.k5 &lt;-  nb2listw(k5, zero.policy=FALSE, style = \"W\")\n\nLa syntaxe ci-dessous permet de comparer les matrices des plus proches voisins de k = 2 à 5 (figure 2.11).\n\npar(mfrow=c(2,2))\nplot(st_geometry(SR), border=\"gray\", lwd=2, col=\"wheat\")\nplot(k2, coords, add=TRUE, col=\"red\", lwd=2)\ntitle(main=\"k = 2\")\n\nplot(st_geometry(SR), border=\"gray\", lwd=2, col=\"wheat\")\nplot(k3, coords, add=TRUE, col=\"red\", lwd=2)\ntitle(main=\"k = 3\")\n\nplot(st_geometry(SR), border=\"gray\", lwd=2, col=\"wheat\")\nplot(k4, coords, add=TRUE, col=\"red\", lwd=2)\ntitle(main=\"k = 4\")\n\nplot(st_geometry(SR), border=\"gray\", lwd=2, col=\"wheat\")\nplot(k5, coords, add=TRUE, col=\"red\", lwd=2)\ntitle(main=\"k = 5\")\n\n\n\nFigure 2.11: Matrices selon le critère des plus proches voisins"
  },
  {
    "objectID": "02-Autocorrelation.html#sec-023",
    "href": "02-Autocorrelation.html#sec-023",
    "title": "2  Autocorrélation spatiale",
    "section": "\n2.3 Autocorrélation spatiale globale",
    "text": "2.3 Autocorrélation spatiale globale\nDans le cadre de cette section, nous présentons uniquement les mesures d’autocorrélation spatiale globale les plus utilisées, à savoir le I de Moran pour évaluer l’autocorrélation spatiale d’une variable continue (section 2.3.1), les statistiques de comptage de jointure (Join Count Statistics) pour une variable binaire ou catégorielle (section 2.3.2) et l’indice de Lee pour évaluer l’autocorrélation spatiale de deux variables continues (section 2.3.3).\n\n2.3.1 Statistique du I de Moran\n\n2.3.1.1 Formulation du I de Moran\nPour évaluer le degré d’autocorrélation spatiale d’une variable continue, les deux principales statistiques utilisées sont le I de Moran (1950) et le c de Geary (1954). Puisqu’elles renvoient une seule valeur pour la variable continue de la couche géographique étudiée, elles sont qualifiées de mesures globales de l’autocorrélation spatiale, par opposition aux mesures locales qui renvoient une valeur par entité spatiale (section 2.4).\nNous présentons ici uniquement le I de Moran pour deux raisons principales. Premièrement, étant basée sur la covariance, son interprétation est bien plus facile que celle du c de Geary (basé sur la variance des écarts), c’est-à-dire qu’elle est très similaire au bien connu coefficient de corrélation de Pearson (Apparicio et Gelb 2022). Deuxièmement, elle constitue la mesure la plus utilisée. Le I de Moran s’écrit :\n\\[\nI = \\frac{n}{\\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij}} \\frac{\\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij}(x_i-\\bar{x})(x_j-\\bar{x})}{\\Sigma_{i=1}^n (x_i-\\bar{x})^2} \\text{ avec :}\n\\tag{2.10}\\]\n\nn, le nombre d’entités spatiales dans la couche géographique;\n\\(w_{ij}\\), la valeur de la pondération spatiale entre les entités spatiales \\(i\\) et \\(j\\);\n\\(x_i\\) et \\(x_j\\), les valeurs de la variable continue pour les entités spatiales \\(i\\) et \\(j\\);\n\\(\\bar{x}\\), la valeur moyenne de la variable \\(X\\) à l’étude.\n\n\n\n\n\n\nStandardisation de la matrice de pondération et I de Moran\n\n\nNous avons vu que si la matrice de pondération spatiale est standardisée en ligne (section 2.2.3), alors chaque ligne de la matrice vaut 1 et la somme de l’ensemble des valeurs de la matrice est égale au nombre d’entités spatiales (\\(n\\)). Or, dans l’équation 2.10), \\(\\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij}\\) représente la somme des pondérations de la matrice, soit \\(n\\) si elles sont standardisées en ligne. Puisque \\(\\frac{n}{n}=1\\), alors l’équation du I de Moran est simplifiée comme suit :\n\\[\nI = \\frac{\\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij}(x_i-\\bar{x})(x_j-\\bar{x})}{\\Sigma_{i=1}^n (x_i-\\bar{x})^2}\n\\tag{2.11}\\]\nComme évoqué dans la section 2.2.3, cela démontre l’intérêt de la standardisation : la comparaison des valeurs du I de Moran obtenues avec différentes matrices de contiguïté afin de sélectionner (éventuellement) celle avec laquelle la dépendance spatiale est la plus forte.\n\n\n\n2.3.1.2 Interprétation du I de Moran\nAvec une matrice standardisée, la statistique du I de Moran varie de -1 à 1 et s’interprète de la façon suivante :\n\nquand \\(I &gt; 0\\), l’autocorrélation est positive, c’est-à-dire que les entités géographiques ont tendance à se ressembler d’autant plus qu’elles sont voisines ou proches;\nquand \\(I = 0\\), l’autocorrélation est nulle, c’est-à-dire que la contiguïté ou la proximité spatiale des zones ne jouent aucun rôle;\nquand \\(I &lt; 0\\), l’autocorrélation est négative, c’est-à-dire que les entités géographiques ont tendance à être dissemblables d’autant plus qu’elles sont voisines ou proches.\n\nLes limites de -1 et 1 sont les maximums théoriques du I de Moran. Dans la pratique, elles sont limitées par la matrice spatiale utilisée dans le calcul. En effet, selon la matrice spatiale, le maximum du I de Moran peut être inférieur à 1, et son minimum supérieur à -1. Le calcul de ces bornes propres à chaque matrice spatiale peut se faire en utilisant les maximums et minimums des valeurs propres de \\(\\frac{W+W^T}{2}\\), quand la matrice spatiale est standardisée.\nÀ titre d’exemple, nous calculons ci-dessous les maximums et minimums possibles pour une matrice de contiguïté selon le partage d’un nœud (Queen) de nos secteurs de recensement.\n\n# Matrice de contiguïté selon le partage d'un nœud (Queen)\nQueen &lt;- poly2nb(SR, queen = TRUE)\nWQueen &lt;- nb2listw(Queen, style = 'W')\nQueenMat &lt;- listw2mat(WQueen)\nvalues &lt;- range(eigen((QueenMat + t(QueenMat))/2)$values)\nprint(round(values,2))\n\n[1] -0.74  1.02\n\n\nIl apparaît ainsi que pour la matrice matrice de contiguïté selon le partage d’un nœud (Queen), quelle soit la variable analysée, la valeur de I de Moran ne pourra pas dépasser les limites -0,74 et 1,02.\n\n2.3.1.3 Significativité du I de Moran\nComme pour le coefficient de corrélation calculé entre deux variables, il est possible de tester la significativité de la valeur du I de Moran obtenue. Sans que nous détaillions les calculs de significativité, notez qu’il existe trois manières de tester la significativité :\n\nselon l’hypothèse de la normalité;\nselon l’hypothèse de la randomisation;\nselon des permutations Monte-Carlo (habituellement avec 999 échantillons).\n\n\n\n\n\n\nComment calculer les trois tests de significativité du I de Moran?\n\n\nPour une description détaillée du calcul des trois tests, consultez l’ouvrage de Jean Dubé et Diego Legros (2014).\n\n\n\n2.3.1.4 Mise en œuvre dans R\n\n\n\n\n\nCalcul du I de Moran dans R\n\n\nPour illustrer le calcul de I de Moran dans R, nous utilisons une couche des aires de diffusion (AD) de la ville de Sherbrooke. Les étapes suivantes sont réalisées :\n\nConstruire une panoplie de matrices de pondération spatiale selon la contiguïté, la connectivité, la proximité et le critère des plus proches voisins.\nComparer les valeurs de significativité (p) pour une variable continue (HabKm2).\nPour cette même variable, trouver avec quelle matrice la valeur du I de Moran est la plus forte.\nComparer les valeurs du I de Moran calculées sur plusieurs variables.\n\n\n\n\n2.3.1.4.1 Étape 1. Construction des matrices de pondération spatiale\n\nlibrary(sf)\nlibrary(spdep)\n## Importation de la couche des aires de diffusion de la ville de Sherbrooke\nAD.DR &lt;- st_read(dsn = \"data/chap02/Recen2021Sherbrooke.gpkg\",\n              layer = \"DR_SherbADDonnees2021\", quiet=TRUE)\n\n## Matrices de contiguïté\n##############################################\n## Partage d'un nœud (Queen)\nQueen &lt;- poly2nb(AD.DR, queen=TRUE)\nW.Queen &lt;- nb2listw(Queen, zero.policy=TRUE, style = \"W\")\n## Partage d'un segment (Rook)\nRook &lt;- poly2nb(AD.DR, queen=FALSE)\nW.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = \"W\")\n## Partage d'un segment (Rook) et ordres d'adjacence de 2 à 5\nRook2 &lt;- nblag_cumul(nblag(Rook, 2))\nRook3 &lt;- nblag_cumul(nblag(Rook, 3))\nRook4 &lt;- nblag_cumul(nblag(Rook, 4))\nRook5 &lt;- nblag_cumul(nblag(Rook, 5))\nW.Rook2 &lt;- nb2listw(Rook2, zero.policy=TRUE, style = \"W\")\nW.Rook3 &lt;- nb2listw(Rook3, zero.policy=TRUE, style = \"W\")\nW.Rook4 &lt;- nb2listw(Rook4, zero.policy=TRUE, style = \"W\")\nW.Rook5 &lt;- nb2listw(Rook5, zero.policy=TRUE, style = \"W\")\n\n## Matrice de connectivité\n##############################################\n## Matrice binaire avec un seuil de 2500 mètres\nConnect2500m &lt;-   dnearneigh(st_centroid(AD.DR), d1 = 0, d2 = 2500)\nW.Connect2500m &lt;- nb2listw(Connect2500m, zero.policy=TRUE, style = \"W\")\n\n## Matrices de proximité\n##############################################\n## Coordonnées géographiques et matrice de distance\ncoords &lt;- st_coordinates(st_centroid(AD.DR))\ndistances &lt;- as.matrix(dist(coords, method = \"euclidean\"))\ndiag(distances) &lt;- 0\n## Matrices inverse de la distance et inverse de la distance au carré\nInvDistances &lt;- ifelse(distances!=0, 1/distances, distances)\nInvDistances2 &lt;- ifelse(distances!=0, 1/distances^2, distances)\n## Matrices de pondération spatiale standardisées en ligne\nW.InvDistances  &lt;- mat2listw(InvDistances, style=\"W\")\nW.InvDistances2 &lt;- mat2listw(InvDistances2, style=\"W\")\n## Création de différentes matrices avec différents seuils\nInvDistances.2500m  &lt;- ifelse(distances&lt;=2500 & distances!=0, 1/distances, 0)\nInvDistances.5000m  &lt;- ifelse(distances&lt;=5000 & distances!=0, 1/distances, 0)\nInvDistances2.2500m &lt;- ifelse(distances&lt;=2500 & distances!=0, 1/distances^2, 0)\nInvDistances2.5000m &lt;- ifelse(distances&lt;=5000 & distances!=0, 1/distances^2, 0)\nW.InvDistances_2500 &lt;- mat2listw(InvDistances.2500m, style=\"W\")\nW.InvDistances_5000 &lt;- mat2listw(InvDistances.5000m, style=\"W\")\nW.InvDistances2_2500 &lt;- mat2listw(InvDistances2.2500m, style=\"W\")\nW.InvDistances2_5000 &lt;- mat2listw(InvDistances2.5000m, style=\"W\")\n## Matrice de distance réduite standardisée\nk1 &lt;- knn2nb(knearneigh(coords))\nplusprochevoisin.max &lt;- max(unlist(nbdists(k1,coords)))\nVoisins.DistMax &lt;- dnearneigh(coords, 0, plusprochevoisin.max)                            \ndistances &lt;- nbdists(Voisins.DistMax, coords)\nInvDistances &lt;- lapply(distances, function(x) (1/x))\nInvDistances2 &lt;- lapply(distances, function(x) (1/x^2))\nW_InvDistancesReduite  &lt;- nb2listw(Voisins.DistMax, glist = InvDistances, style = \"W\")\nW_InvDistances2Reduite &lt;- nb2listw(Voisins.DistMax, glist = InvDistances2, style = \"W\")\n\n## Matrice selon le critère des plus proches voisins\n#####################################################\n## Matrices des plus proches voisins de 2 à 5\nk2 &lt;- knn2nb(knearneigh(coords, k = 2))\nk3 &lt;- knn2nb(knearneigh(coords, k = 3))\nk4 &lt;- knn2nb(knearneigh(coords, k = 4))\nk5 &lt;- knn2nb(knearneigh(coords, k = 5))\n## Matrices de pondération spatiale standardisées en ligne\nW.k2 &lt;-  nb2listw(k2, zero.policy=FALSE, style = \"W\")\nW.k3 &lt;-  nb2listw(k3, zero.policy=FALSE, style = \"W\")\nW.k4 &lt;-  nb2listw(k4, zero.policy=FALSE, style = \"W\")\nW.k5 &lt;-  nb2listw(k5, zero.policy=FALSE, style = \"W\")\n\n\n2.3.1.4.2 Étape 2. Calcul du I de Moran et des trois tests de significativité\nCalculons la statistique du I de Moran sur la variable continue cartographiée à la figure 2.12.\n\n\n\n\nFigure 2.12: Densité de population, aires de diffusion de la ville de Sherbrooke\n\n\n\nLes fonctions moran.test et moran.mc du package spdep permettent de calculer le I de Moran selon les trois façons de tester la significativité :\n\n\nselon l’hypothèse de la normalité avec le paramètre randomisation = FALSE\n\nmoran.test(ObjetSf$Variable, listw=MatriceW, zero.policy=TRUE, randomisation = FALSE)\n\n\n\nselon l’hypothèse de la randomisation avec le paramètre randomisation = TRUE\n\nmoran.test(ObjetSf$Variable, listw=MatriceW, zero.policy=TRUE, randomisation = TRUE)\n\n\n\nselon des permutations Monte-Carlo (ci-dessus avec 999 permutations)\n\nmoran.mc(ObjetSf$Variable, listw=MatriceW, zero.policy=TRUE, nsim=999)\n\n\n\nBien entendu, dans les sorties des trois méthodes, la valeur du I de Moran est la même, contrairement à la valeur de p qui peut varier.\n\nmoran.test(AD.DR$HabKm2,          # nom de l'objet sf et de la variable continue\n           listw=W.Queen,         # nom de la matrice de pondération spatiale\n           zero.policy=TRUE,    \n           randomisation = FALSE) # significativité selon l’hypothèse de la normalité\n\n\n    Moran I test under normality\n\ndata:  AD.DR$HabKm2  \nweights: W.Queen    \n\nMoran I statistic standard deviate = 11.724, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.433714579      -0.004032258       0.001394035 \n\nmoran.test(AD.DR$HabKm2,          # nom de l'objet sf et de la variable continue\n           listw=W.Queen,         # nom de la matrice de pondération spatiale\n           zero.policy=TRUE,    \n           randomisation = TRUE)  # significativité selon l’hypothèse de la randomisation\n\n\n    Moran I test under randomisation\n\ndata:  AD.DR$HabKm2  \nweights: W.Queen    \n\nMoran I statistic standard deviate = 11.761, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.433714579      -0.004032258       0.001385364 \n\nmoran.mc(AD.DR$HabKm2,            # nom de l'objet sf et de la variable continue\n         listw=W.Queen,           # nom de la matrice de pondération spatiale \n         zero.policy=TRUE, \n         nsim=999)                # 999 permutations\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  AD.DR$HabKm2 \nweights: W.Queen  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.43371, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nNous calculons la mesure du I de Moran sur la variable continue cartographiée à la figure 2.13.\nLa statistique du I de Moran (I = 0,43, p &lt; 0,001) indique que la variable densité de population a une forte autocorrélation spatiale positive (figure 2.13), avec des valeurs fortes dans les aires de diffusion contiguës dans la partie centrale de la ville et des valeurs faibles dans les aires de diffusion contiguës dans les secteurs périphériques (figure 2.12).\n\n\nFigure 2.13: Résultats du I de Moran selon l’hypothèse de la loi normale\n\n\n2.3.1.4.3 Étape 3. Identification de la plus forte autocorrélation spatiale selon les différentes matrices\nLa syntaxe ci-dessous permet de calculer la statistique du I de Moran avec plusieurs matrices de pondération spatiale.\n\n## Création d'un vecteur pour les noms des matrices\nVecteurMatrices &lt;- c(\"W.Queen\", \"W.Rook\", \"W.Rook2\", \"W.Rook3\", \"W.Rook4\", \"W.Rook5\",\n                     \"W.Connect2500m\",\n                     \"W.InvDistances\", \"W.InvDistances2\",\n                     \"W_InvDistancesReduite\", \"W_InvDistances2Reduite\",\n                     \"W.InvDistances_2500\", \"W.InvDistances_5000\",\n                     \"W.InvDistances2_2500\",\"W.InvDistances2_5000\",\n                     \"W.k2\", \"W.k3\", \"W.k4\", \"W.k5\")\n## Création d'une liste pour toutes les matrices\nListeMatrices &lt;- list(W.Queen, W.Rook, W.Rook2, W.Rook3, W.Rook4, W.Rook5,\n                      W.Connect2500m,\n                      W.InvDistances, W.InvDistances2,\n                      W_InvDistancesReduite, W_InvDistances2Reduite,\n                      W.InvDistances_2500, W.InvDistances2_2500,\n                      W.InvDistances2_2500, W.InvDistances2_5000,\n                      W.k2, W.k3, W.k4, W.k5)\n## Vecteur pour le I de Moran et la valeur de p\nMoranI &lt;- c()\nPvalue &lt;- c()\ni&lt;-0\n## Boucle pour calculer le I de Moran avec la liste des matrices\nfor (e in ListeMatrices){\n   i&lt;-i+1\n   Test &lt;-moran.mc(AD.DR$HabKm2,\n                   listw=e, \n                   zero.policy=TRUE, \n                   nsim=999)\n   MoranI[i]&lt;-Test$statistic\n   Pvalue[i] &lt;- Test$p.value\n}\n# Création d'un DataFrame avec les valeurs du I de Moran et de p\nMoranData1 &lt;- data.frame(Matrices=VecteurMatrices,\n                         MoranIs=MoranI,\n                         Pvalues=Pvalue)\nprint(MoranData1)\n\n                 Matrices    MoranIs Pvalues\n1                 W.Queen 0.43371458   0.001\n2                  W.Rook 0.44970946   0.001\n3                 W.Rook2 0.32509097   0.001\n4                 W.Rook3 0.21527754   0.001\n5                 W.Rook4 0.12614476   0.001\n6                 W.Rook5 0.07129756   0.001\n7          W.Connect2500m 0.25583250   0.001\n8          W.InvDistances 0.10632882   0.001\n9         W.InvDistances2 0.27216034   0.001\n10  W_InvDistancesReduite 0.28566705   0.001\n11 W_InvDistances2Reduite 0.38836327   0.001\n12    W.InvDistances_2500 0.32115230   0.001\n13    W.InvDistances_5000 0.40350492   0.001\n14   W.InvDistances2_2500 0.40350492   0.001\n15   W.InvDistances2_5000 0.34988630   0.001\n16                   W.k2 0.51070049   0.001\n17                   W.k3 0.44458619   0.001\n18                   W.k4 0.44800959   0.001\n19                   W.k5 0.43874109   0.001\n\n\nLa lecture détaillée du tableau 2.3 permet d’avancer plusieurs constats intéressants :\n\nD’emblée, signalons que toutes les valeurs sont positives et significatives, témoignant d’une autocorrélation spatiale positive.\nConcernant les matrices de contiguïté, la dépendance spatiale est plus forte selon le partage d’un segment que d’un nœud (0,4497 contre 0,4337). Par conséquent, si nous devons choisir une matrice de contiguïté, il serait préférable d’utiliser celle définie selon le partage d’une chaîne (Rook).\nSans surprise, plus nous ajoutons des ordres d’adjacence, plus la valeur de la statistique du I de Moran est faible, passant de 0,3251 à 0,0713 du deuxième au cinquième ordre.\nLa valeur du I de Moran avec une matrice de connectivité avec 2500 mètres est de 0,2558. Elle est plus faible que celles de l’inverse de la distance et l’inverse de la distance au carré, avec le même seuil de 2500 mètres (0,3212 et 0,4035).\nConcernant les matrices de proximité, la méthode de l’inverse de la distance au carré, qui accorde un poids plus important aux entités spatiales très proches (comparativement à l’inverse de la distance), renvoie des valeurs toujours plus élevées, et ce, que la matrice soit complète ou réduite. Aussi, les matrices de distance réduites présentent toujours des valeurs plus fortes que celles complètes.\nConcernant les matrices selon le critère des plus proches voisins, l’autocorrélation spatiale diminue légèrement de k = 2 à k = 5. D’ailleurs, la valeur la plus forte est pour deux voisins (I = 0,5107). Toutefois, retenir uniquement deux voisins est discutable puisque les AD sont très majoritairement contiguës à plus de deux autres AD (sur les 249 AD, seuls 9 sont contiguës à deux autres AD selon le partage d’un segment). Pour le vérifier, tapez summary(W.Rook) et analysez le tableau sous la ligne Link number distribution.\n\n\n\n\n\nTableau 2.3: Résultats du I de Moran selon les différentes matrices\n\nNom\nDescription\nI de Moran\np (999 permutations)\n\n\n\nMatrices de contiguïté\n\n\nW.Queen\nPartage d’un nœud\n0,4337\n0,001\n\n\nW.Rook\nPartage d’un segment\n0,4497\n0,001\n\n\nMatrices de contiguïté selon le partage d’un segment et ordre d'adjacence\n\n\nW.Rook2\nOrdre 2\n0,3251\n0,001\n\n\nW.Rook3\nOrdre 3\n0,2153\n0,001\n\n\nW.Rook4\nOrdre 4\n0,1261\n0,001\n\n\nW.Rook5\nOrdre 5\n0,0713\n0,001\n\n\nMatrices de connectivité\n\n\nW.Connect2500m\n2500 mètres\n0,2558\n0,001\n\n\nMatrices de distance (complètes)\n\n\nW.InvDistances\nInverse de la distance\n0,1063\n0,001\n\n\nW.InvDistances2\nInverse de la distance au carré\n0,2722\n0,001\n\n\nMatrices de distance (réduites)\n\n\nW_InvDistancesReduite\nInverse de la distance\n0,2857\n0,001\n\n\nW_InvDistances2Reduite\nInverse de la distance au carré\n0,3884\n0,001\n\n\nMatrices de distance avec un seuil maximal\n\n\nW.InvDistances_2500\nInverse de la distance (2500 mètres)\n0,3212\n0,001\n\n\nW.InvDistances_5000\nInverse de la distance (5000 mètres)\n0,4035\n0,001\n\n\nW.InvDistances2_2500\nInverse de la distance au carré (2500 mètres)\n0,4035\n0,001\n\n\nW.InvDistances2_5000\nInverse de la distance au carré (5000 mètres)\n0,3499\n0,001\n\n\nMatrices selon le critère des plus proches voisins\n\n\nW.k2\n2 voisins\n0,5107\n0,001\n\n\nW.k3\n3 voisins\n0,4446\n0,001\n\n\nW.k4\n4 voisins\n0,4480\n0,001\n\n\nW.k5\n5 voisins\n0,4387\n0,001\n\n\n\n\n\n\n\n\nQuelle est la matrice avec laquelle la dépendance spatiale de la variable est la plus forte?\nPour la trouver, nous construisons un graphique avec les valeurs du I de Moran triées par ordre décroissant. La valeur la plus forte est obtenue avec la matrice selon les deux plus proches voisins, suivie de la matrice Rook. Quoi qu’il en soit, il serait plus judicieux de privilégier la matrice de contiguïté selon le partage d’un segment comme expliqué plus haut.\n\nlibrary(ggplot2)\nggplot(data=MoranData1, aes(x=reorder(Matrices,MoranIs), y=MoranIs)) +\n  geom_segment( aes(x=reorder(Matrices,MoranIs), \n                    xend=reorder(Matrices,MoranIs), \n                    y=0, yend=MoranIs)) +\n  geom_point( size=4,fill=\"red\",shape=21)+\n  xlab(\"Matrice de pondération spatiale\") +\n  ylab(\"I de Moran\")+\n  coord_flip()\n\n\n\nFigure 2.14: Valeurs du I de Moran selon les différentes matrices de pondération spatiale\n\n\n\n\n2.3.1.4.4 Étape 4. Comparaison des valeurs du I de Moran pour plusieurs variables avec la même matrice\nLa syntaxe ci-dessous permet de calculer la statistique du I de Moran pour plusieurs variables (figure 2.15) avec la même matrice de pondération spatiale (ici, matrice de contiguïté selon le partage d’un segment).\n\n\n\n\nFigure 2.15: Quatre variables sélectionnées pour les AD de la ville de Sherbrooke\n\n\n\n\n## Vecteur pour les variables à analyser\nlisteVars &lt;- c(\"PctLog1960_Av\", \"RevMedMenage\" , \"PctProprieta\", \"PctPop0_14\")\n\n## Boucle pour calculer le I de Moran pour les différentes variables\nMoranData2 &lt;- t(sapply(listeVars, function(e){\n   Test &lt;- moran.mc(AD.DR[[e]],\n                   listw=W.Rook,\n                   zero.policy=TRUE,\n                   nsim=999)\n   result &lt;- c(round(Test$statistic,4),\n               Test$p.value)\n}))\n\nMoranData2 &lt;- data.frame(MoranData2)\nnames(MoranData2) &lt;- c('MoranIs', 'Pvalues')\nMoranData2$Variable &lt;- listeVars\nrownames(MoranData2) &lt;- NULL\n\nprint(MoranData2)\n\n  MoranIs Pvalues      Variable\n1  0.7071   0.001 PctLog1960_Av\n2  0.6168   0.001  RevMedMenage\n3  0.6028   0.001  PctProprieta\n4  0.4474   0.001    PctPop0_14\n\n\nDe nouveau, en quelques lignes de code, il est aisé de réaliser un graphique pour comparer les valeurs du I de Moran pour les différentes variables (figure 2.16).\n\nlibrary(ggplot2)\n    ggplot(data=MoranData2, aes(x=reorder(Variable,MoranIs), y=MoranIs)) + \n    geom_segment( aes(x=reorder(Variable,MoranIs), xend=reorder(Variable,MoranIs), y=0, yend=MoranIs)) + \n    geom_point( size=4,fill=\"red\",shape=21)+ \n    xlab(\"Variable continue\") + ylab(\"I de Moran\")+ \n    coord_flip()\n\n\n\nFigure 2.16: Valeurs du I de Moran pour les quatre variables\n\n\n\n\n2.3.2 Statistiques de comptage de jointure (Join Count Statistics)\nPour évaluer l’autocorrélation spatiale d’une variable qualitative dichotomique (binaire) ou polychotomique (catégorielle), il convient d’utiliser les Join Count Statistics, qui peuvent être traduits par statistiques de comptage de jointure. Ces tests permettent de répondre à la question suivante : est-ce que le voisinage ou la proximité des entités spatiales augmente significativement les chances qu’elles partagent la même valeur (modalité) par rapport à ce que le hasard produirait (Cliff et Ord 1981)?\n\n\n\n\n\nAutocorrélation spatiale sur une variable qualitative\n\n\nCes statistiques permettent ainsi de vérifier si la distribution des modalités d’une variable binaire ou nominale est dispersée aléatoirement dans l’espace d’étude ou si elle tend à se regrouper spatialement. Voici quelques exemples applicatifs :\n\nVariable binaire (oui/non; absence/présence d’un phénomène) avec habituellement des valeurs de 0 ou 1. Dans une ville, les parcelles commerciales sont-elles distribuées aléatoirement ou tendent-elle à se regrouper?\nVariable qualitative nominale. À la suite d’une élection dans un pays donné, il s’agit de vérifier si les districts électoraux adjacents ont significativement été remportés par des personnes candidates du même parti. Autre exemple, la répartition d’espèces d’arbres sur un territoire donné est-elle aléatoire ou favorise-t-elle la proximité entre certaines espèces?\n\n\n\n\n2.3.2.1 Formulation des statistiques de comptage de jointure\n\n2.3.2.1.1 Application à une variable binaire\nPour décrire le fonctionnement de ces tests, nous utilisons deux situations fictives avec toutes deux 36 entités spatiales, dont 9 avec la valeur de 1 (noir, soit B par convention) et 27 avec la valeur de 0 (blanc, soit W par convention) (figure 2.17). La relation d’adjacence entre les entités spatiales est ici mesurée à partir d’une matrice de contiguïté selon le partage d’un segment qui est représentée avec un graphe à la figure 2.17 (b).\n\n\nFigure 2.17: Illustration de l’autocorrélation spatiale pour une variable binaire\n\n\n2.3.2.1.2 Comptage des jointures\nLe comptage des entités voisines partageant la même valeur (BB et WW, soit une autocorrélation spatiale positive) et inversement (WB, soit une autocorrélation spatiale négative) est réalisé comme suit :\n\n\nLe nombre de voisins partageant la valeur de 1 (autocorrélation positive) est obtenu avec l’équation 2.12. Lorsque deux entités ne sont pas adjacentes, alors \\(w_{ij}=0\\) et donc \\(w_{ij}x_ix_j = 0\\). Par contre, lorsqu’elles sont voisines, trois cas de figure sont possibles :\n\n\nToutes deux ont la valeur de 1, alors \\(x_ix_j=1\\times1 = 1\\).\nToutes deux ont la valeur de 0, alors \\(x_ix_j=0\\times0 = 0\\).\nElles ont des valeurs différentes, alors \\(x_ix_j=1\\times0 = 0\\).\n\n\n\n\\[\nO_{BB} = \\frac{1}{2} \\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij} x_i x_j \\text{ avec :}\n\\tag{2.12}\\]\nn étant le nombre d’entités spatiales dans la couche géographique; \\(w_{ij}\\) étant la valeur de la matrice de contiguïté non standardisée entre \\(i\\) et \\(j\\) (1 quand elles sont voisines, sinon 0), \\(x_i\\) et \\(x_j\\) étant les valeurs de la variable binaire (0 ou 1) pour les entités spatiales \\(i\\) et \\(j\\).\n\n\nLe nombre de voisins partageant la valeur de 0 (autocorrélation positive) est obtenu avec l’équation 2.13 avec les cas suivants lorsque les deux entités sont voisines :\n\nToutes deux avec la valeur de 1, alors \\((1-x_i) (1-x_j)=(1-1)\\times(1-1) = 0 \\times 0 = 0\\).\n\nToutes deux avec la valeur de 0, alors \\((1-x_i) (1-x_j)=(1-0)\\times(1-0) = 1 \\times 1 = 1\\).\nAvec des valeurs différentes, alors \\((1-x_i) (1-x_j)=(1-1)\\times(1-0) = 0 \\times 1 = 0\\).\n\n\n\n\\[\nO_{WW} = \\frac{1}{2} \\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij} (1-x_i) (1-x_j)\n\\tag{2.13}\\]\n\n\nLe nombre de voisins ne partageant pas la même valeur (autocorrélation négative) est obtenu avec l’équation 2.14 avec les cas suivants lorsque les deux entités sont voisines :\n\nToutes deux avec la valeur de 1, alors \\((x_i-x_j)^2=(1-1)^2 = 0\\).\nToutes deux avec la valeur de 0, alors \\((x_i-x_j)^2=(0-0)^2 = 0\\).\n\nAvec des valeurs différentes, alors \\((x_i-x_j)^2=(1-0)^2 = 1\\).\n\n\n\n\\[\nO_{BW} = \\frac{1}{2} \\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij} \\left(x_i-x_j \\right)^2\n\\tag{2.14}\\]\n\n\n\n\n\nNote\n\n\nPour les trois équations ci-dessus, les sommes sont divisées par 2 puisque les mêmes résultats sont obtenus entre les paires (\\(i,j\\)) et (\\(j,i\\)). La somme des voisins partageant les mêmes valeurs (\\(O_{BB}\\) et \\(O_{WW}\\)) et ayant des valeurs différentes (\\(O_{BW}\\)) est égale à la somme de la matrice de contiguïté (\\(S_0\\)) divisée par deux (équation 2.15).\n\\[\n  O_{BB} +  O_{WW} +  O_{BW} = \\frac{1}{2}S_0= \\frac{1}{2} \\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij}\n\\tag{2.15}\\]\n\n\nLes résultats des comptages pour les situations A et B (figure 2.17) sont présentés au tableau 2.4. Ils démontrent clairement que les regroupements des valeurs de 0 et 1 sont bien plus importants pour la situation A (BB = 12 et WW = 42) que celle de B (BB = 1 et WW = 33). Reste à vérifier si ces résultats sont significatifs, c’est-à-dire s’ils diffèrent de ce que le hasard produirait.\n\n\n\n\nTableau 2.4: Comptages des jointures\n\nSituation\nBB\nWW\nWB\nSomme\n\n\n\nA (forte autocorrélation spatiale)\n12\n42\n6\n60\n\n\nB (absence autocorrélation spatiale)\n1\n33\n26\n60\n\n\n\n\n\n\n\n2.3.2.1.3 Tests statistiques\nIl existe deux approches d’inférence pour déterminer si des observations voisines tendent à produire certaines paires de catégories (BB, WW ou WB) plus souvent que le hasard : les tests reposant sur la loi binomiale et les tests par permutations. Notez que la seconde approche est habituellement privilégiée.\nTests selon la loi binomiale\nCes tests permettent d’obtenir les valeurs attendues des paires BB, WW et WB pour une absence d’autocorrélation spatiale, puis des valeurs de Z (équation 2.16) et de p.\n\\[\nZ_{WW} = \\frac{O[WW] - E[WW]}{\\sqrt{\\text{Var}[WW]}} \\\\\nZ_{BB} = \\frac{O[BB] - E[BB]}{\\sqrt{\\text{Var}[BB]}} \\\\\nZ_{WB} = \\frac{O[WB] - E[WB]}{\\sqrt{\\text{Var}[WB]}}\\text{ avec :}\n\\tag{2.16}\\]\n\\(O[WW]\\), \\(O[BB]\\) et \\(O[WB]\\) étant les nombres de paires observées; \\(E[WW]\\), \\(E[BB]\\) et \\(E[WB]\\) étant les nombres de paires attendues et \\(\\text{Var}[WW]\\), \\(\\text{Var}[BB]\\) et \\(\\text{Var}[WB]\\) leurs variances selon la loi binomiale pour une distribution aléatoire. Pour une description détaillée des formules des valeurs attendues et des variances selon les deux types d’échantillons (indépendant et dépendant) et selon le type de matrice de pondération spatiale (standardisée ou non), consultez l’ouvrage de Wong et Lee (2005).\nAussi, le calcul des valeurs de Z repose sur deux cas de figure :\n\nÉchantillon dépendant (non free Sampling). Le nombre de valeurs possibles de chaque catégorie est défini en amont et ne peut pas changer. Prenons l’exemple suivant : nous souhaitons vérifier si deux restaurants voisins ont tendance à avoir une licence d’alcool (BB) ou non (WW). Si le nombre de licences d’alcool est réglementé, alors la probabilité d’en avoir une dépend du nombre de licences en circulation (échantillon dépendant).\nÉchantillon indépendant (free Sampling). Pour chaque entité spatiale, la probabilité d’avoir une catégorie est indépendante du nombre d’observations. En d’autres termes, il n’y a pas un nombre défini d’observations pour chaque catégorie (W ou B) qui sont réparties entre les entités spatiales (échantillon indépendant).\n\nBien distinguer ces deux configurations est essentiel, car elles affectent directement les valeurs des variances et par ricochet celles de Z et de p (voir les tableaux 2.5 et 2.4). Avec un test basé sur un échantillon indépendant pour la situation A, nous concluons que les valeurs de 1 et de 0 (BB et WW) ne sont pas distribuées aléatoirement puisque p &lt; 0,05, ce qui traduit une autocorrélation spatiale. Par contre, pour la situation B, toutes les valeurs de p sont supérieures à 0,05, ce qui ne nous permet pas de rejeter l’hypothèse nulle d’une distribution aléatoire selon la loi binomiale.\n\n\n\n\nTableau 2.5: Statistiques de comptage de jointures pour BB (tests basés sur la loi binomiale)\n\n\nO[BB]\nE[BB]\nVar[BB]\nZ\np\n\n\n\nA (échantillon dépendant)\n42\n33,43\n3,73\n4,439\n0,000\n\n\nA (échantillon indépendant)\n42\n33,75\n45,98\n1,217\n0,112\n\n\nB (échantillon dépendant)\n33\n33,43\n3,73\n-0,222\n0,588\n\n\nB (échantillon indépendant)\n33\n33,75\n45,98\n-0,111\n0,544\n\n\n\n\n\n\n\n\n\n\nTableau 2.6: Statistiques de comptage de jointures pour WW (tests basés sur la loi binomiale)\n\nSituation et test\nO[WW]\nE[WW]\nVar[WW]\nZ\np\n\n\n\nA (échantillon dépendant)\n12\n3,43\n2,09\n5,922\n0,000\n\n\nB (échantillon indépendant)\n12\n3,75\n6,98\n3,122\n0,001\n\n\nB (échantillon dépendant)\n1\n3,43\n2,09\n-1,678\n0,953\n\n\nB (échantillon indépendant)\n1\n3,75\n6,98\n-1,041\n0,851\n\n\n\n\n\n\nTests par permutations\nCette seconde approche d’inférence est très simple et consiste à :\n\nMélanger les observations du jeu de données de nombreuses fois (habituellement 999).\nPour chaque permutation, compter les jointures BB, WW et WB.\nEstimer la pseudo valeur de p à partir de l’équation 2.17. Cette valeur peut être interprétée comme la probabilité que le hasard génère plus souvent une paire (BB, WW ou WB) que ce qui est observé avec le jeu de données initial.\n\n\\[\n  \\text{Pseudo valeur de } p = (M+1)/ (R+1) \\text{ avec :}\n\\tag{2.17}\\]\n\\(M\\) étant le nombre de fois que le comptage de jointures (BB ou WW par exemple) est égal ou supérieur à la valeur de référence (\\(O_{BB}\\) ou \\(O_{WW}\\) par exemple) et \\(R\\) étant le nombre de permutations (habituellement 999).\nEn guise d’exemple, nous réalisons 999 permutations des valeurs de la situation A qui comprend 12 paires de BB, 42 de WW et 6 de WB. Les comptages de BB et WW pour ces 999 permutations sont représentés à la figure 2.18. Nous constatons qu’elles sont toujours inférieures aux valeurs de référence (BB = 12 et WW = 42; lignes bleues). Par conséquent, la pseudo valeur de p est égale à \\((0+1)/(999+1)=\\text{0,001}\\). Cela signifie que nous ne pouvons pas rejeter l’hypothèse nulle stipulant que les distributions des paires BB et WW sont dues au hasard.\n\n\n\n\nFigure 2.18: Résultats des 999 permutations pour la situation A\n\n\n\nEffectuons la même démarche pour la situation B avec une seule paire de BB et 33 paires de WW (figure 2.19) :\n\npour BB, il y a 984 permutations qui ont une valeur supérieure ou égale à 1. La pseudo valeur de p est égale à \\((984+1)/(999+1)=\\text{0,985}\\).\npour WW, il y a 653 permutations qui ont une valeur supérieure ou égale à 1. La pseudo valeur de p est égale à \\((653+1)/(999+1)=\\text{0,653}\\).\n\nPar conséquent, nous validons l’hypothèse nulle stipulant que les distributions des paires BB et WW sont dues au hasard pour la situation B.\n\n\n\n\nFigure 2.19: Résultats des 999 permutations pour la situation B\n\n\n\n\n\n\n\n\nComment réaliser une permutation aléatoire des valeurs d’un vecteur dans R?\n\n\nDans R, la fonction sample(x) permet de permuter les valeurs d’un vecteur.\n\n## Valeurs initiales pour les 36 observations (cas A)\nprint(Carres$CasA)\n\n [1] 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n## Valeurs permutées\nprint(sample(Carres$CasA))\n\n [1] 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1\n\n\n\n\n\n\n\n\n\nImportance de la matrice de voisinage pour le Join count Test\n\n\nQuelle que soit la méthode utilisée pour effectuer le Join count Test, le test est sensible à la définition de la matrice de voisinage, définie selon le partage d’un segment ou d’un nœud (Queen ou Rook). Elle affecte directement le nombre de paires observées et attendues et donc le niveau de significativité de l’autocorrélation spatiale.\n\n\n\n2.3.2.2 Mise en œuvre dans R\n\n\n\n\n\nCalcul des statistiques de comptage de jointure dans R\n\n\nPour illustrer le calcul des statistiques de comptage de jointure (Join Count Statistics) dans R, nous utilisons une couche des aires de diffusion (AD) de la ville de Sherbrooke afin de répondre à la question suivante : les AD avec une majorité de locataires et celles avec une majorité de propriétaires sont-elles significativement voisines les unes des autres (autocorrélation spatiale positive) à Sherbrooke?\n\n\nDans le code ci-dessous, nous importons la couche géographique, créons une variable binaire indiquant si l’aire de diffusion a ou non une majorité de locataires et finalement, cartographions cette variable. Sans surprise, les AD avec une majorité de locataires sont concentrées dans la partie centrale de la ville (figure 2.20).\n\n## Chargement des données des aires de diffusion de la ville de Sherbrooke\nADSherb &lt;- st_read(\"data/chap02/Recen2021Sherbrooke.gpkg\", \n                   layer=\"DR_SherbADDonnees2021\",\n                   quiet=TRUE)\n## Création de la variable binaire\nADSherb$maj_locataires &lt;- ifelse(ADSherb$Locataire &gt; ADSherb$Proprietaire, 1, 0)\nADSherb$maj_locataires &lt;- factor(ADSherb$maj_locataires,\n                                 levels =c(0,1),\n                                 labels=c(\"Propriétaires\", \"Locataires\"))\n## Cartographie de la variable binaire\ntmap_mode(\"plot\")\ntm_shape(ADSherb)+\n  tm_borders(col=\"black\", lwd=0.5)+\n  tm_fill(col=\"maj_locataires\", \n          palette=c(\"#ededed\", \"#706f6f\"),\n          title = \"Groupe majoritaire\")+\n  tm_layout(frame=FALSE)+\n  tm_scale_bar(breaks=c(0,5))+\n  tm_credits(\"Source : Statistique Canada, 2021.\", align = \"left\")\n\n\n\nFigure 2.20: Aires de diffusion avec une majorité de locataires ou de propriétaires, Sherbrooke, 2021\n\n\n\nPour calculer les statistiques de comptage de jointure, nous utilisons une matrice de contiguïté selon le partage d’un nœud (Queen). Notez que pour ces statistiques, il est préférable de ne pas standardiser la matrice spatiale (style = \"B\"), car nous souhaitons compter le nombre de paires formées par les deux catégories d’AD.\n\nQueen &lt;- poly2nb(ADSherb, queen = TRUE)\nWQueen &lt;- nb2listw(Queen, style = \"B\")\n\nPour réaliser les tests selon la méthode d’inférence basée sur la loi binomiale, nous utilisons la fonction joincount.test du package spdep. Le paramètre sampling = \"free\" permet de spécifier le calcul des variances selon un échantillon indépendant, puisque les nombres d’AD dans lesquelles les locataires ou les propriétaires sont majoritaires n’ont pas de limites fixes.\n\ntest1 &lt;- joincount.test(ADSherb$maj_locataires, listw = WQueen, sampling = \"free\")\nprint(test1)\n\n\n    Join count test under free sampling\n\ndata:  ADSherb$maj_locataires \nweights: WQueen \n\nStd. deviate for Propriétaires = 3.2832, p-value = 0.0005132\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n             244.0000              163.4227              602.3289 \n\n\n    Join count test under free sampling\n\ndata:  ADSherb$maj_locataires \nweights: WQueen \n\nStd. deviate for Locataires = 3.7367, p-value = 9.324e-05\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n             319.0000              214.8323              777.1427 \n\n\nLes résultats du test présentés à la figure 2.21 s’interprètent comme suit :\n\na. Valeurs observées : 244 AD sont voisines et majoritairement occupées par des propriétaires contre 319 pour les locataires.\nb. Valeurs attendues : 163,4227 pour une majorité de propriétaires et 214,8323 pour une majorité de locataires.\nc. variances pour les deux groupes selon la loi binomiale avec un échantillon indépendant (602,3289 et 777,1427).\nd. Valeurs Z, soit \\((244-\\text{163,4227})/\\sqrt{\\text{602,3289}}=\\text{3,2832}\\) et \\((319-\\text{214,8323})/\\sqrt{\\text{777,1427}}=\\text{3,7367}\\).\ne. Valeurs de p sont inférieures à 0,005, signalant que les deux modalités de la variable binaire sont significativement autocorrélées positivement selon la matrice de contiguïté.\n\n\n\nFigure 2.21: Résultats des statistiques de comptage de jointure avec la fonction joincount.test\n\nÀ des fins de comparaison, nous effectuons le calcul des statistiques de comptage de jointure avec l’approche d’inférence selon le test des permutations (999) avec la fonction joincount.mc.\n\ntest2 &lt;- joincount.mc(ADSherb$maj_locataires, listw = WQueen, nsim = 999)\nprint(test2)\n\n\n    Monte-Carlo simulation of join-count statistic\n\ndata:  ADSherb$maj_locataires \nweights: WQueen \nnumber of simulations + 1: 1000 \n\nJoin-count statistic for Propriétaires = 244, rank of observed\nstatistic = 1000, p-value = 0.001\nalternative hypothesis: greater\nsample estimates:\n    mean of simulation variance of simulation \n              163.1672               109.2556 \n\n\n    Monte-Carlo simulation of join-count statistic\n\ndata:  ADSherb$maj_locataires \nweights: WQueen \nnumber of simulations + 1: 1000 \n\nJoin-count statistic for Locataires = 319, rank of observed statistic =\n1000, p-value = 0.001\nalternative hypothesis: greater\nsample estimates:\n    mean of simulation variance of simulation \n              213.6436               128.2997 \n\n\nL’approche d’inférence basée sur les permutations signale aussi que les deux distributions sont significativement autocorrélées spatialement (p = 0,001).\n\n\n\n\n\nFonction joincount.multi\n\n\nPour une variable qualitative comprenant plus de deux modalités (catégories), utilisez la fonction joincount.multi du package spdep. Cependant, celle-ci ne renvoie pas de valeurs de p, mais uniquement des valeurs de Z et des variances pour chaque modalité. Il est possible d’obtenir des valeurs de p en utilisant la fonction pnorm et surtout en ajustant ces valeurs de p pour contrôler l’effet de la multiplication des tests de significativité avec la fonction p.adjust.\n\n\n\n2.3.3 Indice de Lee : autocorrélation spatiale dans un contexte bivarié\nÀ la section 2.3.1, nous avons vu que le I de Moran permet d’évaluer le degré d’autocorrélation spatiale d’une variable continue, c’est-à-dire de vérifier si les entités spatiales proches ou voisines ont tendance à avoir des valeurs (dis)similaires pour une seule variable continue. Il est logique de vouloir étendre cette analyse à un contexte bivarié : est-ce que deux variables continues partagent ou non une relation marquée par de l’autocorrélation spatiale? En d’autres termes, l’association spatiale bivariée cherche à capturer la présence d’un patron spatial commun pour les deux variables (Lee 2001).\n\n2.3.3.1 Formulation de l’indice de Lee\nIl est important de distinguer la corrélation (non spatiale) entre deux variables et l’autocorrélation spatiale entre deux variables. La première mesure à quel point deux variables tendent à avoir une relation linéaire positive ou négative, alors que la seconde mesure la présence d’un patron spatial commun. Prenons l’exemple d’un jeu de données fictives avec 25 observations et deux variables continues X et Y (tableau 2.7).\n\n\n\n\nTableau 2.7: Jeu de données fictives avec 25 observations et deux variables\n\nX\nY\n\n\n\n-3,47\n-9,99\n\n\n0,24\n1,36\n\n\n2,98\n8,92\n\n\n0,14\n3,40\n\n\n-1,68\n-2,95\n\n\n-1,64\n-7,23\n\n\n-1,19\n1,19\n\n\n0,19\n-2,13\n\n\n-1,24\n0,67\n\n\n0,47\n-3,26\n\n\n-2,51\n-10,69\n\n\n-0,25\n1,75\n\n\n0,59\n9,69\n\n\n2,82\n15,55\n\n\n1,23\n4,76\n\n\n1,58\n2,75\n\n\n2,27\n9,16\n\n\n2,11\n5,27\n\n\n-1,38\n-2,33\n\n\n-0,35\n-7,72\n\n\n-1,41\n-3,25\n\n\n-2,59\n-12,61\n\n\n1,78\n-4,69\n\n\n0,44\n2,00\n\n\n-1,49\n-8,32\n\n\n\n\n\n\nCes deux variables sont caractérisées par une forte corrélation linéaire positive avec un coefficient de corrélation de Pearson proche de 1 (figure 2.22).\n\n\n\n\n\nRetour sur la notion de corrélation entre deux variables continues.\n\n\nPour un rappel sur la notion de corrélation entre deux variables continues, notamment sur l’interprétation du coefficient de corrélation de Pearson, nous vous invitons à lire la section suivante (Apparicio et Gelb 2022).\n\n\n\n\n\n\nFigure 2.22: Relation entre deux variables continues et coefficients de corrélation de Pearson\n\n\n\nCependant, cette information relative à la corrélation entre les deux variables ne nous permet pas d’appréhender leurs caractéristiques spatiales. En effet, ce tableau de données peut correspondre aux différents patrons spatiaux. En examinant les cartes a à d de la figure 2.23, nous constatons l’absence de tout patron spatial apparent. Par contre, à la figure 2.23 (e), nous observons une configuration spatiale particulière : à la fois, une forte autocorrélation spatiale positive univariée de chacune des deux variables (X et Y) et une forte autocorrélation spatiale positive bivariée puisque les deux patrons spatiaux de X et Y sont similaires.\n\n\n\n\nFigure 2.23: Exemples de patrons spatiaux sans et avec autocorrélation spatiale dans un contexte bivarié\n\n\n\nMaintenant que nous avons fait la distinction entre la corrélation bivariée et l’autocorrélation spatiale bivariée, nous pouvons présenter un indicateur d’autocorrélation spatiale globale bivariée, soit l’indice de Lee (2001). Cet indicateur est construit à partir du produit de l’autocorrélation univariée de chacune des deux variables (numérateur) et de la corrélation de Pearson entre les versions spatialement décalées de ces variables (dénominateur). La formule de l’indicateur est la suivante :\n\\[\n\\begin{aligned}\nL_{X, Y}=\\frac{n}{\\sum_i\\left(\\sum_j v_{i j}\\right)^2} \\cdot \\frac{\\sum_i\\left[\\left(\\sum_j v_{i j}\\left(x_j-\\bar{x}\\right)\\right) \\cdot\\left(\\sum_j v_{i j}\\left(y_j-\\bar{y}\\right)\\right)\\right]}{\\sqrt{\\sum_i\\left(x_i-\\bar{x}\\right)^2} \\sqrt{\\sum_i\\left(y_i-\\bar{y}\\right)^2}}\n\\end{aligned}\n\\tag{2.18}\\]\nSi la matrice de pondération spatiale est standardisée en ligne, l’indice de Lee est simplifié comme suit :\n\\[\n\\begin{aligned}\nL_{X, Y}= \\frac{\\sum_i\\left[\\left(\\sum_j v_{i j}\\left(x_j-\\bar{x}\\right)\\right) \\cdot\\left(\\sum_j v_{i j}\\left(y_j-\\bar{y}\\right)\\right)\\right]}{\\sqrt{\\sum_i\\left(x_i-\\bar{x}\\right)^2} \\sqrt{\\sum_i\\left(y_i-\\bar{y}\\right)^2}}\n\\end{aligned}\n\\tag{2.19}\\]\nCet indicateur varie entre −1 (autocorrélation spatiale bivariée négative) et +1 (autocorrélation bivariée positive).\nNous présentons trois cas à la figure 2.24 pour illustrer son interprétation. Prenons trois variables X, Y et X’, avec X’ étant l’inverse spatial de X. Pour simplifier l’exemple, ces trois variables ne peuvent avoir que trois valeurs (1, 2 ou 3).\n\n\n\n\nFigure 2.24: Patrons spatiaux de X, Y et X’\n\n\n\nSi nous calculons l’autocorrélation spatiale entre toutes les paires de variables avec une matrice de contiguïté standardisée, nous obtenons la matrice au tableau 2.8. Les résultats démontrent que les variables X et Y partagent un patron d’autocorrélation spatiale positive modérée (0,327), alors que les variables X et X’ partagent un patron d’autocorrélation spatiale négative (−0,512). En d’autres termes, les endroits où X tend à avoir des regroupements d’observations avec des valeurs fortes, X’ tend à avoir des regroupements d’observations avec des valeurs faibles.\n\n\n\n\nTableau 2.8: Indice de Lee global\n\n\nX\nY\nX’\n\n\n\nX\n\n0,327\n-0,512\n\n\nY\n0,327\n\n-0,293\n\n\nX’\n-0,512\n-0,293\n\n\n\n\n\n\n\n\n\n\n\n\nSignificativité du test de Lee\n\n\nAu même titre que pour les indicateurs vus précédemment, il est possible de tester si l’indice de Lee obtenu est significativement différent de 0 avec des permutations Monte-Carlo.\n\n\n\n2.3.3.2 Mise en œuvre dans R\nPour décrire le calcul de l’indice de Lee dans R, nous utilisons le jeu de données spatiales LyonIris du package geocmeans qui comprend quatre variables environnementales : le bruit routier (Lden dB(A)) (Lden), le dioxyde d’azote (ug/m3) (NO2), les particules fines PM2,5 (PM25) et la canopée (%) (VegHautPrt) pour les îlots regroupés pour l’information statistique (IRIS) de l’agglomération lyonnaise (figure 2.25).\n\n\nFigure 2.25: Cartographie des variables du jeu de données LyonIris\n\nIl est facile de calculer l’indice de Lee en utilisant les fonctions lee.test(x, y, listw),lee.mc(x, y, listw) ou lee(x, y, listw, n) du package spdep. Par exemple, le code ci-dessous permet de le calculer pour les variables Lden et NO2.\n\nlibrary(geocmeans)\ndata(\"LyonIris\")\n## Matrice de pondération spatiale de contiguïté standardisée\nnb &lt;- poly2nb(LyonIris, queen = TRUE)\nWmat &lt;- nb2listw(nb, style = 'W')\n## Calcul de l'indice de Lee entre les deux variables\nlee.test(LyonIris$Lden, LyonIris$NO2, listw = Wmat)\n\n\n    Lee's L statistic randomisation\n\ndata:  LyonIris$Lden ,  LyonIris$NO2 \nweights: Wmat  \n\nLee's L statistic standard deviate = 17.135, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nLee's L statistic       Expectation          Variance \n     0.4072785543      0.1243825152      0.0002725625 \n\nlee.mc(LyonIris$Lden, LyonIris$NO2, listw = Wmat, nsim = 999)\n\n\n    Monte-Carlo simulation of Lee's L\n\ndata:  LyonIris$Lden ,  LyonIris$NO2 \nweights: Wmat  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.40728, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nSi vous analysez plusieurs variables simultanément, il est pertinent de construire à la fois une matrice de corrélation et une matrice d’autocorrélation spatiale bivariée. Pour cette dernière, la diagonale peut être remplacée par le I de Moran de chaque variable.\n\n## Liste des variables à analyser\nvars &lt;- c(\"Lden\",\"NO2\",\"PM25\",\"VegHautPrt\")\n## Calcul de la matrice de corrélation de Pearson\ncorr_mat &lt;- cor(st_drop_geometry(LyonIris)[vars])\n## Calcul d'une matrice d'autocorrélation spatiale bivariée (indice de Lee)\nbivar_autocor_mat &lt;- sapply(vars, function(x1){\n  row_values &lt;- sapply(vars, function(x2){\n    test &lt;- lee(LyonIris[[x1]], LyonIris[[x2]], listw = Wmat, n = nrow(LyonIris))\n    return(test$L)\n  })\n})\n# Remplacement de la diagonale par les valeurs du I de Moran\nmoranIs &lt;- sapply(vars, function(x){\n  moran(LyonIris[[x]], Wmat, n = nrow(LyonIris), S0 = nrow(LyonIris))[[1]]\n})\ndiag(bivar_autocor_mat) &lt;- moranIs\n\n\n\n\n\nTableau 2.9: Matrice de corrélation de Pearson\n\n\nLden\nNO2\nPM25\nVegHautPrt\n\n\n\nLden\n1,000\n0,623\n0,489\n-0,227\n\n\nNO2\n0,623\n1,000\n0,901\n-0,283\n\n\nPM25\n0,489\n0,901\n1,000\n-0,392\n\n\nVegHautPrt\n-0,227\n-0,283\n-0,392\n1,000\n\n\n\n\n\n\nSans surprise, les différents polluants sont corrélés positivement entre eux et moyennement corrélés négativement avec le couvert végétal. La corrélation la plus forte s’observe entre le dioxyde d’azote et les particules fines (r = 0,901; tableau 2.9).\nLa lecture de la diagonale du tableau 2.10 permet de constater que les deux polluants atmosphériques sont les plus fortement spatialement autocorrélés (I de Moran de 0,82 et 0,94). Leur valeur de l’indice de Lee est aussi très forte (0,79), indiquant qu’ils tendent à varier de façon conjointe dans l’espace. En revanche, l’autocorrélation spatiale entre le bruit environnemental (Lden) et le dioxyde d’azote (indice de Lee de 0,41) est à peine plus forte que celle entre le Lden et les particules fines (indice de Lee de 0,38) alors que l’écart de la corrélation entre ces paires de variables est bien plus important (respectivement de 0,62 et 0,49). La corrélation non spatiale marquée entre le bruit et le dioxyde d’azote ne se traduit que par une autocorrélation spatiale bivariée modérée (0,41). La géographie de ces deux pollutions ne peut donc se résumer à un patron commun.\n\n\n\n\nTableau 2.10: Matrice d’autocorrélation spatiale globale bivariée (indice de Lee)\n\n\nLden\nNO2\nPM25\nVegHautPrt\n\n\n\nLden\n0,561\n0,407\n0,379\n-0,231\n\n\nNO2\n0,407\n0,819\n0,789\n-0,283\n\n\nPM25\n0,379\n0,789\n0,935\n-0,412\n\n\nVegHautPrt\n-0,231\n-0,283\n-0,412\n0,585"
  },
  {
    "objectID": "02-Autocorrelation.html#sec-024",
    "href": "02-Autocorrelation.html#sec-024",
    "title": "2  Autocorrélation spatiale",
    "section": "\n2.4 Autocorrélation spatiale locale",
    "text": "2.4 Autocorrélation spatiale locale\nNous avons vu que les statistiques d’autocorrélation spatiale globale comme le I de Moran (section 2.3.1), les statistiques de comptage de jointures (section 2.3.2) et l’indice de Lee (section 2.3.3) renvoient une valeur pour l’ensemble de l’espace d’étude. Une fois démontrée la présence d’autocorrélation spatiale globale (positive ou négative), il est pertinent de réaliser une analyse de l’autocorrélation spatiale locale afin de vérifier si chaque entité spatiale est significativement (dis)semblable de celles voisines ou proches. Comme l’indique l’adjectif locale, les mesures d’autocorrélation spatiale locale renvoient des valeurs pour chacune des entités spatiales.\n\n2.4.1 Statistiques locales de Getis et Ord : repérer les points chauds et froids pour une variable continue\n\n2.4.1.1 Formulation des statistiques locale de Getis et Ord\nLes statistiques locales de Getis et Ord permettent d’évaluer la similarité d’une entité spatiale avec celles voisines ou proches en fonction d’une variable continue (Getis et Ord 1992; Ord et Getis 1995). Autrement dit, elles nous informent si les valeurs fortes et les valeurs faibles d’une variable continue se regroupent significativement dans l’espace, et ce, avec n’importe quel type de matrice de contiguïté, de proximité, de plus proches voisins, etc. Cartographier ces statistiques permet alors de vérifier simultanément l’existence d’agrégats spatiaux de valeurs fortes (points chauds) et d’agrégats spatiaux de valeurs faibles (points froids).\nIl existe deux versions légèrement différentes de ces mesures locales (Getis et Ord 1992; Ord et Getis 1995) :\n\n\\(G_i\\) tient compte uniquement des entités voisines ou proches à l’entité spatiale \\(i\\). Ainsi, \\(\\Sigma_{j=1}^n w_{ij}x_j\\) représente la moyenne pondérée (par les poids de la matrice de pondération spatiale standardisée en ligne) de la variable continue \\(X\\) dans les entités voisines ou proches.\n\\(G_i^*\\) tient compte à la fois des valeurs des entités voisines ou proches, mais aussi de celle de \\(i\\).\n\nToutefois, nous cartographions rarement les valeurs de \\(G_i\\) et de \\(G_i^*\\), mais plutôt celles de \\(Z(G_i)\\) et de \\(Z(G_i^*)\\) (équation 2.20 et équation 2.21) qui représentent la cote Z qui, lorsque positive, indique un agrégat de valeurs plus élevées que la moyenne, et qui, lorsque négative, indique un agrégat de valeurs plus faibles que la moyenne (Bivand et Wong 2018).\nÀ première vue, ces deux formules peuvent sembler quelque peu complexes! Retenez simplement que le numérateur est la différence entre \\(G_i\\) ou \\(G_i^*\\) et la valeur attendue de \\(G_i\\) ou de \\(G_i^*\\) pour une distribution aléatoire (soit globalement la moyenne de la variable), tandis que le dénominateur représente l’écart-type de \\(G_i\\) ou de \\(G_i^*\\).\n\\[\nZ(G_i) = \\frac{\\bigr[ \\Sigma_{j=1}^n w_{ij}x_j \\bigr] - \\bigr[ (\\Sigma_{j=1}^nw_{ij}) \\bar x_i \\bigr]}\n{s_i \\sqrt{ \\Bigr[ \\Bigl((n-1)\\Sigma_{j=1}^nw_{ij}^2-(\\Sigma_{j=1}^nw_{ij})^2\\Bigl) \\Bigr] / (n-1) } } = \\frac{G_i - \\mathbb{E}(G_i)}{\\sqrt{Var(G_i)}} \\text{, }i \\neq j\n\\tag{2.20}\\]\n\\[\\text{avec } \\bar x_i=\\frac{\\Sigma_{j=1}^nx_j}{n-1}  \\text{, et } s_i = \\sqrt{ \\frac{\\Sigma_{j=1}^nx_j^2}{n-1}-\\bar x^2} \\text{, } i \\neq j\\] \\[\nZ(G_i^*) = \\frac{\\bigr[ \\Sigma_{j=1}^n w_{ij}x_j \\bigr] - \\bigr[ (\\Sigma_{j=1}^nw_{ij}) \\bar x^* \\bigr]}\n{s^* \\sqrt{ \\Bigr[ \\Bigl((n-1)\\Sigma_{j=1}^nw_{ij}^2-(\\Sigma_{j=1}^nw_{ij})^2\\Bigl) \\Bigr] / (n-1) } } = \\frac{G_i^* - \\mathbb{E}(G_i^*)}{\\sqrt{Var(G_i^*)}}\\text{, tous }\n\\tag{2.21}\\]\n\\[\\text{avec } \\bar x^*=\\frac{\\Sigma_{j=1}^nx_j}{n}  \\text{, et } s^* = \\sqrt{ \\frac{\\Sigma_{j=1}^nx_j^2}{n}-\\bar x^{*2}} \\text{, tous } j\\] avec :\n\nn, le nombre d’entités spatiales dans la couche géographique;\n\\(w_{ij}\\), la valeur de la pondération spatiale entre les entités spatiales \\(i\\) et \\(j\\);\n\\(x_j\\), la valeur de variable continue \\(X\\) pour l’entité spatiale \\(j\\);\n\\(\\bar{x^*}\\), la valeur moyenne de la variable pour toutes les observations;\n\\(\\bar{x_i}\\), la valeur moyenne de la variable pour toutes les observations sauf \\(i\\).\n\n\n\n\n\n\nDeux manières de calculer \\(Z(G_i)\\) et \\(Z(G_i^*)\\) avec le package spdep\n\n\n\nla fonction localG vous renvoie les valeurs de cote Z (Z-score en anglais) des statistiques de Getis et Ord avec les formules décrites plus haut.\nla fonction localG_perm permet de les obtenir avec la méthode Monte-Carlo (avec habituellement 999 permutations).\n\nCartographie des cotes Z\nÀ partir des cotes Z, utilisez les bornes des classes suivantes et la palette de couleurs -RdBu :\n\nMinimum à -3,29 : point froid significatif avec p &lt; 0,001 (bleu foncé).\n-3,29 à -2,58 : point froid significatif avec p &lt; 0,01 (bleu).\n-2,58 à -1,96 : point froid significatif avec p &lt; 0,05 (bleu pâle).\n-1,96 à 1,96 : non significatif (gris).\n1,96 à 2,58 : point chaud significatif avec p &lt; 0,05 (rouge pâle).\n2,58 à 3,29 : point chaud significatif avec p &lt; 0,01 (rouge).\n3,29 à Maximum : point chaud significatif avec *p &lt;0 ,001 (rouge foncé).\n\nAvec la palette -RdBu, les points froids et les points chauds sont respectivement bleus et rouges comme les pastilles de votre robinet! Bref, un beau travail de plomberie!\nPour un rappel sur la cote Z et les valeurs de p associées, consultez ce lien.\n\n\n\n2.4.1.2 Mise en œuvre dans R\nLa syntaxe ci-dessous calcule les deux statistiques locales avec localG et localG_perm pour le revenu moyen des ménages pour les aires de diffusion de 2021 de la ville de Sherbrooke.\n\nRook1 &lt;- poly2nb(AD.DR, queen=FALSE)\n## Matrice de pondération\nW.RookG    &lt;- nb2listw(Rook1, zero.policy=TRUE)\nW.RookStar &lt;-nb2listw(include.self(Rook1), zero.policy=TRUE) # matrice incluant elle-même\n## Calcul du Z(Gi) et du Z(Gi*)\nlocalGetis     &lt;- localG(AD.DR$RevMedMenage, W.RookG)\nlocalGetisStar &lt;- localG(AD.DR$RevMedMenage, W.RookStar)\n## Calcul avec la méthode Monte-Carlo (999 permutations)\nlocalGetis.MC999     &lt;- localG_perm(AD.DR$RevMedMenage, W.RookG, nsim = 999)\nlocalGetis.StarMC999 &lt;- localG_perm(AD.DR$RevMedMenage, W.RookStar, nsim = 999)\n## Sommaires statistiques\nsummary(localGetis)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-3.80885 -1.70432 -0.09828 -0.04120  1.41123  3.91835 \n\nsummary(localGetisStar)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-4.04870 -1.88689 -0.13297 -0.03841  1.54641  4.10379 \n\nsummary(localGetis.MC999)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-3.80885 -1.70432 -0.09828 -0.04120  1.41123  3.91835 \n\nsummary(localGetis.StarMC999)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-4.04870 -1.88689 -0.13297 -0.03841  1.54641  4.10379 \n\n\nCartographions ces valeurs et repérons les regroupements de valeurs significativement fortes et faibles avec les statistiques de Getis et Ord (figure 2.26 et figure 2.27). Les quatre cartes sont très semblables et permettent de repérer un regroupement de valeurs faibles dans le centre-ville et un autre de valeurs fortes dans l’est de la ville.\n\n\n\n\nFigure 2.26: Points chauds et froids du revenu médian des ménages selon les statistiques de Getis et Ord (méthode classique)\n\n\n\n\n## Enregistrement les résultats dans deux champs de la couche des AD\nAD.DR$RevMed_localGetis.MC999 &lt;- localGetis.MC999\nAD.DR$RevMed_localGetis.StarMC999 &lt;- localGetis.StarMC999\n# Définition des intervalles et des noms des classes\nclasses.intervalles = c(-Inf, -3.29, -2.58, -1.96, 1.96, 2.58, 3.29, Inf)\nclasses.noms = c(\"Point froid (p = 0,05)\", \n                 \"Point froid (p = 0,01)\", \n                 \"Point froid (p = 0,001)\", \n                 \"Non significatif\",\n                 \"Point chaud (p = 0,05)\", \n                 \"Point chaud (p = 0,01)\", \n                 \"Point chaud (p = 0,001)\")\n# Création d'un champ avec les noms des classes\nAD.DR$RevMed_localGetis.MC999P &lt;- cut(AD.DR$RevMed_localGetis.MC999, \n                                breaks = classes.intervalles, \n                                labels = classes.noms)\nAD.DR$RevMed_localGetis.StarMC999P &lt;- cut(AD.DR$RevMed_localGetis.StarMC999, \n                                  breaks = classes.intervalles, \n                                  labels = classes.noms)\n## Cartographie pour le Z(Gi) Mont-Carlo\nCarte3 = tm_shape(AD.DR)+\n            tm_polygons(col =\"RevMed_localGetis.MC999P\", \n                        title=\"Z(Gi)\", palette=\"-RdBu\", lwd = 1)+\n            tm_layout(frame =FALSE)\n## Cartographie pour le Z(Gi)* Mont-Carlo\nCarte4 = tm_shape(AD.DR)+\n            tm_polygons(col =\"RevMed_localGetis.StarMC999P\",\n                        title=\"Z(Gi*)\", palette=\"-RdBu\", lwd = 1)+\n            tm_layout(frame =FALSE)+\n          tm_credits(\"Auteurs : Geoffroy et Jessie Chaux.\", \n           position = c(\"right\", \"bottom\"), size = 0.7, align = \"right\")\n## Composition avec les deux cartes\ntmap_arrange(Carte3, Carte4, ncol = 2, nrow = 1)\n\n\n\nFigure 2.27: Points chauds et froids du revenu médian des ménages selon les statistiques de Getis et Ord (999 permutations Monte-Carlo)\n\n\n\n\n2.4.2 Version locale du I de Moran\n\n2.4.2.1 Formulation de la version locale du I de Moran\nUne version locale du I de Moran (\\(I_i\\)) a été proposée par Luc Anselin (1995). Elle permet de vérifier si une entité spatiale est voisine ou proche – dépendamment de la matrice spatiale utilisée – d’entités spatiales avec des valeurs semblables (contexte d’autocorrélation spatiale locale positive) ou dissemblables (contexte d’autocorrélation spatiale locale négative). Le I de Moran local s’écrit :\n\\[\nI_i = \\frac{(x_i-\\bar{X})\\Sigma_{j=1}^n w_{ij}(x_j-\\bar{X})}\n{\\frac{1}{n} \\Sigma_{i=1}^n(x_i-\\bar{X})^2} =  \\frac{z_i \\Sigma_{j=1}^n z_j}{\\frac{1}{n}\\Sigma_{i=1}^n z_i^2} \\text{, } i \\ne j\n\\tag{2.22}\\]\navec :\n\n\n\\(z_i\\) étant la valeur de la variable continue centrée \\(X\\) pour l’entité spatiale \\(i\\), c’est-à-dire simplement l’écart de \\(i\\) à la moyenne de \\(X\\) (\\(x_i - \\bar X\\)).\n\n\\(z_j\\) étant la valeur de la variable centrée de \\(X\\) pour l’entité spatiale \\(j\\).\n\n\\(w_{ij}\\) étant la valeur de la matrice de pondération spatiale standardisée en ligne entre \\(i\\) et \\(j\\).\n\n\\(n\\) étant le nombre d’entités spatiales dans la couche géographique.\n\nComme pour la version globale, il est possible de tester la significativité du I de Moran local de manière classique (selon la loi normale) ou avec l’approche Monte-Carlo (avec habituellement 999 permutations).\n\n\n\n\n\nTest de significativité du I de Moran local.\n\n\nPour comprendre les différentes variantes pour tester la significativité (Anselin 1995; Sokal, Oden et Thomson 1998), consultez Bivand et Wong (2018) ou l’ouvrage de Dubé et Legros (2014).\n\n\n\n2.4.2.2 Mise en œuvre dans R\nLe calcul du I de Moran local s’opère avec les fonctions localmoran et localMoranI.mc du package spdep (voir la syntaxe ci-dessous). Comme pour la version globale, les résultats du I de Moran local sont les mêmes pour les deux fonctions, seules les valeurs de p peuvent varier.\n\n## Calcul du I de Moran local\nlocalMoranI &lt;- localmoran(AD.DR$RevMedMenage,  # variable\n                          W.RookG)             # matrice de pondération spatiale\n## Calcul du I de Moran local avec la méthode Monte-Carlo\nlocalMoranI.mc &lt;- localmoran_perm(AD.DR$RevMedMenage, # variable\n                             W.RookG,                 # matrice de pondération spatiale\n                             nsim = 999)              # nombre de permutations       \n## Sommaires statistiques\nsummary(localMoranI)\n\n       Ii                E.Ii                Var.Ii               Z.Ii        \n Min.   :-0.90053   Min.   :-3.054e-02   Min.   :0.0000003   Min.   :-1.9962  \n 1st Qu.: 0.08313   1st Qu.:-5.368e-03   1st Qu.:0.0370546   1st Qu.: 0.4539  \n Median : 0.49029   Median :-2.755e-03   Median :0.1339044   Median : 1.4404  \n Mean   : 0.61678   Mean   :-4.032e-03   Mean   :0.2149974   Mean   : 1.2503  \n 3rd Qu.: 0.97602   3rd Qu.:-8.287e-04   3rd Qu.:0.2510284   3rd Qu.: 2.1938  \n Max.   : 3.37625   Max.   :-8.000e-09   Max.   :1.8207164   Max.   : 3.9184  \n Pr(z != E(Ii))     \n Min.   :0.0000892  \n 1st Qu.:0.0282519  \n Median :0.1268505  \n Mean   :0.2467939  \n 3rd Qu.:0.3948293  \n Max.   :0.9833048  \n\nsummary(localMoranI.mc)\n\n       Ii                E.Ii               Var.Ii               Z.Ii        \n Min.   :-0.90053   Min.   :-0.077143   Min.   :0.0000003   Min.   :-2.0405  \n 1st Qu.: 0.08313   1st Qu.:-0.009712   1st Qu.:0.0384477   1st Qu.: 0.4443  \n Median : 0.49029   Median :-0.001289   Median :0.1406886   Median : 1.4076  \n Mean   : 0.61678   Mean   :-0.003918   Mean   :0.2181586   Mean   : 1.2414  \n 3rd Qu.: 0.97602   3rd Qu.: 0.002865   3rd Qu.:0.2605785   3rd Qu.: 2.1315  \n Max.   : 3.37625   Max.   : 0.046552   Max.   :1.8142256   Max.   : 3.8234  \n Pr(z != E(Ii))      Pr(z != E(Ii)) Sim Pr(folded) Sim      Skewness        \n Min.   :0.0001316   Min.   :0.0020     Min.   :0.0010   Min.   :-0.457154  \n 1st Qu.:0.0330465   1st Qu.:0.0240     1st Qu.:0.0120   1st Qu.:-0.212551  \n Median :0.1303801   Median :0.1400     Median :0.0700   Median :-0.075311  \n Mean   :0.2497461   Mean   :0.2545     Mean   :0.1273   Mean   :-0.008966  \n 3rd Qu.:0.3910429   3rd Qu.:0.3960     3rd Qu.:0.1980   3rd Qu.: 0.216325  \n Max.   :0.9895698   Max.   :0.9960     Max.   :0.4980   Max.   : 0.432950  \n    Kurtosis       \n Min.   :-0.46371  \n 1st Qu.:-0.26232  \n Median :-0.17371  \n Mean   :-0.15732  \n 3rd Qu.:-0.07312  \n Max.   : 0.41548  \n\n\nAvec la cartographie du I de Moran local, nous repérons localement l’autocorrélation spatiale positive (valeurs similaires, fortes ou faibles localement) et l’autocorrélation spatiale négative (valeurs dissemblables localement) (figure 2.28).\n\n## Ajout de champs pour le I de Moran local\nAD.DR$RevMed_IlocalMoran.MC999 &lt;- localMoranI.mc[, 1]\nAD.DR$RevMed_IlocalMoran.MC999p &lt;- localMoranI.mc[, 5]\n## Cartographie\ntmap_mode(\"plot\")\nCarte1 = tm_shape(AD.DR)+\n            tm_polygons(col =\"RevMed_IlocalMoran.MC999\", title=\"I Moran local\", \n                        style=\"cont\", palette=\"-RdBu\", lwd = 1)+\n         tm_layout(frame = FALSE)\n\nCarte2 = tm_shape(AD.DR)+\n              tm_polygons(col= \"RevMed_IlocalMoran.MC999p\", title=\"valeur de p\",\n                    palette = c(\"darkgreen\", \"green\", \"lightgreen\", \"gray\"),  lwd = 1, \n                    breaks = c(0, 0.001, 0.01, 0.05, Inf),\n                    legend.format = list(text.separator = \"à\"),\n                    title =\"En %\")+\n            tm_layout(frame = FALSE)\n## Combinaison des deux cartes\ntmap_arrange(Carte1, Carte2, ncol = 2, nrow = 1)\n\n\n\nFigure 2.28: Cartographie du I de Moran local et de la valeur de p associée\n\n\n\n\n2.4.3 Typologie basée sur le diagramme de Moran\n\n2.4.3.1 Formulation de la typologie basée sur le diagramme de Moran dans un contexte univarié\nLa typologie basée sur le diagramme de Moran a été proposée par Luc Anselin (1996). L’idée est fort simple, mais extrêmement efficace! Avant tout, la variable continue est centrée réduite (cote \\(z\\), z-score en anglais). Pour un rappel sur la cote \\(z\\), consultez la section suivante (Apparicio et Gelb 2022). La moyenne est ainsi égale à 0 et l’écart-type à 1. Puis, il s’agit de construire un nuage de points entre :\n\nLes valeurs de la variable centrée réduite (Z) sur l’axe des X pour les entités spatiales de la couche géographique.\nLes valeurs de la variable centrée réduite spatialement décalée obtenues avec les pondérations spatiales de la matrice \\(W\\) (voir l’encadré ci-dessous).\n\n\n\n\n\n\nComment calculer une variable spatialement décalée avec une matrice de pondération spatiale?\n\n\nÀ la figure 2.29, nous détaillons le calcul de la valeur d’une variable spatialement décalée pour l’entité spatiale A à partir d’une matrice de contiguïté (selon le partage d’un segment) standardisée en ligne. Notez que A est adjacente à quatre entités spatiales (b, c, d et e).\n\n\nFigure 2.29: Illustration du calcul d’une variable spatialement décalée\n\nDans R, la syntaxe est fort simple pour créer une cote \\(z\\) et une variable spatiale décalée :\nzx &lt;- (x - mean(x))/sd(x)     # variable X centrée réduite (cote z)\nwzx &lt;- lag.listw(listW,zx)    # variable X centrée réduite spatialement décalée\n\n\nAnalysons les différents éléments du diagramme de Moran à la figure 2.30 :\n\nLa droite de régression résume la relation linéaire entre la variable spatialement décalée (\\(W.ZX\\)) et l’originale (\\(ZX\\)). D’ailleurs, le coefficient de régression pour la variable \\(ZX\\), soit la pente de la droite, équivaut au I de Moran!\nLes traits pointillés représentent les moyennes des deux variables, toutes deux égales à 0 puisqu’elles sont centrées-réduites (cote \\(z\\)).\n\nPour analyser ce nuage, nous le décomposons en quatre quadrants :\n\nLe quadrant HH (High-High) en haut à droite regroupe des entités spatiales avec des valeurs fortes (H) qui sont voisines ou proches d’autres entités spatiales avec aussi des valeurs fortes (H). Nous sommes donc en présence d’autocorrélation spatiale locale positive avec des valeurs fortes (HH).\nLe quadrant LL (Low-Low) en bas à gauche regroupe des entités spatiales avec des valeurs faibles (L) qui sont voisines ou proches d’autres entités spatiales avec aussi des valeurs faibles (L). Nous sommes donc en présence d’autocorrélation spatiale locale positive avec des valeurs faibles (LL).\nLe quadrant HL (High-Low) en bas à droite regroupe des entités spatiales avec des valeurs fortes (H) qui sont voisines ou proches d’autres entités spatiales avec des valeurs faibles (L). Nous sommes donc en présence d’autocorrélation spatiale locale négative (HL). Avec humour, un collègue économiste, Jean Dubé, qualifie ce quadrant de village gaulois (Obélix, Astérix et leurs compagnons sont très forts et ils sont entourés de voisins romains plus faibles…).\nLe quadrant LH (Low-High) en haut à gauche regroupe des entités spatiales avec des valeurs faibles (L) qui sont voisines ou proches d’autres entités spatiales avec des valeurs fortes (H). Nous sommes donc en présence d’autocorrélation spatiale locale négative (LH).\n\n\n\n\n\nFigure 2.30: Diagramme de Moran\n\n\n\n\n2.4.3.2 Mise en œuvre dans R\nPour créer le diagramme de Moran, nous avons écrit la fonction DiagMoranUnivarie.\n\nsource(\"code_complementaire/DiagrammeMoran.R\")\n## Réalisation du diagramme de Moran avec la fonction DiagMoranUnivarie\nDiagMoranUnivarie(\n  x = AD.DR$RevMedMenage,\n  listW = W.Rook,\n  titre = \"Diagramme de Moran (matrice de contiguïté selon le partage d'un segment)\",\n  titreAxeX = \"ZX : Revenu médian des ménages centré réduit\",\n  titreAxeY = \"WZ : Variable ZX spatialement décalée\",\n  AfficheAide=TRUE)\n\n\n\n\nReste à déterminer si l’autocorrélation spatiale locale pour ces quatre quadrants est significative. Rien de plus simple, il suffit d’utiliser la valeur de p du I de Moran local (section 2.4.2). Nous choisissons un seuil de signification (habituellement p = 0,05) et obtenons ainsi la typologie comprenant cinq catégories :\n\nAutocorrélation spatiale locale positive et significative (p &lt; 0,05).\n\nHH\nLL\n\n\nAutocorrélation spatiale locale négative et significative (p &lt; 0,05).\n\nHL\nLH\n\n\n\n\nAutocorrélation spatiale locale non significative (p &gt; 0,05).\n\nLe code R ci-dessous permet d’obtenir la typologie de l’autocorrélation spatiale avec le I de Moran local pour le revenu médian des ménages avec une matrice de contiguïté selon le partage d’un segment (Rook). La figure 2.31 dénote surtout une autocorrélation spatiale positive importante (respectivement 33 et 48 aires de diffusion classées HH et LL), comparativement à l’autocorrélation spatiale négative qui ne comprend qu’une aire de diffusion (LH).\n\nlibrary(dplyr)\n## Cote Z (variable centrée réduite)\nzx &lt;- (AD.DR$RevMedMenage - mean(AD.DR$RevMedMenage))/sd(AD.DR$RevMedMenage)\n## variable X centrée réduite spatialement décalée avec une matrice Rook\nwzx &lt;- lag.listw(W.Rook, zx)    \n## I de Moran local (notez que vous pouvez aussi utiliser la fonction localmoran_perm)\nlocalMoranI   &lt;- localmoran(AD.DR$RevMedMenage, W.Rook)\n#localMoranI.mc   &lt;- localmoran_perm(AD.DR$RevMedMenage, W.Rook, n=999)\nplocalMoranI  &lt;- localMoranI[, 5]\n## Choix d'un seuil de signification\nsignif = 0.05\n## Construction de la typologie\nTypologie &lt;- attributes(localMoranI)$quadr$mean\nTypologie &lt;- case_when(\n  plocalMoranI &lt; signif ~ Typologie,\n  TRUE ~ \"Non sign.\"\n)\n## Enregistrement de la typologie dans un champ\nAD.DR$TypoIMoran.RevMedian &lt;- Typologie\nAD.DR$TypoIMoran.RevMedian &lt;- factor(AD.DR$TypoIMoran.RevMedian,\n                                     levels = c(\"High-High\", \"High-Low\", \"Low-Low\", \"Low-High\", \"Non sign.\"),\n                                     labels = c(\"HH\", \"HL\", \"LL\", \"LH\", \"Non sign.\"))\ntable(AD.DR$TypoIMoran.RevMedian, useNA = \"always\")\n\n\n       HH        HL        LL        LH Non sign.      &lt;NA&gt; \n       33         0        48         1       167         0 \n\n## Couleurs\nCouleurs &lt;- c(\"HH\" = \"#FF0000\",\n              \"HL\" = \"#f4ada8\",\n              \"LL\" =\"#0000FF\",\n              \"LH\" =\"#a7adf9\",\n              \"Non sign.\" = \"#eeeeee\")\n## Cartographie\ntmap_mode(\"plot\")\ntm_shape(AD.DR) + \n  tm_polygons(col = \"TypoIMoran.RevMedian\", \n              palette = Couleurs, \n              title =\"Autocorrélation spatiale locale\")+\n  tm_credits(\"HH : Positive dans un contexte de valeurs fortes (HH)\n              LL : Négative dans un contexte de valeurs faibles (LL)\n              HL : Positive dans un contexte de valeurs fortes (HL)\n              LH : Négative dans un contexte de valeurs fortes (LH) \n              Non sign. : Non significative\",\n             size = .5,\n             position=c(\"right\", \"bottom\"),\n             align = \"right\")+\n  tm_layout(frame= FALSE,\n            legend.outside = TRUE,\n            legend.title.size = 1,\n            legend.position = c(\"right\", \"center\"))\n\n\n\nFigure 2.31: Typologie de l’autocorrélation spatiale locale avec le I de Moran local\n\n\n\n\n\n\n\n\nNe pas confondre les statistiques locales de Getis et Ord et la typologie avec le I de Moran local.\n\n\nSuccinctement, ces deux familles de mesures renvoient deux typologies différentes :\n\nLe I de Moran local comprend cinq classes : HH, LL, HL, LH et non significatif.\nLes statistiques locales de Getis et Ord comprennent trois classes : points chauds (sensiblement l’équivalent de HH), points froids (sensiblement l’équivalent de LL) et non significatifs.\n\n\n\n\n2.4.4 Version locale des statistiques de comptage de jointure (Join Count Statistics)\nUne version locale des statistiques de comptage de jointure a été proposée par Luc Anselin et Xun Li (2019). Au même titre que la version locale du I de Moran, elle consiste à désagréger localement le calcul de l’indicateur global. Pour chaque observation, nous comptons ainsi le nombre de paires 0-0 (WW) ou 1-1 (BB) pour une variable binaire.\nPour déterminer si les associations 0-0 et 1-1 sont plus fréquentes que celles attendues du hasard, il est recommandé d’utiliser un test d’inférence par permutations. Ce test n’est actuellement pas implémenté dans spdep, mais il est disponible dans le package rgeoda avec la fonction local_joincount. Nous reprenons ici l’exemple des deux variables identifiant une majorité de locataires (loc_maj) ou de propriétaires (proprio_maj) pour les aires de diffusion (AD) de la ville de Sherbrooke.\n\nlibrary(rgeoda)\n## Définition de la matrice de contiguïté avec rgeoda\nwmat &lt;- queen_weights(ADSherb)\n## Création de deux variables binaires avec des valeurs de 0 et 1\n## pour les locataires et propriétaires\nADSherb$loc_maj &lt;- ifelse(ADSherb$Locataire &gt; ADSherb$Proprietaire, 1, 0)\nADSherb$proprio_maj &lt;- ifelse(ADSherb$Proprietaire &gt; ADSherb$Locataire, 1, 0)\n## Calcul de la version locale du join count test\ntestLoc &lt;- local_joincount(wmat, ADSherb[\"loc_maj\"], permutations = 999)\ntestPro &lt;- local_joincount(wmat, ADSherb[\"proprio_maj\"], permutations = 999)\n\nAvec le code ci-dessous, nous cartographions les AD en fonction des seuils de significativité de la version locale du test. Sans surprise, les AD avec une autocorrélation spatiale significative sont les suivantes :\n\ncelles au centre de la ville de Sherbrooke avec une majorité de locataires (figure 2.32, a);\ncelles de l’ouest de la ville avec une majorité de propriétaires (figure 2.32, b).\n\n\nlibrary(dplyr)\n## Sauvegarde des résultats\nADSherb$testLoc_pvals &lt;- testLoc$p_vals\nADSherb$testPro_pvals &lt;- testPro$p_vals\n## Calcul d'une variable avec les seuils de significativité pour les locataires\nADSherb$clusterLoc &lt;- case_when(\n  ADSherb$testLoc_pvals &lt; 0.05 & ADSherb$testLoc_pvals &gt;= 0.01 ~ 1,\n  ADSherb$testLoc_pvals &lt; 0.01 ~ 2,\n  TRUE ~ 0\n)\nADSherb$clusterLoc &lt;- factor(ADSherb$clusterLoc, \n                          levels = 0:2,\n                          labels = c(\"Non significatif (p &gt; 0,05)\", \n                                     \"p = 0,05\",\"p = 0,01\"))\n## Calcul d'une variable avec les seuils de significativité pour les propriétaires\nADSherb$clusterPro &lt;- case_when(\n  ADSherb$testPro_pvals &lt; 0.05 & ADSherb$testPro_pvals &gt;= 0.01 ~ 1,\n  ADSherb$testPro_pvals &lt; 0.01 ~ 2,\n  TRUE ~ 0\n)\nADSherb$clusterPro &lt;- factor(ADSherb$clusterPro, \n                          levels = 0:2,\n                          labels = c(\"Non significatif (p &gt; 0,05)\", \n                                     \"p = 0,05\",\"p = 0,01\"))\n\n## Cartographie des résultats\nCarte1 &lt;-  tm_shape(ADSherb) + \n  tm_borders(col=\"black\", lwd=0.5)+\n  tm_fill(col = \"clusterLoc\", \n          palette = c(\"#f0f0f0\", \"#fc9272\", \"#de2d26\"),\n          title = \"Seuil de significativité\")+\n  tm_layout(frame=FALSE,\n            legend.outside = TRUE,\n            legend.outside.position = c(\"bottom\", \"center\"),\n            title = \"a. Locataires majoritaires\",\n            title.size = 1)\nCarte2 &lt;-  tm_shape(ADSherb) + \n  tm_borders(col=\"black\", lwd=0.5)+\n  tm_fill(col = \"clusterPro\", \n          palette = c(\"#f0f0f0\", \"#fc9272\", \"#de2d26\"),\n          title = \"Seuil de significativité\")+\n  tm_layout(frame=FALSE,\n            legend.outside = TRUE,\n            legend.outside.position = c(\"bottom\", \"center\"),\n            title = \"b. Propriétaires majoritaires\",\n            title.size = 1)\ntmap_arrange(Carte1, Carte2)\n\n\n\nFigure 2.32: Cartographie de la version locale des statistiques de comptage de jointure\n\n\n\n\n2.4.5 Version locale de l’indice de Lee\n\n2.4.5.1 Formulation de la version locale de l’indice de Lee\nDans le cas de l’autocorrélation spatiale bivariée, l’indice de Lee dispose aussi d’une version locale. Plus exactement, la version globale de l’indice correspond à l’agrégation des contributions locales de chaque observation. En cartographiant ces contributions locales, il est possible de se faire une idée de la relation spatiale entre les deux variables à l’étude. La version locale de l’indice est définie par l’équation 2.23.\n\\[\nL_i=\\frac{n \\cdot\\left[\\left(\\sum_i w_{i j}\\left(x_j-\\bar{x}\\right)\\right)\\left(\\sum_i w_{i j}\\left(y_j-\\bar{y}\\right)\\right)\\right]}{\\sqrt{\\sum_i\\left(x_i-\\bar{x}\\right)^2} \\sqrt{\\sum_i\\left(y_i-\\bar{y}\\right)^2}}=\\frac{n \\cdot\\left(\\tilde{x}_i-\\bar{x}\\right)\\left(\\tilde{y}_i-\\bar{y}\\right)}{\\sqrt{\\sum_i\\left(x_i-\\bar{x}\\right)^2} \\sqrt{\\sum_i\\left(y_i-\\bar{y}\\right)^2}}\n\\tag{2.23}\\]\nAvec :\n\n\n\\(n\\), le nombre d’observations;\n\n\\(W_{ij}\\), la valeur de la pondération spatiale entre les entités spatiales \\(i\\) et \\(j\\);\n\n\\(\\bar{x}\\), la moyenne de la variable x;\n\n\\(\\bar{y}\\), la moyenne de la variable y.\n\n2.4.5.2 Mise en œuvre dans R\nReprenons notre exemple précédent sur les pollutions atmosphériques à Lyon. Nous avions observé que le dioxyde d’azote partageait un patron d’autocorrélation spatiale positive forte avec les particules fines, et un patron moins prononcé avec le bruit. Nous allons observer ces deux relations à l’échelle locale.\n\nlibrary(geocmeans)\ndata(\"LyonIris\")\n## Liste de variables à analyser\nvars &lt;- c(\"Lden\",\"NO2\",\"PM25\",\"VegHautPrt\")\n## Matrice de pondération spatiale Queen standardisée\nnb &lt;- poly2nb(LyonIris, queen = TRUE)\nWmat &lt;- nb2listw(nb, style = 'W')\n## Calcul de la version locale de l'indice de Lee entre \n# NO2 et PM25\nLee_1 &lt;- lee(LyonIris$NO2, LyonIris$PM25, listw = Wmat, n = nrow(LyonIris))\n# NO2 et DBA\nLee_2  &lt;- lee(LyonIris$NO2, LyonIris$Lden, listw = Wmat, n = nrow(LyonIris))\n\nDans le package spdep, aucune fonction ne permet de calculer un test d’inférence pour les valeurs locales de Lee. Nous pouvons cependant assez facilement obtenir des pseudo valeurs de p en réalisant 999 permutations aléatoires et en comptant combien de fois les valeurs obtenues au hasard sont plus grandes ou plus petites que celles obtenues dans les données initiales.\n\n## Nous créons une matrice à deux colonnes remplies de zéros qui \n## servira à compter le nombre de fois où les valeurs des permutations \n## sont au-dessus ou en dessous des valeurs réelles\np_vals_1 &lt;- cbind(rep(0, nrow(LyonIris)),\n                  rep(0, nrow(LyonIris))\n                  )\np_vals_2 &lt;- cbind(rep(0, nrow(LyonIris)),\n                  rep(0, nrow(LyonIris))\n                  )\nN &lt;- nrow(LyonIris)\nfor(i in 1:999){\n  # Permutation aléatoire de l'ordre des observations\n  perm_data &lt;- LyonIris[sample(1:nrow(LyonIris)),]\n  # Calcul de l'indice de Lee sur les données permutées\n  Lee_1_perm &lt;- lee(perm_data$NO2, perm_data$PM25, listw = Wmat, n = N)\n  Lee_2_perm  &lt;- lee(perm_data$NO2, perm_data$Lden, listw = Wmat, n = N)\n  # Test pour vérifier si les valeurs obtenues sont plus grandes\n  test1 &lt;- Lee_1$localL &lt; Lee_1_perm$localL\n  test2 &lt;- Lee_2$localL &lt; Lee_2_perm$localL\n  # Ajout du résultat aux matrices de valeurs de p\n  p_vals_1[,1] &lt;- p_vals_1[,1] + test1\n  p_vals_2[,1] &lt;- p_vals_2[,1] + test2\n  # Test pour vérifier si les valeurs obtenues sont plus petites\n  test1 &lt;- Lee_1$localL &gt; Lee_1_perm$localL\n  test2 &lt;- Lee_2$localL &gt; Lee_2_perm$localL\n  # Ajout du résultat aux matrices de valeurs de p\n  p_vals_1[,2] &lt;- p_vals_1[,2] + test1\n  p_vals_2[,2] &lt;- p_vals_2[,2] + test2\n}\n\n## Ajout des données originales\nLyonIris$Lee1 &lt;- Lee_1$localL\nLyonIris$Lee2 &lt;- Lee_2$localL\nLyonIris$Lee1_p_higher &lt;- p_vals_1[,1] / 999\nLyonIris$Lee1_p_lower &lt;- p_vals_1[,2] / 999\nLyonIris$Lee2_p_higher &lt;- p_vals_2[,1] / 999\nLyonIris$Lee2_p_lower &lt;- p_vals_2[,2] / 999\n\n## Cartographie des résultats avec deux cartes dans lesquelles\n## les valeurs non significatives au seuil 0,01 sont représentées en gris clair.\ntest_sign1 &lt;- LyonIris$Lee1_p_higher &lt; 0.01 | LyonIris$Lee1_p_lower &lt; 0.01\nmap1 &lt;- tm_shape(LyonIris)+tm_borders(col=\"black\")+\n  tm_shape(subset(LyonIris, test_sign1)) + \n  tm_polygons(col = 'Lee1', n = 5, style = 'kmeans', midpoint = 0, palette = '-RdBu',\n              legend.format = list(digits = 2, text.separator = \"à\"),\n              title = 'Indice de Lee') + \n  tm_shape(subset(LyonIris, !test_sign1)) + \n  tm_polygons(col = 'lightgrey')+ \n  tm_layout(legend.outside = FALSE, frame = FALSE,\n            title = \"a. NO2 versus PM25\", title.size = .75)+\n  tm_credits(text=\"Gris : non significatif (p &gt; 0,001).\",\n             position = c(\"RIGHT\", \"BOTTOM\"))\n\ntest_sign2 &lt;- LyonIris$Lee2_p_higher &lt; 0.01 | LyonIris$Lee2_p_lower &lt; 0.01\nmap2 &lt;- tm_shape(LyonIris)+tm_borders(col=\"black\")+\n  tm_shape(subset(LyonIris, test_sign2)) + \n  tm_polygons(col = 'Lee2', n = 5, style = 'kmeans', palette = '-RdBu',\n              legend.format = list(digits = 2, text.separator = \"à\"),\n              title = 'Indice de Lee') + \n  tm_shape(subset(LyonIris, !test_sign2)) + \n  tm_polygons(col = 'lightgrey') + \n  tm_layout(legend.outside = FALSE, frame = FALSE,\n            title = \"b. NO2 versus Lden\", title.size = .75)+\n  tm_credits(text=\"Gris : non significatif (p &gt; 0,001).\",\n             position = c(\"RIGHT\", \"BOTTOM\"))\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\nFigure 2.33: Cartographie des valeurs de l’indice de Lee local\n\n\n\nÀ la figure 2.33 (a), nous constatons une forte autocorrélation spatiale bivariée positive dans deux secteurs de l’agglomération :\n\nAu centre avec des entités spatiales ayant des valeurs fortes pour les deux variables.\nDans les zones périphériques au nord, à l’est et à l’ouest avec des entités spatiales ayant des valeurs faibles pour les deux variables.\n\nÀ la figure 2.33 (b), nous observons trois patrons d’autocorrélation spatiale bivariée :\n\nAutocorrélation spatiale positive pour des entités spatiales au centre de l’agglomération avec des valeurs fortes de bruit et de dioxyde d’azote.\nAutocorrélation spatiale positive pour des entités spatiales au nord de l’agglomération avec des valeurs faibles de bruit et de dioxyde d’azote.\nAutocorrélation spatiale négative à l’est de l’agglomération avec des valeurs fortes pour le bruit, mais faibles pour le dioxyde d’azote.\n\n\n\n\n\n\nAutocorrélation spatiale multivariée\n\n\nIl est possible d’analyser l’autocorrélation spatiale dans un contexte multivarié. Cependant, ces méthodes sont parfois difficiles à interpréter, car l’augmentation de la dimensionnalité des données analysées ajoute un haut niveau d’abstraction. Si ces méthodes vous intéressent, nous vous recommandons de jeter un œil à :\n\nL’indice de Geary multivarié local (Anselin 2019), qui peut être calculé facilement dans R (avec la fonction local_multigeary du package rgeoda) en faisant la moyenne des valeurs locales de l’indice de Geary univarié.\nLes méthodes factorielles intégrant l’espace, dont l’analyse en composantes principales spatiales (spatial principal component analysis en anglais, SPCA) (Jombart et al. 2008) qui permet d’identifier des facteurs maximisant le produit de la variance expliquée d’un ensemble de variables continues et l’autocorrélation spatiale de ces facteurs (mesurée avec le I de Moran). Dans R, les packages pertinents pour ces méthodes sont adegenet et ade4.\n\n\n\n\n2.4.6 Typologie basée sur le diagramme de Moran dans un contexte bivarié\n\n2.4.6.1 Formulation de la typologie basée sur le diagramme de Moran dans un contexte bivarié\nÀ la section 2.4.3, nous avons décrit la construction du diagramme de Moran dans un contexte univarié avec la variable centrée réduite (ZX) sur l’axe des X et la variable centrée réduite spatialement décalée (WZX) sur l’axe des Y. Par la suite, ce diagramme est décomposé en quatre quadrants (HH, LL, HL, LH). Cette démarche peut être adaptée à un contexte bivarié avec deux variables continues, c’est-à-dire avec ZX et WZY pour construire le diagramme. Les quatre quadrants s’interprètent alors comme suit (figure 2.33) :\n\nHH (High-High) regroupe des entités spatiales avec des valeurs fortes (H) pour la variable X qui sont voisines ou proches d’autres entités spatiales avec des valeurs fortes pour la variable Y (H). Nous sommes donc en présence d’autocorrélation spatiale locale bivariée positive avec des valeurs fortes (HH).\nLL (Low-Low) regroupe des entités spatiales avec des valeurs faibles (L) pour la variable X qui sont voisines ou proches d’autres entités spatiales avec des valeurs faibles pour la variable Y (L). Nous sommes donc en présence d’autocorrélation spatiale locale bivariée positive avec des valeurs faibles (LL).\nHL (High-Low) regroupe des entités spatiales avec des valeurs fortes (H) pour la variable X qui sont voisines ou proches d’autres entités spatiales avec des valeurs faibles pour la variable Y (L), soit de l’autocorrélation spatiale locale bivariée négative (HL).\nLH (Low-High) regroupe des entités spatiales avec des valeurs faibles (L) pour la variable X qui sont voisines ou proches d’autres entités spatiales avec des valeurs fortes (H) pour la variable Y, soit de l’autocorrélation spatiale bivariée locale négative (LH).\n\nPour déterminer si l’autocorrélation spatiale locale (positive ou négative) pour les quatre est significative, nous utilisons la valeur de p de la version locale du I de Moran bivariée (habituellement p = 0,05).\n\n\n\n\nFigure 2.34: Diagramme de Moran dans un contexte bivarié\n\n\n\n\n\n\n\n\nNon-symétrie du I de Moran bivarié et du diagramme de Moran dans un contexte bivarié\n\n\nÀ titre de rappel, le diagramme est construit avec ZX et WZY, tout comme l’indice de Moran bivarié. Par conséquent, les résultats entre ZX et WZY (figure 2.33, a) et ZY et WZX (figure 2.33, b) ne sont pas identiques!\n\n\n\n2.4.6.2 Mise en œuvre dans R\nPour créer le diagramme de Moran dans un contexte bivarié, nous avons écrit la fonction DiagMoranUnivarie.\n\nsource(\"code_complementaire/DiagrammeMoran.R\")\nlibrary(ggpubr)\n## Matrice de contiguïté\nQueen &lt;- poly2nb(LyonIris, queen=FALSE)\nW.Queen &lt;- nb2listw(Queen, zero.policy=TRUE, style = \"W\")\n## Réalisation du diagramme de Moran avec la fonction DiagMoranBivarie\nDiag1 &lt;- DiagMoranBivarie(x = LyonIris$NO2,\n                 y = LyonIris$PM25,\n                 listW = W.Queen,\n                 titre = \"Diagramme de Moran\",\n                 titreAxeX = \"ZX : Dioxyde d'azote (NO2)\",\n                 titreAxeY = \"WZY : Particules fines (PM2,5)\",\n                 AfficheAide=FALSE)\nDiag2 &lt;- DiagMoranBivarie(x = LyonIris$PM25,\n                 y = LyonIris$NO2,\n                 listW = W.Queen,\n                 titre = \"Diagramme de Moran\",\n                 titreAxeX = \"ZX : Particules fines (PM2,5)\",\n                 titreAxeY = \"WZY : Dioxyde d'azote (NO2)\",\n                 AfficheAide=FALSE)\nggarrange(Diag1, Diag2)\n\n\n\n\nPour mettre en œuvre la typologie, vous pouvez utiliser soit la fonction local_bimoran du package rgeoda, soit la fonction localmoran_bv du package spdep. Cette dernière est mobilisée dans le code ci-dessous pour obtenir la typologie de l’autocorrélation spatiale bivariée entre :\n\nLe dioxyde d’azote (ZX) et les particules fines (ZWY) (figure 2.33, a).\nLes particules fines (ZX) et le dioxyde d’azote (ZWY) (figure 2.33, b).\n\n\nlibrary(spdep)\n## Matrice de contiguïté\nQueen &lt;- poly2nb(LyonIris, queen=FALSE)\nW.Queen &lt;- nb2listw(Queen, zero.policy=TRUE, style = \"W\")\n## Variables centrées réduites\nz.no2 &lt;- (LyonIris$NO2 - mean(LyonIris$NO2))/sd(LyonIris$NO2)\nz.pm25 &lt;- (LyonIris$PM25 - mean(LyonIris$PM25))/sd(LyonIris$PM25)\n## Variables spatialement décalées\nwz.no2 &lt;- lag.listw(W.Queen, z.no2)\nwz.pm25 &lt;- lag.listw(W.Queen, z.pm25)\n## I de Moran local bivarié\nlocalMoranBivariee_NO2_PM25 &lt;- localmoran_bv(LyonIris$NO2, LyonIris$PM25, W.Queen, nsim = 999)\nlocalMoranBivariee_PM25_NO2 &lt;- localmoran_bv(LyonIris$PM25, LyonIris$NO2, W.Queen, nsim = 999)\n## Valeur de p du I de Moran bivariée\nplocalMoranI_NO2_PM25  &lt;- localMoranBivariee_NO2_PM25[, 7]\nplocalMoranI_PM25_NO2  &lt;- localMoranBivariee_PM25_NO2[, 7]\n## Choix d'un seuil de signification\nsignif = 0.05\n## Construction de la typologie NO2 versus PM25\nTypoNO2_PM25 &lt;- ifelse(z.no2 &gt; 0 & wz.pm25 &gt; 0, \"1. HH\", NA)\nTypoNO2_PM25 &lt;- ifelse(z.no2 &lt; 0 & wz.pm25 &lt; 0, \"2. LL\", TypoNO2_PM25)\nTypoNO2_PM25 &lt;- ifelse(z.no2 &gt; 0 & wz.pm25 &lt; 0, \"3. HL\", TypoNO2_PM25)\nTypoNO2_PM25 &lt;- ifelse(z.no2 &lt; 0 & wz.pm25 &gt; 0, \"4. LH\", TypoNO2_PM25)\nTypoNO2_PM25 &lt;- ifelse(plocalMoranI_NO2_PM25 &gt; signif, \"Non sign\", TypoNO2_PM25)\n## Construction de la typologie PM25 versus NO2 \nTypoPM25_NO2 &lt;- ifelse(z.pm25 &gt; 0 & wz.no2 &gt; 0, \"1. HH\", NA)\nTypoPM25_NO2 &lt;- ifelse(z.pm25 &lt; 0 & wz.no2 &lt; 0, \"2. LL\", TypoPM25_NO2)\nTypoPM25_NO2 &lt;- ifelse(z.pm25 &gt; 0 & wz.no2 &lt; 0, \"3. HL\", TypoPM25_NO2)\nTypoPM25_NO2 &lt;- ifelse(z.pm25 &lt; 0 & wz.no2 &gt; 0, \"4. LH\", TypoPM25_NO2)\nTypoPM25_NO2 &lt;- ifelse(plocalMoranI_PM25_NO2 &gt; signif, \"Non sign\", TypoPM25_NO2)\n## Enregistrement des résultats dans la couche LyonIRS\nLyonIris$TypoNO2_PM25 &lt;- TypoNO2_PM25\nLyonIris$TypoPM25_NO2 &lt;- TypoPM25_NO2\n## Couleurs\nCouleurs &lt;- c(\"1. HH\" = \"#FF0000\",\n              \"2. LL\" =\"#0000FF\",\n              \"3. HL\" = \"#f4ada8\",\n              \"4. LH\" =\"#a7adf9\",\n              \"Non sign\" = \"#eeeeee\")\n## Cartographie\ntmap_mode(\"plot\")\nCarte1 &lt;- tm_shape(LyonIris) +\n            tm_polygons(col = \"TypoNO2_PM25\",\n              palette = Couleurs,\n              title =\"Typologie\")+\n            tm_layout(frame = FALSE, \n                      legend.outside = TRUE,\n                      legend.outside.position = c(\"bottom\", \"center\"),\n                      title = \"a. NO2 versus PM2,5\",\n                      title.size = .9)\nCarte2 &lt;- tm_shape(LyonIris) +\n            tm_polygons(col = \"TypoPM25_NO2\",\n              palette = Couleurs,\n              title =\"Typologie\")+\n            tm_layout(frame = FALSE, \n                      legend.outside = TRUE,\n                      legend.outside.position = c(\"bottom\", \"center\"),\n                      title = \"b. PM2,5 versus NO2\",\n                      title.size = .9)\ntmap_arrange(Carte1, Carte2)\n\n\n\nFigure 2.35: Typologies basées sur le diagramme de Moran dans un contexte bivarié\n\n\n\n\n\n\n\n\nAutocorrélation spatiale locale dans un contexte bivarié pour les variables binaires\n\n\nÀ la section 2.4.4, nous avons abordé les statistiques locales de comptage de jointure. Tout comme la version locale de I de Moran, elles peuvent être adaptées à un contexte bivarié avec deux variables binaires. Pour ce faire, vous pouvez utiliser la fonction local_bijoincount du package rgeoda.\n\n\n\n2.4.7 Autocorrélation locale pour une variable qualitative (catégorielle) : l’indicateur ELSA\n\n2.4.7.1 Formulation de l’indicateur ELSA\nRécemment, un indicateur basé sur la mesure de l’entropie locale a été proposé pour mesurer l’autocorrélation spatiale locale d’une variable qualitative comprenant plusieurs modalités (catégories) (Naimi et al. 2019). Cet indicateur ELSA (Entropy-based local indicator of spatial association), qui varie de 0 (autocorrélation spatiale parfaite) à 1 (forte hétérogénéité spatiale), est calculé comme suit :\n\\[\n\\begin{aligned}\nE_i=E_{a i} \\times E_{c i} \\\\\n& E_{a i}=\\frac{\\sum_j \\omega_{i j} d_{i j}}{\\max \\{d\\} \\sum_j \\omega_{i j}}, j \\neq i \\\\\n& E_{c i}=-\\frac{\\sum_{k=1}^{m_\\omega} p_k \\log _2\\left(p_k\\right)}{\\log _2 m_i}, j \\neq i \\\\\n& m_i=\\left\\{\\begin{array}{l}\nm \\text { if } \\sum_j \\omega_{i j}&gt;m \\\\\n\\sum_j \\omega_{i j}, \\text { otherwise }\n\\end{array}\\right. \\\\\n& d_{i j}=\\left|c_i-c_j\\right|\n\\end{aligned}\n\\tag{2.24}\\]\nIl s’agit donc du produit de deux termes, soit \\(E_{a i}\\) et \\(E_{c i}\\). Le premier mesure la dissimilarité entre l’observation i et ses voisins j. Le second est l’indice d’entropie de Shannon et quantifie la diversité des observations voisines de i.\nL’indicateur ELSA a la particularité de tenir compte du degré de dissimilarité entre les modalités (catégories) de la variable qualitative à l’étude. Aussi, il permet de spécifier qu’une catégorie A est plus ressemblante à une catégorie B qu’à une catégorie C. Or, nous supposons habituellement que A est différent de B et de C, ce qui produit une matrice des distances sémantiques binaire, telle que présentée au tableau 2.11.\n\n\n\n\nTableau 2.11: Matrice des distances sémantiques classique (binaire)\n\n\nA\nB\nC\n\n\n\nA\n0\n1\n1\n\n\nB\n1\n0\n1\n\n\nC\n1\n1\n0\n\n\n\n\n\n\nToutefois, si nous considérons que les catégories A et B sont plus proches entre elles que de la catégorie C, la matrice des distances sémantiques devrait être différente comme présentée au tableau 2.12.\n\n\n\n\nTableau 2.12: Matrice des distances sémantiques modifiée\n\n\nA\nB\nC\n\n\n\nA\n0,0\n0,5\n1\n\n\nB\n0,5\n0,0\n1\n\n\nC\n1,0\n1,0\n0\n\n\n\n\n\n\nPour déterminer si l’autocorrélation spatiale est significative, Babak Naimi et ses collègues (2019) proposent une approche par simulations Monte-Carlo, très semblable aux tests par permutations vus pour les autres mesures d’autocorrélation spatiale.\n\n2.4.7.2 Mise en œuvre dans R\nNous illustrons ici comment calculer l’indice ELSA sur des données matricielles (raster) avec une image dérivée d’un modèle numérique de terrain (élévation) et d’une image d’un indice de végétation par différence normalisée (NDVI) (figure 2.36). Cette image est en accès libre sur le portail de données de la Communauté métropolitaine de Montréal. Pour accélérer les calculs, nous avons extrait une petite partie de l’image qui couvre une zone de la ville de Laval. Cette image classifie le territoire en quatre catégories :\n\nMinéral bas (&lt; 3 mètres et NDVI &lt; 0,3) (route, stationnement, etc.).\nMinéral haut (&gt;= 3 mètres et NDVI &lt; 0,3) (construction).\nVégétal bas (&lt; 3 mètres et NDVI &gt;= 0,3) (culture, gazon, etc.).\nVégétal haut (&gt;= 3 mètres et NDVI &gt;= 0,3) (canopée).\n\n\nlibrary(terra)\nlibrary(tmap)\nlibrary(geocmeans)\nlibrary(ggpubr)\n## Chargement du raster\nlaval_data &lt;- rast(\"data/chap02/65005_IndiceCanopee_2021_sub.tif\")\ntm_shape(laval_data) +\n  tm_raster(palette = c(\"#fbd4b4\", \"#e36c0a\", \"#92d050\", \"#76923c\"),\n            style = 'cat',\n            title = \"Classe (catégorie)\",\n            labels = c(\"1. Minéral bas\", \"2. Minéral haut\",\n                       \"3. Végétal bas\", \"4. Végétal haut\"))+\n  tm_layout(legend.outside = TRUE, frame = FALSE)\n\n\n\nFigure 2.36: Données matricielles sur une portion du territoire de Laval\n\n\n\nPour calculer l’indicateur ELSA, nous devons premièrement définir une matrice de voisinage. Pour ce faire, nous créons plusieurs fenêtres circulaires de taille différente (100, 200, 300 et 400 mètres; figure 2.37). Notez que la résolution spatiale de l’image utilisée est de 5 m x 5 m, ce qui est possible de vérifier avec la fonction terra::res. La figure 2.37 illustre la taille des différentes matrices spatiales circulaires que nous allons construire. Chaque pixel de la zone en noir sera considéré comme un voisin du pixel central pour lequel l’indicateur ELSA sera calculé.\n\n\n\n\nFigure 2.37: Illustration des matrices spatiales circulaires sur une image\n\n\n\nEn plus de la matrice spatiale, nous devons définir une matrice de dissimilarité entre les modalités (catégories) de la variable qualitative. Nous proposons de considérer que les catégories Végétal haut et Végétal bas ont une distance de 0,5 entre elles, tout comme les catégories de surface Minéral bas et Minéral haut. Par contre, la distance est fixée à 1 entre les surfaces végétales et celles minérales (tableau 2.13).\n\n\n\n\nTableau 2.13: Matrice de dissimilarité entre les catégories de l’image\n\n\nMB\nMH\nVB\nVH\n\n\n\nMinéral bas (MB)\n0,0\n0,5\n1,0\n1,0\n\n\nMinéral haut (MH)\n0,5\n0,0\n1,0\n1,0\n\n\nVégétal bas (VB)\n1,0\n1,0\n0,0\n0,5\n\n\nVégétal haut (VH)\n1,0\n1,0\n0,5\n0,0\n\n\n\n\n\n\nFinalement, nous calculons l’indicateur d’autocorrélation spatiale ELSA avec uniquement des rayons de 100 et de 400 mètres afin de limiter le temps de calcul. Pour ce faire, nous utilisons le package geocmeans et ses fonctions circular_window et elsa_raster.\n\n## Avec geocmeans, la première catégorie doit avoir la valeur 0\nlaval_data &lt;- laval_data - 1\n## Définition des fenêtres circulaires à 100 et 400 mètre\nw100 &lt;- circular_window(100,5)\nw400 &lt;- circular_window(400,5)\n## Calcul de l'indicateur ELSA\nelsa_100 &lt;- elsa_raster(laval_data, w100, matrice_dissim)\nelsa_400 &lt;- elsa_raster(laval_data, w400, matrice_dissim)\n## Cartographie des résultats\ncarte1 &lt;- tm_shape(elsa_100) + \n  tm_raster(palette = \"RdYlGn\", n = 6, \n            legend.is.portrait = FALSE, title = \"ELSA 100m\") + \n  tm_layout(legend.outside = TRUE, frame = FALSE,\n            legend.outside.position = \"bottom\",\n            legend.stack = \"horizontal\",\n            legend.text.size = .55)\ncarte2 &lt;- tm_shape(elsa_400) + \n  tm_raster(palette = \"RdYlGn\", n = 6, \n            legend.is.portrait = FALSE, title = \"ELSA 400m\") + \n  tm_layout(legend.outside = TRUE, frame = FALSE,\n            legend.outside.position = \"bottom\",\n            legend.stack = \"horizontal\",\n            legend.text.size = .55)\ntmap_arrange(carte1, carte2, ncol = 2)\n\n\n\nFigure 2.38: Cartographie des valeurs ELSA obtenues\n\n\n\nLa figure 2.38 révèle que l’utilisation d’une matrice circulaire de 400 mètres lisse trop les résultats. Par conséquent, nous privilégions ceux obtenus avec une matrice circulaire de 100 mètres. Puis, avec le code ci-dessous, nous souhaitons identifier les bâtiments avec un environnement immédiat (défini à 100 mètres) avec une forte hétérogénéité spatiale, c’est-à-dire végétalisé.\n\n# Nous isolons les bâtiments avec forte dissimilarité à proximité\n# la valeur de l'image de sortie (buildings_green) sera 1\nbuildings_green &lt;- (laval_data == 1) & (elsa_100 &gt;= 0.4)\n# Nous isolons les bâtiments avec faible dissimilarité à proximité\n# la valeur de l'image de sortie (buildings_other) sera 2\nbuildings_other &lt;- ((laval_data == 1) & (elsa_100 &lt; 0.4)) * 2\n# Nous combinons les deux images\nraster_buildings &lt;- buildings_green + buildings_other\ntmap_mode(\"view\")\ntm_basemap(c(\"Esri.WorldImagery\", \"OpenStreetMap\"))+\n  tm_shape(raster_buildings) + \n  tm_raster(palette = c(alpha(\"white\",0),  \"green\", \"grey\"),\n            title = \"Bâtiments\",\n            labels = c(\"Transparent (0) : autre que bâtiments\",\n                       \"Vert (1) : forte dissimilarité\", \n                       \"Gris (2) : faible dissimilarité\"),\n            style = 'cat', zindex = 1)\n\n\n\n\n\nSur la carte ci-dessus, nous distinguons clairement deux groupes de bâtiments avec une forte hétérogénéité spatiale dans leur environnement immédiat (en vert) : ceux en bordure du quartier résidentiel et ceux situés le long des rangs Saint-Elzéar Est et Haut-Saint-François au nord-ouest. À l’inverse, les bâtiments au cœur du quartier résidentiel de banlieue affichent une faible hétérogénéité spatiale dans leur environnement immédiat (en gris).\n\n\n\n\n\nIndicateur ELSA : données vectorielles et test d’inférence\n\n\nDonnées vectorielles\nSi les données sont vectorielles et non matricielles, il convient alors d’utiliser la fonction elsa_vector du package geocmeans.\nIndicateur ELSA et test d’inférence\nDans les fonctions elsa_raster et elsa_vector, aucun test d’inférence n’est implémenté pour obtenir une valeur de p locale. Toutefois, il est assez facile dans R d’appliquer la méthode par simulation Monte-Carlo décrite dans l’article de Naimi et al. (2019). Appliquée à des données matricielles, la démarche est la suivante :\n\nCréer 999 images dont les valeurs des pixels sont tirées aléatoirement avec remplacement.\nRecalculer l’indicateur ELSA pour chaque image.\nPour chaque pixel, compter le nombre de fois où la valeur observée de l’indicateur ELSA est supérieure ou égale à celles des 999 valeurs simulées.\n\nLa pseudo valeur de p pour chaque pixel i s’écrit alors :\n\\[\n  \\text{Pseudo valeur de } p_i = (M+1) / (R+1) \\text{ avec :}\n\\tag{2.25}\\]\n\\(M\\) étant le nombre de fois que la valeur observée est supérieure ou égale à la valeur simulée (\\(E_i \\ge E^*_{ir}\\)) (Naimi et al. 2019, pp. 33-35).\n\n\nAppliquons la démarche ci-dessous dans R pour obtenir une pseudo valeur pour l’indicateur ELSA obtenu sur l’image avec un rayon de 100 mètres. Notez que pour améliorer la vitesse de calcul, nous convertissons notre objet terra::rast en une matrice.\n\n## Conversion des données matricielles en matrice\nbase_data &lt;- terra::values(laval_data, format = \"matrice\")\n## calcul de la proportion de chaque catégorie\nN &lt;- ncell(base_data)\nprobs &lt;- table(base_data) / N\n## Conversion de l'indicateur ELSA (à 100 mètres) en matrice\nbase_elsa &lt;- terra::values(elsa_100, format = \"matrice\")\n## Création d'une matrice qui contiendra les pseudo valeurs de p\np_matrix &lt;- matrix(0, nrow = nrow(base_elsa), ncol = ncol(base_elsa))\n## Lancement des 999 simulations\nfor(i in 1:999){\n  # pour chaque simulation, nous tirons au hasard la valeur de chaque pixel\n  sim_values &lt;- sample(c(0,1,2,3), size = N, replace = TRUE, prob = probs[1:4])\n  # Nous enregistrons les valeurs tirées au hasard dans une matrice\n  dim(sim_values) &lt;- dim(base_elsa)\n  # Calcul de la valeur de ELSA simulée\n  sim_elsa &lt;- elsa_raster(sim_values, w100, matrice_dissim)\n  # Comparaison des valeurs de l'indicateur ELSA (valeurs simulées versus observées)\n  comp_matrix &lt;- base_elsa &gt;= sim_elsa\n  # Ajout à notre matrice des valeurs de p\n  p_matrix &lt;- p_matrix + comp_matrix\n}\n## Conversion des comptages en pseudo valeur de p\np_values &lt;- p_matrix / (999+1)\n\nDans la matrice obtenue, les pseudos valeurs de p représentent la probabilité que l’indice de ELSA obtenu sur les données réelles soit inférieur à celui obtenu si les données étaient réparties aléatoirement sur le territoire. Une pseudo valeur de p inférieure à 0,001 signifie donc qu’il n’y aurait que 0,1 % de chance que le hasard produise un patron d’autocorrélation spatiale plus fort que celui observé avec les données initiales.\n\nraster_p_vals &lt;- laval_data\nvalues(raster_p_vals) &lt;- p_values &lt;= 0.001\ntmap_mode(\"plot\")\n\nCarte1 &lt;- tm_shape(raster_p_vals) +\n  tm_raster(palette = c(\"grey\", \"red\"),\n            style = 'cat',\n            title = \"Significativité\",\n            labels = c(\"Non significatif (p &gt; 0,001)\",\n                       \"Significatif (p &lt;= 0,001)\")) +\n  tm_layout(legend.outside = TRUE, \n            frame = FALSE,\n            legend.outside.position = c(\"bottom\", \"center\"))\n\nCarte2 &lt;- tm_shape(elsa_100) + \n  tm_raster(palette = \"RdYlGn\", n = 6, \n            legend.format = list(text.separator = \"à\"),\n            title = \"ELSA 100 mètres\") + \n  tm_layout(legend.outside = TRUE, \n            frame = FALSE,\n            legend.outside.position =  c(\"bottom\", \"center\"))\n\nCarte3 &lt;- tm_shape(laval_data) + \n  tm_raster(palette = c(\"#fbd4b4\", \"#e36c0a\", \"#92d050\", \"#76923c\"),\n            style = 'cat',\n            title = \"Classe (catégorie)\",\n            labels = c(\"1. Minéral bas\",\n                       \"2. Minéral haut\",\n                       \"3. Végétal bas\",\n                       \"4. Végétal haut\"))+\n  tm_layout(legend.outside = TRUE, \n            frame = FALSE,\n            legend.outside.position =  c(\"bottom\", \"center\"))\n\ntmap_arrange(Carte3, Carte2, Carte1, ncol=3)\n\n\n\nFigure 2.39: Cartographie des valeurs de p de l’indicateur ELSA\n\n\n\nLa figure 2.39 montre en rouge les pixels situés dans des secteurs avec une forte autocorrélation spatiale, soit ceux principalement végétalisés ou principalement minéralisés."
  },
  {
    "objectID": "02-Autocorrelation.html#sec-025",
    "href": "02-Autocorrelation.html#sec-025",
    "title": "2  Autocorrélation spatiale",
    "section": "\n2.5 Quiz de révision du chapitre",
    "text": "2.5 Quiz de révision du chapitre\n\n\n\n\n\nParmi les matrices de pondération spatiale ci-dessous, lesquelles sont des matrices de contiguïté?\n\n\nRelisez au besoin la section 2.2.\n\n\n\n\n\n\nPartage d’un nœud\n\n\n\n\n\n\n\nPartage d’un segment\n\n\n\n\n\n\n\nPartage d’un nœud et ordre d’adjacence\n\n\n\n\n\n\n\nPartage d’un segment et ordre d’adjacence\n\n\n\n\n\n\n\nConnectivité selon la distance\n\n\n\n\n\n\n\n\n\n\nEn anglais, comment est appelée une matrice selon le partage d’un nœud?\n\n\nRelisez au besoin le début de la section 2.2.\n\n\n\n\n\n\nRook\n\n\n\n\n\n\n\nQueen\n\n\n\n\n\n\n\n\n\n\nComparativement à une matrice de l’inverse de la distance, une matrice de l’inverse de la distance au carré accorde un poids plus important aux entités proches.\n\n\nRelisez au besoin la section 2.2.2.3.\n\n\n\n\n\n\nVrai\n\n\n\n\n\n\n\nFaux\n\n\n\n\n\n\n\n\n\n\nQuels sont les avantages de la standardisation en ligne des matrices de pondération spatiale?\n\n\nRelisez au besoin la section 2.2.3.\n\n\n\n\n\n\nLa somme de chaque ligne est égale à 1.\n\n\n\n\n\n\n\nLa somme de l’ensemble de la matrice est égale au nombre d’entités spatiales.\n\n\n\n\n\n\n\nLa standardisation permet de comparer la dépendance spatiale selon différentes matrices.\n\n\n\n\n\n\n\nLa standardisation augmente la vitesse des calculs.\n\n\n\n\n\n\n\n\n\n\nQuelle est la différence entre les deux mesures locales de Getis et Ord?\n\n\nRelisez au besoin la section 2.4.1.\n\n\n\n\n\n\nGi* tient compte à la fois des valeurs des entités voisines ou proches, mais aussi de celle de i.\n\n\n\n\n\n\n\nContrairement à Gi, Gi* a un z-score.\n\n\n\n\n\n\n\n\n\n\nParmi les quatre catégories de la typologie basée sur le nuage de points du I de Moran, lesquelles renvoient à de l’autocorrélation spatiale locale positive?\n\n\nRelisez au besoin la section 2.4.3.\n\n\n\n\n\n\nHH\n\n\n\n\n\n\n\nLL\n\n\n\n\n\n\n\nHL\n\n\n\n\n\n\n\nLH\n\n\n\n\n\n\n\n\n\n\nLe village gaulois correspond à quelle catégorie?\n\n\nRelisez au besoin la section 2.4.3.\n\n\n\n\n\n\nHH\n\n\n\n\n\n\n\nLL\n\n\n\n\n\n\n\nHL\n\n\n\n\n\n\n\nLH\n\n\n\n\n\n\n\n\n\n\nQuelles sont les trois manières de tester la significativité des mesures d’autocorrélation globales?\n\n\nRelisez au besoin la section 2.3.1.3.\n\n\n\n\n\n\nAvec l’hypothèse de la normalité\n\n\n\n\n\n\n\nEn relançant plusieurs fois les calculs\n\n\n\n\n\n\n\nAvec l’hypothèse de la randomisation\n\n\n\n\n\n\n\nAvec la méthode Monte-Carlo (habituellement avec 999 échantillons)\n\n\n\n\n\n\n\n\n\nVérifier votre résultat"
  },
  {
    "objectID": "02-Autocorrelation.html#sec-026",
    "href": "02-Autocorrelation.html#sec-026",
    "title": "2  Autocorrélation spatiale",
    "section": "\n2.6 Exercices de révision",
    "text": "2.6 Exercices de révision\n\n\n\n\n\nExercice 2. Construction de matrices de pondération spatiale\n\n\nConstruisez les matrices de pondération spatiale suivante pour la région métropolitaine de Québec :\n\nMatrice de pondération spatiale selon le partage d’un segment commun (voir la section 2.2.4.1).\nMatrice de pondération spatiale selon l’inverse de la distance au carré, à partir de la distance maximale et un SR et son voisin le plus proche (voir la section 2.2.4.4).\nMatrice de pondération spatiale selon le critère des plus proches voisins (k = 2) (voir la section 2.2.4.5).\n\nComplétez le code ci-dessous.\n\nlibrary(sf)\nlibrary(spdep)\nlibrary(tmap)\n## Importation de la couche des secteurs de recensement\nSRQc &lt;- st_read(dsn = \"data/chap02/exercice/RMRQuebecSR2021.shp\", quiet=TRUE)\n\n## Matrice selon le partage d'un segment (Rook)\nRook &lt;- À compléter\nW.Rook &lt;- À compléter\n\n## Coordonnées des centroïdes des entités spatiales\ncoords &lt;- st_coordinates(st_centroid(SRQc))\n\n## Matrice de l'inverse de la distance réduite\n# Trouver le plus proche voisin avec la fonction knn2nb\nk1 &lt;- À compléter\nplusprochevoisin.max &lt;- max(unlist(nbdists(k1,coords)))\n# Voisins les plus proches avec le seuil de distance maximal\nVoisins.DistMax &lt;- À compléter\n# Distances avec le seuil maximum\ndistances &lt;- À compléter\n# Inverse de la distance au carré\nInvDistances2 &lt;- À compléter\n# Matrices de pondération spatiale standardisées en ligne\nW_InvDistances2 &lt;- À compléter\n\n## Matrice des plus proches voisins avec k = 2\nk2 &lt;- À compléter\nW.k2 &lt;-  À compléter\n\nCorrection à la section 9.2.2.\n\n\n\n\n\n\n\nExercice 3. Calcul du I de Moran global\n\nCalculez le I de Moran global pour la variable D1pct (pourcentage du premier décile de revenu des familles économiques) de la couche SRQc avec les différentes matrices de pondération spatiale (voir la section 2.3.2.2). Complétez le code ci-dessous.\n\n\nlibrary(sf)\nlibrary(spdep)\nlibrary(tmap)\n## Cartographie de la variable\ntm_shape(SRQc)+\n  tm_polygons(col=\"D1pct\", title = \"Premier décile de revenu (%)\",\n              style=\"quantile\", n=5, palette=\"Greens\")+\n  tm_layout(frame = F)+tm_scale_bar(c(0,5,10))\n\n## I de Moran avec la méthode Monte-Carlo avec 999 permutations\n# utilisez la fonction moran.mc\n# avec la matrice W.Rook\nÀ compléter\n# avec la matrice W_InvDistances2Reduite\nÀ compléter\n# avec la matrice W.k2\nÀ compléter\n\nQuelle matrice de pondération spatiale donne la dépendance spatiale la plus forte? Correction à la section 9.2.3.\n\n\n\n\n\n\n\nExercice 3. Mesures d’autocorrélation spatiale locales\n\n\nCalculez et cartographiez les mesures d’autocorrélation spatiale locale pour la variable D1pct de la couche SRQc avec la matrice spatiale W.Rook :\n\nMesure \\(G_i\\) de Getis et Ord (voir la section 2.4.1).\nTypologie du nuage de points du I de Moran avec la fonction localmoran (voir la section 2.4.3).\n\nComplétez le code ci-dessous.\n\n####################\n## Calcul du Z(Gi)\n####################\nSRQc$D1pct_localGetis &lt;- localG(À compléter, \n                                À compléter, \n                                zero.policy=TRUE)\n# Définition des intervalles et des noms des classes\nclasses.intervalles = À compléter\nclasses.noms = c(\"Point froid (p = 0,001)\", \n                \"Point froid (p = 0,01)\", \n                \"Point froid (p = 0,05)\", \n                \"Non significatif\",\n                \"Point chaud (p = 0,05)\", \n                \"Point chaud (p = 0,01)\", \n                \"Point chaud (p = 0,001)\")\n## Création d'un champ avec les noms des classes\nSRQc$D1pct_localGetisP &lt;- cut(SRQc$D1pct_localGetis,\n                              breaks = classes.intervalles,\n                              labels = classes.noms)\n## Cartographie\nÀ compléter\n\n####################\n## Typologie LISA\n####################\n## Cote Z (variable centrée réduite)\nzx &lt;- À compléter\n## variable X centrée réduite spatialement décalée avec une matrice Rook\nwzx &lt;- lag.listw(À compléter)\n## I de Moran local (notez que vous pouvez aussi utiliser la fonction localmoran_perm)\nlocalMoranI  &lt;- localmoran(À compléter)\nplocalMoranI &lt;- localMoranI[, 5]\n## Choisir un seuil de signification\nsignif = 0.05\n## Construction de la typologie\nTypologie &lt;- ifelse(zx &gt; 0 & wzx &gt; 0, \"1. HH\", NA)\nTypologie &lt;- ifelse(zx &lt; 0 & wzx &lt; 0, \"2. LL\", Typologie)\nTypologie &lt;- ifelse(zx &gt; 0 & wzx &lt; 0, \"3. HL\", Typologie)\nTypologie &lt;- ifelse(zx &lt; 0 & wzx &gt; 0, \"4. LH\", Typologie)\nTypologie &lt;- ifelse(plocalMoranI &gt; signif, \"Non sign\", Typologie)  # Non significatif\n## Enregistrement de la typologie dans un champ\nSRQc$TypoIMoran.D1pct &lt;- Typologie\n## Couleurs\nCouleurs &lt;- c(\"red\", \"blue\", \"lightpink\", \"skyblue2\", \"lightgray\")\nnames(Couleurs) &lt;- c(\"1. HH\",\"2. LL\",\"3. HL\",\"4. LH\",\"Non sign\")\n## Cartographie\ntmap_mode(\"plot\")\nÀ compléter\n\nCorrection à la section 9.2.4.\n\n\n\n\n\n\nAnselin, Luc. 1995. « Local indicators of spatial association—LISA. » Geographical analysis 27 (2): 93‑115. https://doi.org/10.1111/j.1538-4632.1995.tb00338.x.\n\n\n———. 1996. « The Moran scatterplot as an ESDA tool to Assess Local Instability in Spatial Association. » In Spatial Analytical Perspectives on GIS, sous la dir. de Manfred M Fischer, 121‑137. Taylor & Francis Group. https://doi.org/10.1201/9780203739051.\n\n\n———. 2019. « A local indicator of multivariate spatial association: extending Geary’s C. » Geographical Analysis 51 (2): 133‑150. https://doi.org/10.1111/gean.12164.\n\n\nAnselin, Luc et Xun Li. 2019. « Operational local join count statistics for cluster detection. » Journal of geographical systems 21: 189‑210. https://doi.org/10.1007/s10109-019-00299-x.\n\n\nApparicio, Philippe et Jérémy Gelb. 2022. Méthodes quantitatives en sciences sociales : un grand bol d’R. FabriqueREL, Licence CC BY-SA. https://laeq.github.io/LivreMethoQuantBolR/.\n\n\nApparicio, Philippe, Jérémy Gelb, Anne-Sophie Dubé, Simon Kingham, Lise Gauvin et Éric Robitaille. 2017. « The approaches to measuring the potential spatial access to urban health services revisited: distance types and aggregation-error issues. » International journal of health geographics 16 (1): 32. https://doi.org/10.1186/s12942-017-0105-9.\n\n\nBivand, Roger et David WS Wong. 2018. « Comparing implementations of global and local indicators of spatial association. » Test 27 (3): 716‑748. https://doi.org/10.1007/s11749-018-0599-x.\n\n\nCliff, Andrew David et J Keith Ord. 1981. Spatial processes: models & applications. Taylor & Francis.\n\n\nDubé, Jean et Diègo Legros. 2014. Econométrie spatiale appliquée des microdonnées. ISTE Group.\n\n\nGeary, Robert C. 1954. « The contiguity ratio and statistical mapping. » The incorporated statistician 5 (3): 115‑146. https://doi.org/10.2307/2986645.\n\n\nGetis, Arthur et J Keith Ord. 1992. « The analysis of spatial association by use of distance statistics. » Geographical analysis 24 (3): 189‑206. https://doi.org/10.1111/j.1538-4632.1992.tb00261.x.\n\n\nJombart, T, S Devillard, Anne-Béatrice Dufour et D Pontier. 2008. « Revealing cryptic spatial patterns in genetic variability by a new multivariate method. » Heredity 101 (1): 92‑103. https://doi.org/10.1038/hdy.2008.34.\n\n\nLee, Sang-Il. 2001. « Developing a bivariate spatial association measure: an integration of Pearson’s r and Moran’s I. » Journal of geographical systems 3: 369‑385. https://doi.org/10.1007/s101090100064.\n\n\nMoran, Patrick. 1950. « A test for the serial independence of residuals. » Biometrika 37 (1/2): 178‑181. https://doi.org/10.2307/2332162.\n\n\nNaimi, Babak, Nicholas AS Hamm, Thomas A Groen, Andrew K Skidmore, Albertus G Toxopeus et Sara Alibakhshi. 2019. « ELSA: Entropy-based local indicator of spatial association. » Spatial statistics 29: 66‑88. https://doi.org/10.1016/j.spasta.2018.10.001.\n\n\nOrd, J Keith et Arthur Getis. 1995. « Local spatial autocorrelation statistics: distributional issues and an application. » Geographical analysis 27 (4): 286‑306. https://doi.org/10.1111/j.1538-4632.1995.tb00912.x.\n\n\nSokal, Robert R, Neal L Oden et Barbara A Thomson. 1998. « Local spatial autocorrelation in a biological model. » Geographical Analysis 30 (4): 331‑354. https://doi.org/10.1111/j.1538-4632.1998.tb00406.x.\n\n\nTobler, Waldo R. 1970. « A computer movie simulating urban growth in the Detroit region. » Economic geography 46 (sup1): 234‑240. https://doi.org/10.2307/143141.\n\n\nWong, David WS et Jay Lee. 2005. Statistical analysis of geographic information with ArcView GIS and ArcGIS. Wiley."
  },
  {
    "objectID": "03-MethodesRepartitionPonctuelle.html#sec-031",
    "href": "03-MethodesRepartitionPonctuelle.html#sec-031",
    "title": "3  Méthodes de répartition ponctuelle",
    "section": "\n3.1 Fréquence et densité des points dans l’espace d’étude",
    "text": "3.1 Fréquence et densité des points dans l’espace d’étude\nLa fréquence est tout simplement le nombre de points présents dans une région donnée (par exemple, le nombre d’hôpitaux, de stations de métro, d’arbres, etc.). La densité est le ratio entre la fréquence et la superficie totale de la région donnée ou la population.\nPar exemple, le tableau 3.1 renvoie le nombre et la densité des stations de métro (pour 10 000 habitants) pour trois villes. Interprétez ces chiffres avec prudence, car ils varient en fonction de la taille du territoire retenu pour les trois villes.\n\n\n\n\nTableau 3.1: Fréquence et densité des stations de métro dans trois villes\n\n\n\n\n\n\n\nVille\nPopulation\nStations de métro\nDensité (stations / 10 000 hab.)\n\n\n\nNew York\n8 804 000\n424\n0,5\n\n\nParis\n2 165 000\n309\n1,4\n\n\nÎle de Montréal\n2 004 000\n68\n0,3"
  },
  {
    "objectID": "03-MethodesRepartitionPonctuelle.html#sec-032",
    "href": "03-MethodesRepartitionPonctuelle.html#sec-032",
    "title": "3  Méthodes de répartition ponctuelle",
    "section": "\n3.2 Analyse centrographique",
    "text": "3.2 Analyse centrographique\nL’analyse centrographique est une approche qui a été largement utilisée durant les décennies 1990 et 2000. Son utilisation est parfois critiquée pour deux raisons principales : 1) elle ne décrit que partiellement le semis de points; 2) aucun test d’inférence n’est calculé. Quoi qu’il en soit, elle permet d’explorer les données avant de se lancer dans des analyses plus avancées.\n\n\n\n\n\nUtilisation de l’analyse centrographique au Québec et dans le monde francophone\n\n\nMarius Thériault (géographe et professeur émérite à l’Université Laval) a largement contribué à la popularité de l’analyse centrographique au Québec et ailleurs. Il est le créateur de MapStat, un module développé avec le langage MapBasic intégré dans le logiciel SIG MapInfo permettant de réaliser une analyse centrographique avant même qu’elle soit implémentée dans ArcGIS. En guise d’exemple, les études suivantes utilisent l’analyse centrographique calculée avec MapStat (López Castro, Thériault et Vandersmissen 2015; Barbonne, Villeneuve et Thériault 2007). Consultez-les au besoin.\n\n\n\n3.2.1 Paramètres de tendance centrale d’un semis de points\nLes deux principaux paramètres de tendance centrale d’un semis de points sont le centre moyen et le point central qui peuvent être ou non pondérés.\n\n3.2.1.1 Centre moyen\nLe centre moyen (\\(cm\\)) est le centre de gravité du semis de points et correspond aux valeurs des moyennes arithmétiques des coordonnées géographiques (équation 3.1).\n\\[\n(\\bar{x}_{cm}, \\bar{y}_{cm}) = \\Biggl( \\frac{\\Sigma_{i=1}^n x_i}{n}, \\frac{\\Sigma_{i=1}^n y_i}{n}\\Biggl)\\text{ avec :}\n\\tag{3.1}\\]\n\n\n\\((\\bar{x}_{cm}, \\bar{y}_{cm})\\), les coordonnées géographiques du point moyen.\n\n\\(n\\), le nombre de points dans la couche géographique.\n\n\\(x_i\\) et \\(y_j\\), les coordonnées géographiques du point \\(i\\).\n\nIl est possible de calculer le centre moyen en pondérant chacun des points du semis avec la valeur d’une variable donnée (équation 3.2). Ainsi, l’importance accordée à chacun des points n’est pas la même. Par exemple, nous pourrions calculer le centre moyen pondéré (\\(cmp\\)) des cliniques médicales d’une ville en pondérant chaque clinique par le nombre de médecins, ou encore le point moyen des hôpitaux pondéré par le nombre de lits. Autre exemple, avec un jeu de données sur les arbres dans une érablière, nous pourrions utiliser une pondération basée sur le diamètre à la hauteur de la poitrine (DHP) afin d’accorder un poids plus important aux arbres de plus « grand volume ».\n\n\n\n\n\nMoyenne pondérée\n\n\nPour un rappel sur le calcul d’une moyenne pondérée, consultez la section intitulée Statistiques descriptives pondérées (Apparicio et Gelb 2022).\n\n\n\\[\n(\\bar{x}_{cmp}, \\bar{y}_{cmp}) = \\Biggl( \\frac{\\Sigma_{i=1}^n w_ix_i}{\\Sigma_{i=1}^nw_i}, \\frac{\\Sigma_{i=1}^n w_iy_i}{\\Sigma_{i=1}^nw_i}\\Biggl) \\text{ avec :}\n\\tag{3.2}\\]\n\n\n\\(n\\), \\(x_i\\) et \\(y_j\\) étant définis plus haut.\n\n\\((\\bar{x}_w, \\bar{y}_w)\\), les coordonnées géographiques du point moyen pondéré.\n\n\\(w_i\\), la valeur de pondération associée au point \\(i\\).\n\nLe centre moyen et le centre moyen pondéré sont des mesures très utiles pour comparer la distribution de plusieurs semis de points (de différents services et équipements collectifs par exemple) ou encore pour décrire l’évolution dans le temps de la répartition d’un semis de points. L’analyse du déplacement du centre moyen (pondéré) à différentes dates nous informe ainsi de l’évolution du phénomène à l’étude et plus spécifiquement, de son orientation et de sa direction.\n\n\n\n\n\nExemple d’utilisation temporelle du centre moyen pondéré\n\n\nUn exemple classique d’utilisation du centre moyen pondéré sur plusieurs années est l’évolution du centre moyen pondéré de la population des États-Unis de 1790 à 2020. Il est calculé à partir des centroïdes des comtés américains et de la population comme variable de pondération extraite des recensements de l’US Census Bureau. Bien entendu, le centre moyen pondéré se déplace vers l’ouest.\nVous pouvez consulter cette carte ou visionner cette courte vidéo YouTube ludique.\n\n\nNotez que les centres moyen et moyen pondéré peuvent aussi être calculés sur des données géographiques comprenant des valeurs d’élévation (\\(x,y,z\\)) :\n\\[\n(\\bar{x}_{cm}, \\bar{y}_{cm}, \\bar{z}_{cm}) = \\Biggl( \\frac{\\Sigma_{i=1}^n x_i}{n}, \\frac{\\Sigma_{i=1}^n y_i}{n}, \\frac{\\Sigma_{i=1}^n z_i}{n}\\Biggl)\n\\tag{3.3}\\]\n\\[\n(\\bar{x}_{cmp}, \\bar{y}_{cmp}, \\bar{z}_{cmp}) = \\Biggl( \\frac{w_i\\Sigma_{i=1}^n x_i}{\\Sigma_{i=1}^nw_i}, \\frac{w_i\\Sigma_{i=1}^n y_i}{\\Sigma_{i=1}^nw_i}, \\frac{w_i\\Sigma_{i=1}^n z_i}{\\Sigma_{i=1}^nw_i}\\Biggl)\n\\tag{3.4}\\]\n\n3.2.1.2 Point central\nLe point central d’un semis de points est celui qui minimise la somme des distances le séparant de tous les autres points (équation 3.5). Tout comme le point moyen, il est aussi possible de calculer le point central pondéré (équation 3.6).\n\\[\npc=\\text{Min}\\Biggl(\\Sigma_{i=1}^n \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}\\Biggl)\\text{; avec }i \\ne j\n\\tag{3.5}\\]\n\\[\npcp=\\text{Min}\\Biggl(\\Sigma_{i=1}^n w_i \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}\\Biggl)\\text{; avec }i \\ne j\n\\tag{3.6}\\]\n\n\n\n\n\nDifférence entre point central et centre moyen\n\n\nContrairement au centre moyen, le point central fait partie du semis de points initial.\nUtilité du point central pondéré\nImaginez que des personnes étudiantes en géographie et en géomatique de toutes les universités du Québec souhaitent organiser une rencontre en présence. Calculer le point central, pondéré par le nombre de personnes étudiantes par établissement participant à la rencontre, permet d’identifier l’université la plus centrale. Idéalement, il faudrait obtenir ce point central pondéré avec les distances temps calculées avec un réseau routier.\n\n\n\n3.2.2 Paramètres de dispersion d’un semis de points\nLes deux principaux paramètres de dispersion d’un semis de points sont la distance standard et la distance standard pondérée. La dispersion d’un semis de points peut être représentée graphiquement avec une enveloppe convexe, un cercle de rayon de standard ou une ellipse (avec ou sans pondération).\n\n3.2.2.1 Distance standard et distance standard pondérée\nEn statistique univariée, l’écart-type (équation 3.7) est une mesure de dispersion bien connue : plus la valeur de l’écart-type est élevée, plus la dispersion des valeurs de la variable autour de la moyenne (\\(\\mu\\)) est importante.\n\\[\n\\sigma=\\sqrt{\\frac{\\sum_{i=1}^n (x_{i}-\\mu)^2}{n}}\n\\tag{3.7}\\]\n\n\n\n\n\nÉcart-type\n\n\nPour un rappel sur l’écart-type, consultez la section intitulée Paramètres de dispersion (Apparicio et Gelb 2022).\n\n\nDistance standard (pondérée ou non) des X et des Y\nDe manière analogue à l’écart-type, nous pouvons calculer la distance standard pour les coordonnées X et pour les coordonnées Y des points, soit l’écart moyen respectif au centre moyen (équation 3.8) ou au centre moyen pondéré (équation 3.9).\n\\[\n\\sigma_x=\\sqrt{\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x}_{cm})^2}{n}} \\text{ et }\n\\sigma_y=\\sqrt{\\frac{\\sum_{i=1}^n (y_{i}-\\bar{y}_{cm})^2}{n}}\n\\tag{3.8}\\]\n\\[\n\\sigma_{xw}=\\sqrt{\\frac{\\sum_{i=1}^n w_i(x_{i}-\\bar{x}_{cmp})^2}{\\sum_{i=1}^n{w_i}}} \\text{ et } \\sigma_{yw}=\\sqrt{\\frac{\\sum_{i=1}^n w_i(y_{i}-\\bar{y}_{cmp})^2}{\\sum_{i=1}^n{w_i}}}\n\\tag{3.9}\\]\nDistance standard (pondérée ou non)\nNous pouvons aussi calculer la distance standard (\\(ds\\)) sans pondération (équation 3.10) et avec pondération (équation 3.11). Plus elle est forte, plus les points sont dispersés autour du centre moyen ou du centre moyen pondéré. Inversement, une faible distance standard indique une concentration de points autour du centre moyen.\n\\[\nds=\\sqrt{\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x}_{cm})^2}{n} + \\frac{\\sum_{i=1}^n (y_{i}-\\bar{y}_{cm})^2}{n}}\n\\tag{3.10}\\]\n\\[\nds_w=\\sqrt{\\frac{\\sum_{i=1}^n w_i(x_{i}-\\bar{x}_{cmp})^2}{\\sum_{i=1}^n w_i} + \\frac{\\sum_{i=1}^n w_i(y_{i}-\\bar{y}_{cmp})^2}{\\sum_{i=1}^n w_i}}\n\\tag{3.11}\\]\nDe nouveau, ces mesures peuvent être adaptées pour des points avec une élévation (\\(x,y,z\\)) :\n\\[\nds=\\sqrt{\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x}_{cm})^2}{n} + \\frac{\\sum_{i=1}^n (y_{i}-\\bar{y}_{cm})^2}{n} + \\frac{\\sum_{i=1}^n (z_{i}-\\bar{z}_{cm})^2}{n}}\n\\tag{3.12}\\]\n\\[\nds_w=\\sqrt{\\frac{\\sum_{i=1}^n w_i(x_{i}-\\bar{x}_{cmp})^2}{\\sum_{i=1}^n w_i} + \\frac{\\sum_{i=1}^n w_i(y_{i}-\\bar{y}_{cmp})^2}{\\sum_{i=1}^n w_i} + \\frac{\\sum_{i=1}^n w_i(z_{i}-\\bar{z}_{cmp})^2}{\\sum_{i=1}^n w_i}}\n\\tag{3.13}\\]\n\n3.2.2.2 Représenter la dispersion : cercle de distance standard et ellipse\nLa dispersion d’un semis de points peut être représentée de quatre manières différentes :\n\nUne enveloppe convexe des points décrite à la section 1.2.2.5).\nUn rectangle centré sur le centre moyen avec les déviations des coordonnées X et Y pondérées ou non (équation 3.8 et équation 3.9 décrites précédemment). Dans le cas de données comprenant l’élévation, la forme géométrique est un parallélépipède rectangle.\nUn cercle de rayon de distance standard pondérée ou non (équation 3.10 et équation 3.11) décrites précédemment). Dans le cas de données comprenant l’élévation, la forme géométrique est une sphère de rayon de distance standard.\nUne ellipse de distance standard pondérée ou non. Dans le cas de données comprenant l’élévation, la forme géométrique est un ellipsoïde.\n\nPrenons un jeu de données fictives pour décrire ces quatre représentations graphiques. Imaginons que nous avons observé le parc de la Laurentie à Sherbrooke pour comprendre son utilisation. Pour collecter des données sur la localisation des personnes utilisatrices, nous aurions pu utiliser un questionnaire dans QField ou ArcGIS Survey 123. La figure 3.1 illustre la localisation de dix personnes (points rouges).\n\n\nFigure 3.1: Données fictives sur des personnes utilisatrices du parc de la Laurentie à Sherbrooke\n\nÀ partir de ces dix points, nous obtenons :\n\nLes coordonnées du centre moyen qui sont égales à -8 007 869 et 5 685 921, soit simplement les moyennes des coordonnées X et Y des dix observations.\nL’enveloppe convexe des points qui contient tous les points.\nLe rectangle construit avec les déviations standards des X et des Y qui est centré sur le centre moyen. À partir de ce point, nous ajoutons à l’est et à l’ouest la valeur de la distance standard des X et au nord et au sud celle des Y. Les valeurs de ces distances standards sont égales à 32,60 et 52,20 mètres (voir les calculs au tableau 3.2).\nLa distance standard est égale à 61,59 mètres (voir les calculs au tableau 3.2). À partir de cette distance, nous traçons le cercle ayant comme rayon la distance standard.\nL’ellipse de déviation de distance standard (figure 3.2).\n\n\n\n\n\nTableau 3.2: Calcul des distances standards des X et des Y et de la distance standard\n\n\n\n\n\n\n\n\n\nPoint\n\\(x_i\\)\n\\(y_i\\)\n\\((x_{i}-\\bar{x}_{cm})^2\\)\n\\((y_{i}-\\bar{y}_{cm})^2\\)\n\n\\((x_{i}-\\bar{x}_{cm})^2+\\)\\((y_{i}-\\bar{y}_{cm})^2\\)\n\n\n\n\n1\n-8007877\n5686000\n52,80\n6347,70\n6400,50\n\n\n2\n-8007843\n5685993\n716,90\n5298,00\n6014,90\n\n\n3\n-8007836\n5685976\n1082,00\n3046,30\n4128,30\n\n\n4\n-8007859\n5685928\n98,90\n60,30\n159,20\n\n\n5\n-8007911\n5685935\n1770,20\n214,60\n1984,80\n\n\n6\n-8007913\n5685871\n1934,80\n2461,00\n4395,80\n\n\n7\n-8007888\n5685855\n365,70\n4363,40\n4729,10\n\n\n8\n-8007835\n5685857\n1185,00\n4016,80\n5201,80\n\n\n9\n-8007824\n5685886\n2071,70\n1203,50\n3275,20\n\n\n10\n-8007906\n5685904\n1376,50\n266,70\n1643,30\n\n\nn\n10\n\n\n\n\n\n\nSomme\n-80078693\n56859207\n10654,50\n27278,30\n37932,80\n\n\nMoyenne\n-8007869\n5685921\n1065,45\n2727,83\n3793,28\n\n\nRacine carrée\n\n\n32,60\n52,20\n61,59\n\n\n\n\n\n\n\n\nFigure 3.2: Trois éléments composant une ellipse\n\n\n\n\n\n\nCalcul des ellipses : des résultats qui varient d’un logiciel à l’autre…\n\n\nIl existe plusieurs solutions pour tracer une ellipse. Pour une discussion détaillée de ces différentes solutions, lisez le court texte très intéressant de Martin Leroux.\n\nEllipse de Yuill (Tveite 2020).\nEllipse basée sur la covariance implémentée dans ArcGIS Pro.\nEllipse de distance standard implémentée dans le logiciel CrimeStat.\nEllipse avec la correction proposée par Wang et ses collègues (2015).\n\nNotez qu’un plugin QGIS nommé The QGIS Standard Deviational Ellipse Plugin (Tveite 2020) intègre plusieurs de ces méthodes. Les résultats varient ainsi d’un logiciel à l’autre selon la méthode implémentée. Autrement dit, pour un même jeu de données ponctuelles, les ellipses obtenues avec ArcGIS Pro, ArcMap 9.3, ArcMap 10.x et QGIS seront différentes.\nQuoi faire alors?\nQuelle que soit la méthode utilisée, l’ellipse est toujours centrée sur le point moyen et a toujours le même angle de rotation. Par contre, la taille de l’ellipse (superficie) varie. Par conséquent, si vous souhaitez comparer des ellipses différentes, assurez-vous toujours qu’elles sont toutes obtenues dans le même logiciel et avec la même solution.\n\n\n\n\n\n\n\nCalcul de l’ellipse selon la méthode implémentée dans CrimeStat\n\n\nNed Levine (2006; 2021), créateur du logiciel CrimeStat, propose les formulations suivantes pour le calcul d’une ellipse :\n\\[\n\\theta = \\frac{\\text{arctan} \\Biggl\\{ \\Bigl(\\sum_{i=1}^nx_d^2-\\sum_{i=1}^ny_d^2 \\Bigl)+  \\Bigr[\\Bigl(\\sum_{i=1}^nx_d^2-\\sum_{i=1}^ny_d^2 \\Bigl)^2 + 4 \\Bigl(\\sum_{i=1}^nx_dy_d \\Bigl)^2\\Bigr]^{1/2} \\Biggl\\}} {2 \\sum_{i=1}^nx_dy_d}\n\\tag{3.14}\\]\navec \\(\\theta\\) est la rotation de l’ellipse, \\(x_d = x_i-\\bar{x}\\) et \\(y_d = y_i-\\bar{y}\\).\n\\[\n\\sigma_x =\\sqrt{2\\times \\frac{\\sum_{i=1}^n\\Bigl((x_i-\\bar{x}) \\text{cos}\\theta-(y_i-\\bar{y})\\text{sin}\\theta\\Bigl)^2}{n-2}}\n\\tag{3.15}\\]\n\\[\n\\sigma_y =\\sqrt{2\\times \\frac{\\sum_{i=1}^n\\Bigl((x_i-\\bar{x}) \\text{sin}\\theta-(y_i-\\bar{y})\\text{cos}\\theta\\Bigl)^2}{n-2}}\n\\tag{3.16}\\]\n\\[\nl_x=2\\sigma_x \\text{ et }l_y=2\\sigma_y \\text{ et }S_e=\\pi\\sigma_x\\sigma_y\n\\tag{3.17}\\]\navec \\(l_x\\), \\(l_y\\) et \\(S_e\\) étant les longueurs de axes X et Y et la superficie de l’ellipse.\n\n\nPrenons quatre situations fictives de répartition de dix personnes utilisatrices du parc de la Laurentie à Sherbrooke :\n\n\nSituation A. Les observations sont concentrées autour de l’aire de jeu.\n\nSituation B. Les observations sont dispersées dans la partie est du parc.\n\nSituation C. Les observations sont concentrées dans la partie nord du parc.\n\nSituation D. Les observations sont concentrées dans la partie nord du parc, excepté deux observations au sud.\n\n\n\nFigure 3.3: Données fictives sur des personnes utilisatrices du parc de la Laurentie à Sherbrooke (quatre situations)\n\nLes cercles et les ellipses de distance standard (\\(ds\\)) centrés au centre moyen (\\(cm\\)) sont représentés à la (figure 3.4).\n\n\nFigure 3.4: Ellipse et cercle de distance standard pour les quatre situations\n\n\n3.2.2.3 Comparaison de la dispersion de deux semis de points dans deux régions différentes\nPour comparer la dispersion de deux semis de points situés dans des régions de taille différente, il convient de supprimer les effets de taille des deux régions. Pour ce faire, nous divisons la distance standard ou la distance standard pondérée par la superficie de la région. Cette approche est donc très similaire au coefficient de variation en statistique univariée, soit le rapport entre l’écart-type et la moyenne.\nPar exemple, si nous comparons les dispersions des personnes utilisatrices du parc de la Laurentie (0,078 ha) et parc du Mont-Bellevue (409 ha). Inévitablement, la valeur de la distance standard est plus forte pour le parc du Mont-Bellevue que celle du parc de la Laurentie. Il faut donc diviser chaque distance standard par la superficie associée.\n\n3.2.2.4 Comparaison de la dispersion de deux semis de points dans la même région\nPour comparer la distribution spatiale de deux semis de points situés dans la même région, nous comparons leur cercle ou leur ellipse respective (par exemple, des points représentant des accidents l’été versus l’hiver ou encore deux espèces végétales sur le même territoire).\nUne démarche similaire peut être appliquée à deux groupes de population rattachés à des entités polygonales : ils ont la même distribution spatiale si les deux ellipses de distance pondérée se juxtaposent significativement. David W.S. Wong (1999) propose d’ailleurs un indice dénommé S basé sur la comparaison des deux ellipses (équation 3.18). Le numérateur représente la surface d’intersection entre les deux ellipses tandis que le dénominateur représente leur surface d’union. L’indice varie de 0 à 1, soit respectivement d’une similitude parfaite à une dissemblance la plus grande entre les deux distributions spatiales :\n\nSi deux groupes de population ont des distributions spatiales identiques, les ellipses sont les mêmes et donc \\(E_i \\cap E_j = E_i \\cup E_j = 1\\) et \\(S_{ij} = 1 - 1 = 0\\).\nSi les deux groupes de population ont des distributions spatiales totalement différentes, les ellipses ne se touchent pas, alors \\(E_i \\cap E_j = 0\\) et la valeur de \\(S = 1 - 0 = 1\\).\n\nBien évidemment, cet indice peut être étendu pour comparer les distributions de plus de deux groupes de population simultanément (équation 3.19).\n\\[\nS_{ij}=1-\\frac{E_i \\cap E_j}{E_i \\cup E_j}\n\\tag{3.18}\\]\n\\[\nS=1-\\frac{E_1 \\cap E_2 \\cap E_3 \\cap \\text{…} \\cap E_n}{E_1 \\cup E_2 \\cup E_3 \\cup \\text{…} \\cup E_n}\n\\tag{3.19}\\]\nVoyons un exemple concret : calculons l’indice de Wong (1999) (équation 3.18) pour les ellipses de distances standards pondérées par les effectifs de propriétaires et de locataires par secteur de recensement pour la ville de Sherbrooke en 2021 (figure 3.5). D’emblée, nous constatons que les propriétaires ont une distribution plus dispersée que celle des locataires plus présents dans le centre de la ville.\n\n\n\n\nFigure 3.5: Propriétaires et locataires dans la ville de Sherbrooke (avec ellipse de distance standard), 2021\n\n\n\nLe code suivant permet d’obtenir l’indice de Wong (1999).\n\n## Importation des deux ellipses \nE1 &lt;- st_read(dsn = \"data/chap03/EllipseProprio.shp\", quiet=TRUE) \nE2 &lt;- st_read(dsn = \"data/chap03/EllipseLocataire.shp\", quiet=TRUE) \n## Intersection \nInter &lt;- st_intersection(E1, E2)\n## Union\nUnion &lt;- st_union(E1, E2)\n## Calcul de l'indice de Wong\nWong &lt;- 1 - ( as.numeric(st_area(Inter)) / as.numeric(st_area(Union)))\nprint(paste0(\"Valeur de l'indice de Wong : \",round(Wong,3)))\n\n[1] \"Valeur de l'indice de Wong : 0.515\"\n\n\n\n3.2.3 Mise en œuvre de l’analyse centrographique dans R\n\n3.2.3.1 Calcul de mesures non pondérées\nPour illustrer la mise en œuvre des différentes mesures de l’analyse centrographique dans R, nous utilisons un jeu de données ouvertes sur les incidents de sécurité publique de la ville de Sherbrooke. Dans le code ci-dessous, nous importons les données et constituons une couche sf par année (2019 à 2022) et extrayons les coordonnées géographiques.\n\nlibrary(sf)\nlibrary(tmap)\n## Importation des données \nArrondissements &lt;-  st_read(dsn = \"data/chap03/Arrondissements.shp\", quiet=TRUE)\nIncidents &lt;- st_read(dsn = \"data/chap03/IncidentsSecuritePublique.shp\", quiet=TRUE)\n## Changement de projection\nArrondissements &lt;- st_transform(Arrondissements, crs = 3798) \nIncidents &lt;- st_transform(Incidents, crs = 3798)\n## Extaction des méfaits\nMefaits &lt;- subset(Incidents, DESCRIPTIO == \"Méfait\")\n# Méfaits par année\nM2019 &lt;- subset(Mefaits, ANNEE==2019)\nM2020 &lt;- subset(Mefaits, ANNEE==2020)\nM2021 &lt;- subset(Mefaits, ANNEE==2021)\nM2022 &lt;- subset(Mefaits, ANNEE==2022)\n# Coordonnées géographiques\nxy.2019 &lt;- st_coordinates(M2019)\nxy.2020 &lt;- st_coordinates(M2020)\nxy.2021 &lt;- st_coordinates(M2021)\nxy.2022 &lt;- st_coordinates(M2022)\n\nLes méfaits par année sont présentés à la figure 3.6.\n\n\n\n\nFigure 3.6: Localisation des méfaits par année, ville de Sherbrooke, 2021\n\n\n\nCentre moyen\nAvec la fonction mean, nous pouvons calculer les valeurs moyennes sur les coordonnées X et Y, puis créer un objet sf avec les centres moyens.\n\n## Récupération de la projection cartographique dans une variable\nProjCarto &lt;- st_crs(Mefaits)\n## Calcul du centre moyen pour une année (2019)\nprint(c(mean(xy.2019[,1]), mean(xy.2019[,2])))\n\n[1] 649990.3 157059.3\n\n## Calcul pour toutes les années\n# vecteur pour les moyennes des X\nX.moy &lt;- c(mean(xy.2019[,1]), mean(xy.2020[,1]), mean(xy.2021[,1]), mean(xy.2022[,1]))\n# Vecteur pour les moyennes des Y\nY.moy &lt;- c(mean(xy.2019[,2]), mean(xy.2020[,2]), mean(xy.2021[,2]), mean(xy.2022[,2]))\n# Enregistrement dans un objet sf\nCentreMoyen &lt;- data.frame(Annee = c(\"2019\", \"2020\", \"2021\", \"2022\"),\n                          X = X.moy, \n                          Y = Y.moy,\n                          CMx = X.moy,  \n                          CMy = Y.moy)\nCentreMoyen &lt;- st_as_sf(CentreMoyen, coords = c(\"X\", \"Y\"), crs = ProjCarto)\n# Affichage des résultats\nprint(CentreMoyen)\n\nSimple feature collection with 4 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 649696 ymin: 157059.3 xmax: 650663.7 ymax: 157544.8\nProjected CRS: NAD83 / MTQ Lambert\n  Annee      CMx      CMy                  geometry\n1  2019 649990.3 157059.3 POINT (649990.3 157059.3)\n2  2020 649696.0 157544.8   POINT (649696 157544.8)\n3  2021 650663.7 157214.8 POINT (650663.7 157214.8)\n4  2022 650442.5 157237.0   POINT (650442.5 157237)\n\n\nPoint central\nLe code ci-dessous illustre comment identifier le point central, qui fait partie du jeu de données, pour l’année 2019.\n\n## Calcul de la matrice de distances entre les points de l'année 2019\nDistMatrice2019 &lt;- as.matrix(dist(xy.2019, method = \"euclidean\", diag = TRUE, upper = TRUE))\n## Somme de chaque ligne de la matrice, soit la somme des distances à tous les autres points\nM2019$DistATous &lt;- rowSums(DistMatrice2019)\n## Sélection du point avec plus petite distance à tous les autres\nPointCentral2019 &lt;- subset(M2019, M2019$DistATous==min(M2019$DistATous))\n\nDistance standard sur les coordonnées X et Y et distance standard combinée\nLe code ci-dessous permet de calculer les trois distances standards.\n\n## Calcul de la distance standard pour une année (2019)\n# Distance standard sur les coordonnées X et Y\nc(sqrt(mean((xy.2019[,1] - mean(xy.2019[,1]))^2)),\n  sqrt(mean((xy.2019[,2] - mean(xy.2019[,2]))^2)))\n\n[1] 3732.173 2593.207\n\n# Distance standard\nsqrt(mean((xy.2019[,1] - mean(xy.2019[,1]))**2 + \n            (xy.2019[,2] - mean(xy.2019[,2]))**2))\n\n[1] 4544.649\n\n## Calcul pour toutes les années et enregistrement des centres moyens dans de nouveaux champs\nCentreMoyen$DS.X &lt;- c(sqrt(mean((xy.2019[,1] - mean(xy.2019[,1]))^2)),\n                      sqrt(mean((xy.2020[,1] - mean(xy.2020[,1]))^2)),\n                      sqrt(mean((xy.2021[,1] - mean(xy.2021[,1]))^2)),\n                      sqrt(mean((xy.2022[,1] - mean(xy.2022[,1]))^2)))\n\nCentreMoyen$DS.Y &lt;- c(sqrt(mean((xy.2019[,2] - mean(xy.2019[,2]))^2)),\n                       sqrt(mean((xy.2020[,2] - mean(xy.2020[,2]))^2)),\n                       sqrt(mean((xy.2021[,2] - mean(xy.2021[,2]))^2)),\n                       sqrt(mean((xy.2022[,2] - mean(xy.2022[,2]))^2)))\n\nCentreMoyen$DS &lt;- c(sqrt(mean((xy.2019[,1] - mean(xy.2019[,1]))**2 +\n                              (xy.2019[,2] - mean(xy.2019[,2]))**2)),\n                    sqrt(mean((xy.2020[,1] - mean(xy.2020[,1]))**2 +\n                              (xy.2020[,2] - mean(xy.2020[,2]))**2)),+\n                    sqrt(mean((xy.2021[,1] - mean(xy.2021[,1]))**2 +\n                              (xy.2021[,2] - mean(xy.2021[,2]))**2)),+\n                    sqrt(mean((xy.2022[,1] - mean(xy.2022[,1]))**2 +\n                              (xy.2022[,2] - mean(xy.2022[,2]))**2)))\n## Visualisation des résultats\nhead(CentreMoyen)\n\nSimple feature collection with 4 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 649696 ymin: 157059.3 xmax: 650663.7 ymax: 157544.8\nProjected CRS: NAD83 / MTQ Lambert\n  Annee      CMx      CMy                  geometry     DS.X     DS.Y       DS\n1  2019 649990.3 157059.3 POINT (649990.3 157059.3) 3732.173 2593.207 4544.649\n2  2020 649696.0 157544.8   POINT (649696 157544.8) 4134.913 2393.048 4777.466\n3  2021 650663.7 157214.8 POINT (650663.7 157214.8) 3374.821 2294.171 4080.764\n4  2022 650442.5 157237.0   POINT (650442.5 157237) 3601.208 2170.215 4204.585\n\n\nReprésentations graphiques de la dispersion\nUne fois que la couche sf des centres moyens avec les trois champs pour la distance standard est créée, il suffit de tracer un rectangle et un cercle de distance standard.\n\n## Enveloppe convexe\nsf.Enveloppes &lt;- st_sf(data.frame(Id=c(\"2019\", \"2020\", \"2021\", \"2022\")),\n                    geometry = c(st_convex_hull(st_union(M2019)),\n                                 st_convex_hull(st_union(M2020)),\n                                 st_convex_hull(st_union(M2021)),\n                                 st_convex_hull(st_union(M2022))))\n## Rectangle avec les distances standards sur les coordonnées X et Y\n#' Fonction pour tracer le rectangle\n#' @param MoyX coordonnées X du centre moyen.\n#' @param MoyY coordonnées Y du centre moyen.\n#' @param SDx distance standard sur les coordonnées X.\n#' @param SDy distance standard sur les coordonnées Y.\n#' @param crs projection cartographique.\nCreationRec &lt;- function(MoyX, MoyY, SDx, SDy, ProjCarto){\n  pt1 = c(MoyX - SDx, MoyY - SDy)\n  pt2 = c(MoyX - SDx, MoyY + SDy)\n  pt3 = c(MoyX + SDx, MoyY + SDy)\n  pt4 = c(MoyX + SDx, MoyY - SDy)\n  Rectangle = st_polygon(list(rbind(pt1, pt2, pt3, pt4, pt1)))\n  Rectangle = st_sfc(Rectangle)\n  st_crs(Rectangle) = ProjCarto\n  return(Rectangle)\n}\nMoyX &lt;- CentreMoyen$CMx\nMoyY &lt;- CentreMoyen$CMy\nSDx &lt;- CentreMoyen$DS.X\nSDy &lt;- CentreMoyen$DS.Y\nsf.Rectangles &lt;- st_sf(data.frame(Id=c(\"2019\", \"2020\", \"2021\", \"2022\")),\n                    geometry = c(CreationRec(MoyX[1], MoyY[1], SDx[1], SDy[1], ProjCarto),\n                                 CreationRec(MoyX[2], MoyY[2], SDx[2], SDy[2], ProjCarto),\n                                 CreationRec(MoyX[3], MoyY[3], SDx[3], SDy[3], ProjCarto),\n                                 CreationRec(MoyX[4], MoyY[4], SDx[4], SDy[4], ProjCarto)))\n## Cercle de distance standard avec la fonction st_buffer\nsf.CercleDS &lt;- st_buffer(CentreMoyen, dist = CentreMoyen$DS)\n\nLe calcul de l’ellipse est un peu plus complexe. Par conséquent, nous avons écrit deux fonctions :\n\nCreateEllipse qui construit une ellipse à partir d’une couche sf de points.\nCreateEllipse_gp qui construit des ellipses à partir d’une couche sf de points en fonction d’une colonne indiquant différents groupes de points (ici, les différentes années).\n\n\n## Appel des deux fonctions dans le fichier ellipses.R\nsource(\"code_complementaire/ellipses.R\")\n## Création d'une ellipse pour une année \nsf.Ellipse2019 &lt;- CreateEllipse(M2019)\n## Création de plusieurs ellipses regroupées selon les différentes années\nsf.Ellipse &lt;- CreateEllipse_gp(points = Mefaits, group = \"ANNEE\")\nhead(sf.Ellipse)\n\nSimple feature collection with 4 features and 12 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 643833.8 ymin: 153377.3 xmax: 655559 ymax: 160937.9\nProjected CRS: NAD83 / MTQ Lambert\n       CMx      CMy   Sigmax   Sigmay       Lx        Ly     Aire    Theta\n1 649990.3 157059.3 3055.157 5683.790 6110.315 11367.580 54553357 64.60944\n2 649696.0 157544.8 3154.712 5994.647 6309.423 11989.294 59411858 75.81068\n3 650663.7 157214.8 3128.891 4869.299 6257.783  9738.598 47863757 76.14675\n4 650442.5 157237.0 2540.594 5406.184 5081.187 10812.368 43149511 68.52718\n  ThetaCorr  Major  Minor                       geometry ANNEE\n1  64.60944 SigmaY SigmaX POLYGON ((655125.1 159496.4...  2019\n2  75.81068 SigmaY SigmaX POLYGON ((655507.7 159014.3...  2020\n3  76.14675 SigmaY SigmaX POLYGON ((655391.4 158380.7...  2021\n4  68.52718 SigmaY SigmaX POLYGON ((655473.5 159216, ...  2022\n\n\nLes quatre représentations de la dispersion sont présentées à la figure 3.7 : 1) enveloppe convexe (gris), 2) cercle de distance standard (bleu), 3) rectangle de distance standard sur les X et Y (noir) et 4) ellipse de distance standard (rouge).\n\n\n\n\nFigure 3.7: Représentations de la dispersion des méfaits pour les quatre années\n\n\n\n\n3.2.3.2 Calcul de mesures pondérées\nPour illustrer le calcul des mesures pondérées, nous utilisons des données sur les effectifs des premier et dernier déciles de revenu après impôt des familles économiques pour les secteurs de recensement de la ville de Sherbrooke en 2021 (figure 3.8).\n\n\n\n\nFigure 3.8: Déciles extrêmes de revenu après impôt des familles économiques\n\n\n\nLe code suivant permet d’importer et de structurer les données, puis de calculer les différentes mesures (centre moyen pondéré et distance standard pondérée).\n\n## Importation et structuration des données\ndfdecile &lt;- read.csv(\"data/chap03/DataDecilesSR.csv\", header = TRUE, sep = \",\")\ndfdecile$SRIDU &lt;- substr(dfdecile$SRIDU, 1, 10)\nSR &lt;- st_read(dsn = \"data/chap03/Recen2021Sherbrooke.gpkg\",\n              layer = \"DR_SherbSRDonnees2021\", quiet=TRUE)\nSR &lt;- merge(SR[,c(\"SRIDU\")], dfdecile, by = \"SRIDU\")\nProjCarto &lt;- st_crs(SR)\n## Coordonnées géographiques des secteurs de recensement\nxy &lt;- st_coordinates(st_point_on_surface(SR))\n## Pondérations pour les deux déciles\nwd1  &lt;- SR$D1\nwd10 &lt;- SR$D10\n## Sommes des pondérations\nswd1  &lt;- sum(wd1)\nswd10 &lt;- sum(wd10)\n## Calcul du centre pondéré\nXmoyD1  &lt;- sum(xy[,1]*wd1) / swd1\nXmoyD10 &lt;- sum(xy[,1]*wd10) / swd10\nYmoyD1  &lt;- sum(xy[,2]*wd1) / swd1\nYmoyD10 &lt;- sum(xy[,2]*wd10) / swd10\nCentreMoyenPond &lt;- data.frame(Decile = c(\"Premier\", \"Dernier\"),\n                          X = c(XmoyD1, XmoyD10), \n                          Y = c(YmoyD1, YmoyD10),\n                          CMwx = c(XmoyD1, XmoyD10),  \n                          CMwy = c(YmoyD1, YmoyD10))\nCentreMoyenPond &lt;- st_as_sf(CentreMoyenPond, coords = c(\"X\", \"Y\"), crs = ProjCarto)\n## Calcul de la distance standard pondérée\nsdD1  &lt;- sqrt((sum(wd1*(xy[,1]- XmoyD1)^2) / swd1) + (sum(wd1*(xy[,2]- YmoyD1)^2) / swd1))\nsdD10 &lt;- sqrt((sum(wd10*(xy[,1]- XmoyD10)^2) / swd1) + (sum(wd10*(xy[,2]- YmoyD10)^2) / swd10))\n## Zones tampons avec la distance standard pondérée\nCentreMoyenPond$SDw &lt;- c(sdD1, sdD10)\nsf.CercleDSW &lt;- st_buffer(CentreMoyenPond, dist = CentreMoyenPond$SDw)\n\n## Calcul de l'ellipse pondérée pour le premier décile\nSR.points &lt;- st_point_on_surface(SR)\nellipse.D1 &lt;- CreateEllipse(SR.points, w = SR.points$D1)\nellipse.D10 &lt;- CreateEllipse(SR.points, w = SR.points$D10)\n\n# Carte 1 : premier décile\nCarte1 = tm_shape(SR)+\n              tm_polygons(col=\"whitesmoke\", border.col = \"grey30\", lwd = 1)+\n          tm_shape(CentreMoyenPond[1,])+\n              tm_dots(size = .5, col=\"black\")+\n          tm_shape(sf.CercleDSW[1,])+\n              tm_borders(col=\"blue\", lwd = 2)+      \n          tm_shape(ellipse.D1)+\n              tm_borders(col=\"red\", lwd = 2)+    \n  tm_layout(main.title = \"A. Premier décile (le plus pauvre)\",\n            main.title.size = .9, frame = FALSE)\n# Carte 2 : dernier décile\nCarte2 = tm_shape(SR)+\n              tm_polygons(col=\"whitesmoke\", border.col = \"grey30\", lwd = 1)+\n          tm_shape(CentreMoyenPond[2,])+\n              tm_dots(size = .5, col=\"black\")+\n          tm_shape(sf.CercleDSW[2,])+\n              tm_borders(col=\"red\", lwd = 2)+         \n          tm_shape(ellipse.D10)+\n              tm_borders(col=\"blue\", lwd = 2)+    \n tm_layout(main.title = \"B. Dernier décile (le plus riche)\",\n            main.title.size = .9, frame = FALSE)+\ntm_credits(\"Source : recensement de 2021, Statistique Canada\\nAuteur : Jéremy Lécartemplace.\", \n             position = c(\"right\", \"bottom\"), size = 0.7, align = \"right\")\n# Combinaison des deux cartes\ntmap_arrange(Carte1, Carte2, ncol = 2, nrow = 1)\n\n\n\nFigure 3.9: Cercles de distance standard et ellipses pondérés"
  },
  {
    "objectID": "03-MethodesRepartitionPonctuelle.html#sec-033",
    "href": "03-MethodesRepartitionPonctuelle.html#sec-033",
    "title": "3  Méthodes de répartition ponctuelle",
    "section": "\n3.3 Forme d’un semis de points",
    "text": "3.3 Forme d’un semis de points\nÉtudier la forme d’un semis de points, c’est vouloir décrire l’arrangement spatial et l’espacement des points dans une région donnée. Autrement dit, l’objectif est de répondre à la question suivante : comment se répartissent les points dans une région donnée? Nous distinguons habituellement trois types de distribution spatiale d’un semis de points (figure 3.10) :\n\n\nDistribution dispersée quand les points du semis sont régulièrement espacés.\n\nDistribution aléatoire quand la distribution des points n’est nullement guidée par des considérations géographiques. Autrement dit, chaque point du semis a la même probabilité d’être situé dans n’importe quelle partie de la zone d’étude.\n\nDistribution concentrée quand il existe des regroupements de points dans une ou plusieurs parties de la région d’étude. Par exemple, les musées et les théâtres sont habituellement concentrés dans les parties centrales des métropoles.\n\n\n\nFigure 3.10: Trois types de distribution spatiale d’un semis de points\n\n\n\n\n\n\nDeux grandes familles pour décrire la forme d’un semis de points\n\n\n\nCelles basées sur la distance (l’indice du plus proche voisin, fonctions K et L de Ripley) (section 3.3.1).\nCelles basées sur la densité (méthode des quadrats avec différents tests statistiques) (section 3.3.2).\n\n\n\n\n3.3.1 Méthode du plus proche voisin\nLe principe de base de cette méthode est fort simple et se décompose en quatre étapes :\n\nMesurer, pour chaque point du semis, la distance le séparant du point le plus proche, puis calculer la distance moyenne du point le plus proche (équation 3.20).\nCalculer la moyenne attendue du point le plus proche pour une dispersion aléatoire (équation 3.22).\n\nCalculer l’indice du plus proche voisin, soit le ratio entre la distance observée et la distance aléatoire (équation 3.21). L’indice R s’interprète alors comme suit :\n\nSi R est égal à 1, la dispersion du semis de points est aléatoire.\nSi R est inférieur à 1, la distribution du semis de points tend vers la concentration (avec une concentration absolue quand R = 0; tous les points ont les mêmes coordonnées géographiques).\nSi R est supérieur à 1, la distribution du semis de points tend vers la dispersion.\n\n\nCalculer les valeurs de Z et de p pour déterminer si la valeur de R obtenue est significative (équation 3.23).\n\n\\[\nR_{o}= \\frac{\\sum_{i=1}^n d_i}{n}\n\\tag{3.20}\\]\n\\[\nR_{a}= \\frac{1}{2 \\sqrt{(n/S)}}\n\\tag{3.21}\\]\n\\[\nR = \\frac{R_{o}}{R_{a}}\n\\tag{3.22}\\]\n\\[\nZ = \\frac{R_o-R_a}{SE} \\text{, } SE = \\frac{0.26136}{\\sqrt{(n^2/S)}} \\text{ avec :}\n\\tag{3.23}\\]\n\n\n\\(n\\), nombre de points.\n\n\\(d_i\\), distance séparant le point i de son voisin le plus proche.\n\n\\(S\\), superficie de l’espace d’étude.\n\nLe code ci-dessous permet de mettre en œuvre la méthode du plus proche voisin pour les méfaits pour les quatre années.\n\nlibrary(spatstat)\nlibrary(ggplot2)\n## Indice du plus proche voisin : R observé (équation 3.20)\n# le paramètre k indique le nombre de plus proches voisins\nRobs2019 &lt;- mean(nndist(st_coordinates(M2019),k=1))\nRobs2020 &lt;- mean(nndist(st_coordinates(M2020),k=1))\nRobs2021 &lt;- mean(nndist(st_coordinates(M2021),k=1))\nRobs2022 &lt;- mean(nndist(st_coordinates(M2022),k=1))\n## Indice du plus proche voisin : R attendu (distribution aléatoire) (équation 3.21)\n# Attention, il faut spéficier S, la superficie de l'espace d'étude\nArrondissements &lt;-  st_read(dsn = \"data/chap03/Arrondissements.shp\", quiet=TRUE)\nArrondissements &lt;- st_transform(Arrondissements, crs = 3798) \nS &lt;- as.numeric(st_area(st_union(Arrondissements)))\n# Nombre de points par année\nN2019 &lt;- nrow(M2019)\nN2020 &lt;- nrow(M2020)\nN2021 &lt;- nrow(M2021)\nN2022 &lt;- nrow(M2022)\n# Calcul de Ra\nRa2019 &lt;- 1 / (2 * sqrt(N2019 / S))\nRa2020 &lt;- 1 / (2 * sqrt(N2020 / S))\nRa2021 &lt;- 1 / (2 * sqrt(N2021 / S))\nRa2022 &lt;- 1 / (2 * sqrt(N2022 / S))\n## Calculons le R\n# Création d'un DataFrame\nIndicePPV &lt;- data.frame(id = c(\"2019\", \"2020\", \"2021\", \"2022\"),\n                               points = c(N2019, N2020, N2021, N2022),\n                               Superficie = c(S, S, S, S),\n                               Robs = c(Robs2019, Robs2020, Robs2021, Robs2022),\n                               Rattendu = c(Ra2019, Ra2020, Ra2021, Ra2022))\n# Calcul du R (équation 3.22)\nIndicePPV$R &lt;- IndicePPV$Robs / IndicePPV$Rattendu\n# Calcul du Z (équation 3.23)\nIndicePPV$SE &lt;- 0.26136 / sqrt(IndicePPV$points^2 / IndicePPV$Superficie)\nIndicePPV$Z &lt;- (IndicePPV$Robs - IndicePPV$Rattendu) / IndicePPV$SE\nIndicePPV$P &lt;- round(2*pnorm(q=abs(IndicePPV$Z), lower.tail=FALSE),3)\nprint(IndicePPV)\n\n    id points Superficie     Robs Rattendu         R       SE         Z P\n1 2019    251  366358191 251.5498 604.0684 0.4164260 19.93051 -17.68739 0\n2 2020    383  366358191 183.9866 489.0166 0.3762380 13.06151 -23.35335 0\n3 2021    344  366358191 227.1881 515.9929 0.4402931 14.54232 -19.85961 0\n4 2022    220  366358191 251.3347 645.2256 0.3895299 22.73890 -17.32234 0\n\n\nInterprétation des résultats\nAnalysons les différentes colonnes du tableau 3.3 :\n\npoints (n) : il y a respectivement 251, 383, 344 et 220 méfaits pour les années 2019 à 2022.\nR observé : en moyenne, un méfait est distant de 252, 184, 227 et 251 mètres du méfait le plus proche pour les quatre années.\nR attendu : pour une distribution aléatoire, un méfait devrait être distant du méfait le plus proche de 604, 489, 516, et 645 mètres.\nIndice du plus proche voisin : toutes les valeurs sont inférieures à 1, indiquant des distributions spatiales concentrées. La concentration est la plus forte pour l’année 2020 (R = 0,376).\nvaleur de p : toutes les valeurs sont égales à 0, signalant que les résultats sont significatifs.\n\n\n\n\n\nTableau 3.3: Résultats de la méthode du plus proche voisin pour les méfaits par année\n\n\n\n\n\n\n\n\n\n\n\nAnnée\npoints (n)\n\nR observé\n\nR attendu\nIndice plus proche voisin\nErreur standard\nZ\np\n\n\n\n2019\n251\n252\n604\n0,416\n19,931\n-17,687\n0\n\n\n2020\n383\n184\n489\n0,376\n13,062\n-23,353\n0\n\n\n2021\n344\n227\n516\n0,440\n14,542\n-19,860\n0\n\n\n2022\n220\n251\n645\n0,390\n22,739\n-17,322\n0\n\n\n\n\n\n\nNotez qu’il est possible aussi de construire un graphique pour le R observé avec plusieurs voisins, tel que réalisé avec le code ci-dessous avec k = 1 à 50 (figure 3.11).\n\n# k = 1 à 50\nRobs2019N1_50 &lt;- apply(nndist(st_coordinates(M2019), k=1:50), 2, FUN=mean)\nRobs2020N1_50 &lt;- apply(nndist(st_coordinates(M2020), k=1:50), 2, FUN=mean)\nRobs2021N1_50 &lt;- apply(nndist(st_coordinates(M2021), k=1:50), 2, FUN=mean)\nRobs2022N1_50 &lt;- apply(nndist(st_coordinates(M2022), k=1:50), 2, FUN=mean)\n# Enregistrement dans des dataFrames\nRobs2019N1_50 &lt;- data.frame(An=\"2019\", Voisins=1:length(Robs2019N1_50), Robs=Robs2019N1_50)\nRobs2020N1_50 &lt;- data.frame(An=\"2020\", Voisins=1:length(Robs2020N1_50), Robs=Robs2020N1_50)\nRobs2021N1_50 &lt;- data.frame(An=\"2021\", Voisins=1:length(Robs2021N1_50), Robs=Robs2021N1_50)\nRobs2022N1_50 &lt;- data.frame(An=\"2022\", Voisins=1:length(Robs2022N1_50), Robs=Robs2022N1_50)\n# Combinaison des dataFrames en un seul\nRobsN1_50 &lt;- rbind(Robs2019N1_50, Robs2020N1_50, Robs2021N1_50, Robs2022N1_50)\n# Création du graphique\nggplot(RobsN1_50)+\n geom_point(aes(x = Voisins, y = Robs, color = An))+\n geom_line(aes(x = Voisins, y = Robs, color = An))+\nlabs(x = \"Nombre de voisins\",\n     y = \"Robs - Distance en mètres\",\n     color = \"Année\")\n\n\n\nFigure 3.11: Distance au plus proche voisin de 1 à 50\n\n\n\n\n3.3.2 Méthode des quadrats\n\n3.3.2.1 Principe de base\nLe principe de base de la méthode des quadrats peut être décomposé en trois étapes :\n\nSuperposer à la région d’étude comprenant le semis de points un ensemble de quadrats (habituellement une grille régulière formée d’un ensemble de carrés).\nCompter le nombre de points compris dans chacun des quadrats. De la sorte, certains quadrats ne comprennent aucun point tandis que d’autres en contiennent un, deux, trois, etc. Nous obtenons ainsi un tableau des fréquences.\nRéaliser des tests statistiques à partir des fréquences observées et théoriques pour qualifier la distribution du semis de points (test de Kolmogorov-Smirnov, test du khi-deux ou méthode Monte-Carlo).\n\n3.3.2.2 Forme, distribution et taille des quadrats\nIl est possible de paramétrer les quadrats selon leur forme, leur distribution et leur taille. Habituellement, la forme retenue pour les quadrats est le carré, mais d’autres formes géométriques peuvent être utilisées comme l’hexagone et plus rarement, le cercle. La distribution des quadrats peut aussi être soit régulière, soit aléatoire (figure 3.12). Notez que dans le cas d’un cercle, le maillage ne peut être qu’irrégulier puisque certains points risqueraient de ne pas être contenus dans un cercle pour un maillage régulier.\n\n\nFigure 3.12: Formes et distributions de quadrats\n\nBien entendu, les résultats varient selon la taille des quadrats. Par exemple, dans le cas d’une distribution spatiale concentrée d’un semis de points, diminuer la taille des quadrats risque d’augmenter la perception de la dispersion. Certains auteurs proposent alors une formule pour déterminer la superficie optimale du quadrat (Wong et Lee 2005; Mitchel 2005) :\n\\[\nS_q = \\frac{2S}{n}\n\\tag{3.24}\\]\n\\[\nl_q = \\sqrt{S_q} \\text{ et } r_q = \\sqrt{\\frac{S_q}{\\pi}} \\text{ et } l_a = \\sqrt{ \\frac{2S_q}{3 \\sqrt{3}}} \\text{ avec :}\n\\tag{3.25}\\]\n\n\n\\(S_q\\), superficie du quadrat.\n\n\\(n\\), nombre de points dans le semis.\n\n\\(l_q\\), longueur du côté si la forme du quadrat est un carré.\n\n\\(r_q\\), longueur du rayon si la forme du quadrat est un cercle.\n\n\\(l_a\\), longueur du côté d’un hexagone régulier.\n\n3.3.2.3 Tests statistiques\nConstruction du tableau de fréquences observées et théoriques\nUne fois les quadrats créés, nous devons compter le nombre de points compris dans chacun d’eux. Une distribution spatiale concentrée à l’extrême se traduit par la localisation de tous les points du semis d’un seul quadrat, tandis que pour une distribution dispersée maximale se traduit par le fait que tous les quadrats contiennent le même nombre de points. Par la suite, nous construisons un tableau de fréquences.\nPrenons deux situations à la figure 3.13 :\n\n\nA. Une distribution dispersée, puisque les points sont présents dans la plupart des quadrats.\n\nB. Une distribution concentrée, puisque les points sont localisés dans quelques quadrats.\n\nNotez que pour les deux situations, nous avons 42 points (\\(n\\)) et 36 quadrats (\\(k\\)), soit une moyenne de 1,167 point par quadrat (\\(\\lambda = n / k = 42 / 36 = 1,167\\)). Détaillons les différentes colonnes du tableau de fréquences observées et théoriques :\n\nFréquences observées (\\(f_o\\)) : pour la situation A, nous avons 16 quadrats qui ne comprennent aucun point, 4 quadrats avec 1 point, 10 quadrats avec 2 points et finalement 6 quadrats avec 3 points. À l’inverse, pour la situation B, 27 quadrats sur les 36 ne comprennent aucun point, suggérant ainsi une concentration plus forte!\nProportions observées : simplement les fréquences observées divisées par le nombre total de quadrats (par exemple, \\(16 / 36 = 0,444\\) pour A).\nProportions théoriques : à partir de la loi de probabilité de Poisson (équation 3.26), il est possible de calculer les proportions théoriques que nous devrions avoir si les points étaient distribués aléatoirement. Pas de panique avec la lecture de la formule, nous verrons qu’il existe une fonction pour la calculer facilement dans R. Nous calculons aussi les proportions théoriques cumulées.\nFréquences théoriques : les fréquences théoriques sont simplement les proportions théoriques multipliées par le nombre de quadrats (par exemple, \\(\\text{0,311} \\times 36 = \\text{11,196}\\)).\n\n\\[\np(x = k )= \\frac{\\lambda^k e^{-\\lambda}}{x!}\\text{ avec :}\n\\tag{3.26}\\]\n\\(\\lambda \\text{ (lambda)} = n / k\\), soit le nombre moyen de points (\\(n\\)) par quadrat (\\(k\\)); \\(x\\), le nombre de points dans le quadrat (0, 1, 2, etc.); \\(!x\\), la factorielle d’un nombre (par exemple, \\(!3 = 1 \\times 2 \\times 3 = 6\\)); \\(e\\), la constante de l’Euler, soit \\(exp(1) = \\text{2,718282}\\).\nÀ partir de ce tableau des fréquences observées et théoriques, nous pouvons calculer les tests de Kolmogorov-Smirnov et du khi-deux.\n\n\nFigure 3.13: Illustrations des tests statistiques sur les quadrats\n\nTest statistique de Kolmogorov-Smirnov\nCe test se décompose en six étapes :\n\nFormuler l’hypothèse nulle stipulant que les fréquences observées et théoriques ne sont pas statistiquement différentes (\\(H_0\\)).\nChoisir un seuil de signification pour valider ou réfuter l’hypothèse nulle (par exemple, \\(\\alpha = \\text{0,05}\\)).\nCalculer la différence absolue entre les proportions cumulées observées et théoriques.\nCalculer la statistique \\(D\\), soit la plus forte valeur des différences absolues entre les fréquences cumulées observées et théoriques (équation 3.27).\nCalculer la valeur critique pour une distribution aléatoire avec un seuil de signification \\(\\alpha\\) (équation 3.28).\n\nComparer les valeurs de \\(D\\) et de \\(D_{\\alpha = 0.05}\\) :\n\nSi \\(D = D_{\\alpha \\text{ = 0,05}}\\), la distribution est aléatoire.\nSi \\(D &lt; D_{\\alpha \\text{ = 0,05}}\\), la distribution est dispersée.\nSi \\(D &gt; D_{\\alpha \\text{ = 0,05}}\\), la distribution est concentrée. Plus la valeur de \\(D\\) est élevée, plus la distribution spatiale du semis de points est concentrée.\n\n\n\n\\[\nD = \\text{max}\\lvert poi_{cumulé} - pti_{cumulé} \\rvert\n\\tag{3.27}\\]\n\\[\nD_{\\alpha = \\text{0,05}}= \\frac{\\text{1,36}}{\\sqrt{m}}\\text{ avec }\n\\tag{3.28}\\]\n\\(m\\) étant le nombre total de quadrats; \\(poi_{cumulé}\\) et \\(pti_{cumulé}\\), les proportions cumulées observées et théoriques.\nAppliquons cette démarche du test de Kolmogorov-Smirnov aux deux distributions de la figure 3.13 :\n\n\\(D_{\\alpha = \\text{0,05}}= \\frac{\\text{1,36}}{\\sqrt{36}}=\\text{0,210}\\)\npour la situation A, \\(D = \\text{0,133}\\), donc \\(D &lt; D_{\\alpha = \\text{0,05}}\\), alors la distribution est significativement dispersée.\npour la situation B, \\(D = \\text{0,439}\\), donc \\(D &gt; D_{\\alpha = \\text{0,05}}\\), alors la distribution est significativement concentrée.\n\nTest statistique du khi-deux\nCe test se décompose en quatre étapes :\n\nFormuler l’hypothèse nulle stipulant que la distribution des fréquences observées dans les quadrats suit une distribution de Poisson (\\(H_0\\)).\nCalculer le khi-deux (équation 3.29).\nComparer la valeur du khi-deux obtenue avec celle du khi-deux théorique (\\(\\chi^2_{\\alpha,dl}\\)) avec \\(k-1\\) degrés de liberté (\\(dl\\)) et un seuil de signification \\(\\alpha\\) (0,05 par exemple).\nSi \\(\\chi^2 &gt; \\chi^2_{\\alpha,dl}\\), alors l’hypothèse nulle est rejetée.\n\n\\[\n\\chi^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\\text{ avec }\n\\tag{3.29}\\]\n\\(O_i\\) et \\(E_i\\) étant respectivement les fréquences observée et attendue pour \\(i\\) (quadrat avec 0 point, 1, 2, etc.).\nPour les deux situations, le khi-deux calculé est supérieur au khi-deux théorique avec un seuil \\(\\alpha\\) de 0,001 et 35 degrés de liberté (paramètre df dans la fonction qchisq pour degrees of freedom). Par conséquent, les deux distributions ne sont pas aléatoires.\n\nround(qchisq(p=0.95,  df=35, lower.tail = TRUE),3)\n\n[1] 49.802\n\nround(qchisq(p=0.99,  df=35, lower.tail = TRUE),3)\n\n[1] 57.342\n\nround(qchisq(p=0.999,  df=35, lower.tail = TRUE),3)\n\n[1] 66.619\n\n\n\n\n\n\n\nLoi de Poisson : pas de panique!\n\n\nVous n’êtes pas familier avec la loi de probabilité de Poisson et le test du khi-deux, retenez simplement la démarche générale, nous utilisons des fonctions qui vont vous faciliter la vie dans R!\n\n\n\n3.3.2.4 Mise en œuvre dans R\nLe code suivant permet de déterminer la superficie optimale des quadrats en fonction du nombre de points (méfaits pour l’année 2020) et de la superficie de l’espace d’étude.\n\nlibrary(spatstat)\n## Taille des quadrats\n# Nombre de points\nnpoints &lt;- nrow(M2020)\n# Superficie de l'espace d'étude\nS &lt;- as.numeric(st_area(st_union(Arrondissements)))\n# Superficie du quadrat (équation 3.24)\nSq &lt;- (2*S) / npoints\n# Longueur du carré et du côté de l'hexagone régulier (équation 3.25)\nlq &lt;- sqrt(Sq)\nla &lt;- sqrt( (2*Sq) / (3*sqrt(3)))\n# Trouver la longueur du côté du carré dans lequel est compris l'hexagone \ncellsizeHex &lt;- 2 * sqrt(Sq/((3*sqrt(3)/2))) * sqrt(3)/2\ncat(\"Nombre de points =\", npoints,\n    \"\\nSuperficie (éq. 3.24) =\", Sq,\n    \"\\nLongueur du côté du carré (éq. 3.25) =\", lq, \n    \"\\nLongueur du côté du l'hexagone (éq. 3.25) =\", la,\n    \"\\nLongueur du côté du carré dans lequel est compris l'hexagone =\", cellsizeHex, \"\\n\")\n\nNombre de points = 383 \nSuperficie (éq. 3.24) = 1913098 \nLongueur du côté du carré (éq. 3.25) = 1383.148 \nLongueur du côté du l'hexagone (éq. 3.25) = 858.1093 \nLongueur du côté du carré dans lequel est compris l'hexagone = 1486.289 \n\n\nNous pouvons ensuite créer deux couches avec des quadrats carrés et hexagonaux avec la fonction st_make_grid du package sf. Repérez le paramètre square dans la fonction st_make_grid : écrivez square = TRUE pour obtenir des carrés et square = FALSE pour des hexagones réguliers.\n\n## Création des quadrats\n#Géométrie pour l'espace d'étude\nEspaceEtude &lt;- st_geometry(st_union(Arrondissements))\n# Création des carrés\nCarres.sf &lt;- st_make_grid(EspaceEtude, \n                     lq,  \n                     crs = st_crs(Arrondissements),\n                     what  = \"polygons\",\n                     square = TRUE)\n# Création des hexagones\nHexagones.sf &lt;- st_make_grid(EspaceEtude, \n                     cellsizeHex,  \n                     crs = st_crs(Arrondissements),\n                     what  = \"polygons\",\n                     square = FALSE)\nCarres.sf &lt;- st_sf(idCarre = 1:length(lengths(Carres.sf)), Carres.sf)\nHexagones.sf &lt;- st_sf(idHex = 1:length(lengths(Hexagones.sf)), Hexagones.sf)\ncat(\"Superficie (éq. 3.24) =\", Sq,\n    \"\\nVérifier la superficie des carrés et des hexagones\",\n    \"\\nSuperficie des carrés =\", as.numeric(st_area(Carres.sf[1,])),\n    \"\\nSuperficie des hexagones =\", as.numeric(st_area(Hexagones.sf[1,])), \n    \"Les superficies sont bien égales!\\n\")\n\nSuperficie (éq. 3.24) = 1913098 \nVérifier la superficie des carrés et des hexagones \nSuperficie des carrés = 1913098 \nSuperficie des hexagones = 1913098 Les superficies sont bien égales!\n\n## Visualisation\ntmap_mode(\"plot\")\n tm_shape(Hexagones.sf)+tm_borders(col=\"black\")+\n tm_shape(Carres.sf)+tm_borders(col=\"red\")+\n tm_shape(Arrondissements)+tm_borders(col=\"blue\", lwd=3)+\n   tm_shape(M2020)+tm_dots(col=\"green\", size=0.15)+\n   tm_layout(frame = FALSE)\n\n\n\n\nLa figure ci-dessus permet de constater que certains carrés et hexagones n’intersectent pas l’espace d’étude. Par conséquent, nous les supprimons puis calculons le nombre de points par carré et par hexagone.\n\n## Suppression des carrés qui n'intersectent pas les quatre arrondissements\nRequeteSpatiale &lt;- st_intersects(Carres.sf, \n                                 st_union(Arrondissements), sparse = FALSE)\nCarres.sf$Intersect &lt;- RequeteSpatiale[, 1]\nCarres.sf &lt;- Carres.sf[Carres.sf$Intersect== TRUE, ]\n## Suppression des hexagones qui n'intersectent pas les quatre arrondissements\nRequeteSpatiale &lt;- st_intersects(Hexagones.sf, \n                                 st_union(Arrondissements), sparse = FALSE)\nHexagones.sf$Intersect &lt;- RequeteSpatiale[, 1]\nHexagones.sf &lt;- Hexagones.sf[Hexagones.sf$Intersect== TRUE, ]\n## Jointure spatiale : compter le nombre de méfaits de 2020 dans les carrés et les hexagones\nCarres.sf$Mefaits2020    = lengths(st_intersects(Carres.sf, M2020))\nHexagones.sf$Mefaits2020 = lengths(st_intersects(Hexagones.sf, M2020))\n## Tableau de fréquences\ntable(Carres.sf$Mefaits2020)\n\n\n  0   1   2   3   4   5   6   7   8  10  11  13  21  38  41  64 \n172  25   6   3   8   1   3   4   3   2   3   1   1   1   1   1 \n\ntable(Hexagones.sf$Mefaits2020)\n\n\n  0   1   2   3   4   5   6   7   8   9  15  17  19  20  40  48 \n173  23   5   5   3   5   6   1   3   1   2   1   1   1   1   2 \n\n\nVisualisons les résultats à la figure 3.14. Il y a clairement une tendance à la concentration puisque de nombreux quadrats ne contiennent aucun point.\n\n## Visualisation\ntmap_mode(\"plot\")\nCarte1 =\n  tm_shape(subset(Carres.sf, Mefaits2020 == 0))+\n     tm_polygons(col=\"gray90\", border.col = \"white\", lwd = 1)+\n   tm_shape(subset(Carres.sf, Mefaits2020!= 0))+\n      tm_polygons(col=\"Mefaits2020\", style=\"cont\", title=\"Nombre\", \n                  border.col = \"white\", lwd = 1)+\n   tm_shape(Arrondissements)+tm_borders(col=\"black\", lwd=2)+\n   tm_layout(frame = FALSE)\nCarte2 =\n  tm_shape(subset(Hexagones.sf, Mefaits2020 == 0))+\n     tm_polygons(col=\"gray90\", border.col = \"white\", lwd = 1)+\n   tm_shape(subset(Hexagones.sf, Mefaits2020!= 0))+\n      tm_polygons(col=\"Mefaits2020\", style=\"cont\", title=\"Nombre\",\n                  border.col = \"white\", lwd = 1)+\n   tm_shape(Arrondissements)+tm_borders(col=\"black\", lwd=2)+\n   tm_layout(frame = FALSE) \ntmap_arrange(Carte1, Carte2)\n\n\n\nFigure 3.14: Nombre de méfaits dans les deux géométries de quadrats\n\n\n\nNous pouvons maintenant construire le tableau de fréquences et mettre en œuvre les différents tests statistiques. Le code ci-dessous génère le tableau de fréquences et applique le test de Kolmogorov-Smirnov.\n\n## Construction du tableau de fréquences\nTabFreq &lt;- as.data.frame(table(Carres.sf$Mefaits2020))\nnames(TabFreq) &lt;- c(\"Npoints\",\"Fo\")\nTabFreq$Npoints &lt;- as.numeric(as.character(TabFreq$Npoints))\n# Calcul pour les fréquences observées (fo)\nTabFreq$Fo.pro &lt;- TabFreq$Fo / sum(TabFreq$Fo)\nTabFreq$Fo.proCum &lt;- cumsum(TabFreq$Fo.pro)\n# Calcul pour les fréquences théoriques\nnpoints &lt;- sum(TabFreq$Npoints*TabFreq$Fo)\nnquadrats &lt;- sum(TabFreq$Fo)\nLambda &lt;- npoints / nquadrats\nTabFreq$Ft.pro &lt;- dpois(TabFreq$Npoints, lambda= Lambda)\nTabFreq$Ft.proCum &lt;- ppois(TabFreq$Npoints, lambda= Lambda, lower.tail = TRUE)\nTabFreq$Ft &lt;- TabFreq$Ft.pro * TabFreq$Npoints\n# Différences absolues entre les fréquences observées et théoriques cumulées\nTabFreq$Difffoft &lt;- abs(TabFreq$Fo.proCum - TabFreq$Ft.proCum)\n#calcul de D et Da\nD &lt;- max(TabFreq$Difffoft)\nDa &lt;- 1.36 / sqrt(nquadrats) # avec p à 0,05\n## Diagnostic\nif (D&gt;Da){\n  cat(\"D =\",round(D,3),\" et Da =\", round(Da,3),\n      \"\\nD &gt; Da avec p =0,05, alors la distribution tend vers la concentration.\")\n}else{\n  cat(\"D =\",round(D,3),\" et Da =\", round(Da,3),\n      \"\\nD &lt; Da avec p =0,05, alors la distribution tend vers la dispersion.\")\n}\n\nD = 0.536  et Da = 0.089 \nD &gt; Da avec p =0,05, alors la distribution tend vers la concentration.\n\n\nLe package spatstat permet de réaliser différents tests avec la fonction quadrat.test. Pour ce faire, il faut préalablement convertir les données dans les formats utilisés par ce package (ppp, owin et tess).\n\n## Conversion des points au format ppp\nM2020.ppp &lt;- ppp(x = st_coordinates(M2020)[,1],\n                 y = st_coordinates(M2020)[,2],\n                 window = as.owin(Arrondissements), \n                 check = T)\n## Conversion des quadrats carrés ou hexagonaux en objet owin\nquadrats &lt;- as(st_geometry(Carres.sf), \"Spatial\") \nFenetresC &lt;- lapply(quadrats@polygons,function(x){\n  coords &lt;- x@Polygons[[1]]@coords\n  coords&lt;-coords[nrow(coords):1,]\n  owin(poly = coords)})\n\nquadrats &lt;- as(st_geometry(Hexagones.sf), \"Spatial\") \nFenetresH &lt;- lapply(quadrats@polygons,function(x){\n  coords &lt;- x@Polygons[[1]]@coords\n  coords&lt;-coords[nrow(coords):1,]\n  owin(poly = coords)})\n## Ces fenêtres sont ensuite converties en un objet tess (tesselation)\nTessalationC &lt;- as.tess(FenetresC)\nTessalationH &lt;- as.tess(FenetresH)\n\nNous pouvons calculer différents tests avec la fonction quadrat.test. Dans un premier temps, nous vérifions si la distribution est dispersée avec la méthode Monte-Carlo et 999 permutations (alternative =\"regular\"). Que ce soit avec les quadrats carrés ou hexagonaux, la valeur de p est égale à 0. Cela signifie que nous sommes 100 % certains que la distribution n’est pas dispersée.\n\n## Réalisation des différents tests du khi-deux\n# test pour une distribution dispersée (Carrés)\ncat(\"Test pour une distribution dispersée avec les quadrats carrés\")\n\nTest pour une distribution dispersée avec les quadrats carrés\n\nquadrat.test(M2020.ppp, tess = TessalationC,\n             method = \"MonteCarlo\", nsim = 999,\n             alternative =\"regular\")  # dispersée\n\n\n    Conditional Monte Carlo test of CSR using quadrat counts\n    Test statistic: Pearson X2 statistic\n\ndata:  M2020.ppp\nX2 = 5212.2, p-value = 1\nalternative hypothesis: regular\n\nQuadrats: 235 tiles (irregular windows)\n\ncat(\"Test pour une distribution dispersée avec les quadrats hexagonaux\")\n\nTest pour une distribution dispersée avec les quadrats hexagonaux\n\nquadrat.test(M2020.ppp, tess = TessalationH,\n             method = \"MonteCarlo\", nsim = 999,\n             alternative =\"regular\")  # dispersée\n\n\n    Conditional Monte Carlo test of CSR using quadrat counts\n    Test statistic: Pearson X2 statistic\n\ndata:  M2020.ppp\nX2 = 4792.3, p-value = 1\nalternative hypothesis: regular\n\nQuadrats: 233 tiles (irregular windows)\n\n\nDans un second temps, nous vérifions si la distribution est concentrée, toujours avec la méthode Monte-Carlo et 999 permutations (alternative =\"clustered\"). Que ce soit avec les quadrats carrés ou hexagonaux, la valeur de p est égale à 0,001. Cela signifie qu’il y a moins de 1 % de chances que la distribution concentrée soit due au hasard.\n\n# test pour une distribution concentrée\ncat(\"Test pour une distribution concentrée avec les quadrats carrés\")\n\nTest pour une distribution concentrée avec les quadrats carrés\n\nquadrat.test(M2020.ppp, tess = TessalationC,\n             method = \"MonteCarlo\", nsim = 999,\n             alternative =\"clustered\") # concentrée\n\n\n    Conditional Monte Carlo test of CSR using quadrat counts\n    Test statistic: Pearson X2 statistic\n\ndata:  M2020.ppp\nX2 = 5212.2, p-value = 0.001\nalternative hypothesis: clustered\n\nQuadrats: 235 tiles (irregular windows)\n\ncat(\"Test pour une distribution concentrée avec les quadrats hexagonaux\")\n\nTest pour une distribution concentrée avec les quadrats hexagonaux\n\nquadrat.test(M2020.ppp, tess = TessalationH,\n             method = \"MonteCarlo\", nsim = 999,\n             alternative =\"clustered\") # concentrée\n\n\n    Conditional Monte Carlo test of CSR using quadrat counts\n    Test statistic: Pearson X2 statistic\n\ndata:  M2020.ppp\nX2 = 4792.3, p-value = 0.001\nalternative hypothesis: clustered\n\nQuadrats: 233 tiles (irregular windows)"
  },
  {
    "objectID": "03-MethodesRepartitionPonctuelle.html#sec-034",
    "href": "03-MethodesRepartitionPonctuelle.html#sec-034",
    "title": "3  Méthodes de répartition ponctuelle",
    "section": "\n3.4 Cartographie de la densité des points",
    "text": "3.4 Cartographie de la densité des points\nPour repérer les concentrations d’un semis de points dans une région, il faut cartographier la densité des points dans une maille soit irrégulière, soit régulière.\n\n3.4.1 Cartographie de la densité dans une maille irrégulière\nUne maille irrégulière est habituellement un découpage administratif comme les municipalités régionales de comté (MRC) du Québec ou les arrondissements d’une ville.\nÀ la figure 3.15, nous avons compté le nombre de méfaits par secteur de recensement pour l’année 2021, puis calculé la densité, soit le nombre de méfaits pour 1000 habitants. Les cercles proportionnels permettent ainsi de repérer les secteurs comprenant le plus de méfaits. Comme la population totale varie d’un SR à l’autre, il est préférable de cartographier la densité, soit le nombre de méfaits pour 1000 habitants. Notez que pour d’autres jeux de données, il serait plus judicieux d’utiliser la superficie comme dénominateur pour calculer la densité (par exemple, le nombre d’arbres au kilomètre carré ou à l’hectare).\nEn quelques lignes de code, il est très facile de calculer et de cartographier la densité dans un maillage irrégulier.\n\n## Secteurs de recensement (SR) de la ville de Sherbrooke en 2021\nSR &lt;- st_read(dsn = \"data/chap03/Recen2021Sherbrooke.gpkg\",\n              layer = \"DR_SherbSRDonnees2021\", quiet=TRUE)\n## Sélection des méfaits pour l'année 2021\nIncidents &lt;- st_read(dsn = \"data/chap03/IncidentsSecuritePublique.shp\", quiet=TRUE)\nM2021 &lt;- subset(Incidents, DESCRIPTIO == \"Méfait\" & ANNEE==2021)\n## Nous nous assurons que les deux couches ont la même projection cartographique\nSR &lt;- st_transform(SR, st_crs(M2021))\n## Calcul du nombre d'incidents par SR \nSR$Mefaits2021 &lt;- lengths(st_intersects(SR, M2021))\n## Calcul du nombre de méfaits pour 1000 habitants\nSR$DensiteM21Hab &lt;- SR$Mefaits2021 / (SR$SRpop_2021 / 1000)\n## Cartographie\ntm_shape(SR)+\n  tm_polygons(col=\"Mefaits2021\", style=\"pretty\", \n              title=\"Nombre pour 1000 habitants\",\n              legend.format = list(text.separator = \"à\"),\n              border.col = \"black\", lwd = 1)+\n  tm_bubbles(size = \"DensiteM21Hab\", border.col = \"black\", alpha = .5,\n                     col = \"aquamarine3\", title.size = \"Nombre\", scale = 1.5)+ \n  tm_layout(frame = FALSE)+tm_scale_bar(text.size = .5, c(0, 5, 10))\n\n\n\nFigure 3.15: Densité des méfaits par secteur de recensement, ville de Sherbrooke, 2021\n\n\n\n\n3.4.2 Cartographie de la densité dans une maille régulière\n\n3.4.2.1 Notion de processus spatial\nLorsque nous analysons des évènements ayant eu lieu dans un espace donné, nous pouvons considérer que ces évènements résultent d’un processus spatial que nous ne pouvons pas observer. Ce processus spatial est caractérisé par des points chauds (endroits où des évènements se produisent souvent) et des points froids (endroits où des évènements se produisent rarement). Si nous collectons suffisamment d’évènements (observations), alors leur densité devrait nous renseigner sur ce processus spatial. Puisqu’une image vaut mille mots, examinons un exemple d’un processus spatial à la figure 3.16.\n\n\n\n\nFigure 3.16: Exemple d’un processus spatial\n\n\n\nImaginons que ce processus spatial représente le niveau de risque de se faire attaquer par un écureuil dans un parc urbain délimité ici par les limites de la figure. Dans la réalité, nous ne pouvons pas mesurer directement ce phénomène, mais nous disposons d’un registre des attaques d’écureuils qui ont été enregistrées et spatialisées (figure 3.17). Bien entendu, ces données sont fictives! Dans un langage plus technique, il est dit que les évènements sont des réalisations du processus spatial.\n\npts &lt;- rpoint(1500, dens1)\nplot(pts, main = \"Attaques d'écureuils enregistrées\")\n\n\n\nFigure 3.17: Réalisation du processus spatial\n\n\n\nPuisque nous disposons d’un grand nombre d’observations, nous pouvons reconstruire le processus spatial qui produit les évènements en appliquant la méthode d’analyse de densité dans une maille régulière (figure 3.18).\n\n\n\n\nFigure 3.18: Reconstruction du processus spatial\n\n\n\nLa qualité d’estimation du processus initial dépend ainsi directement du nombre d’évènements dont nous disposons (figure 3.18).\n\n\n\n\nFigure 3.19: Reconstruction du processus spatial\n\n\n\nUne fois que nous sommes en mesure de recréer le processus spatial, nous pouvons en apprendre plus sur le phénomène et ses causes. Par exemple, est-ce que les secteurs du parc avec les plus grandes concentrations d’accidents sont aussi ceux dans lesquels sont localisés des jeux pour enfants?\n\n3.4.2.2 Description de la méthode KDE\nLe principe de base d’une cartographie de la densité dans une maille régulière – appelée aussi carte de chaleur ou estimation de la densité par noyau (kernel density estimation – KDE, en anglais) – est relativement simple et se décompose en trois étapes :\n\nJuxtaposer une grille de cellules sur l’espace d’étude, soit les pixels d’une image de n mètres de côté (par exemple, 50 mètres).\nPour chaque pixel, juxtaposer une zone de recherche de n mètres de rayon (1000 mètres par exemple) et calculer le nombre de points présents dans cette zone de recherche.\nCalculer la densité à partir d’une fonction simple (appelée habituellement fonction uniforme) ou d’une fonction de densité (kernel). Pour une fonction simple, nous divisons simplement le nombre de points présents dans la zone de recherche par la superficie de la zone, soit \\(πr^2\\); cette approche est toutefois peu recommandée puisqu’elle accorde la même pondération à chaque point situé de la zone d’influence! En utilisant une fonction de densité (kernel) (équation 3.30), nous accordons une pondération à chacun des points compris dans la zone de recherche : plus le point est proche du centre de la cellule, plus son poids est important dans l’estimation de la densité.\n\n\\[\n\\lambda(s) = \\sum_{i=1}^n \\frac{1}{\\pi r^2} k\\biggl(\\frac{d_{si}}{r}\\biggl)\\text{ avec }\n\\tag{3.30}\\]\n\n\n\\(\\lambda(s)\\), estimation de la densité à la localisation \\(s\\).\n\n\\(r\\), rayon de la zone de recherche.\n\n\\(d_{is}\\), distance entre la localisation \\(s\\) et le point \\(i\\).\n\n\\(k\\), fonction kernel définissant la pondération à accorder au ratio \\(\\frac{d_{si}}{r}\\) quand \\(0 &lt; d_{si} \\leq r\\). Si \\(d_{si} &gt; r\\), alors \\(k(d_{si}/r) = 0\\).\n\nIl existe différentes fonctions kernel pour obtenir les pondérations des points. Telle qu’illustrée à la figure 3.20, l’idée générale est que plus la distance entre la localisation et le point augmente (\\(d_{si}\\)), plus la pondération \\(k(d_{si}/r)\\) de l’équation équation 3.30 est faible.\n\n\nFigure 3.20: Principe de la fonction kernel pour définir les pondérations des points voisins dans la zone d’influence\n\n\n\n\n\n\nLes différentes fonctions kernel d’estimation de la densité\n\n\nPour une description des différentes fonctions kernel, consultez le lien suivant. Notez que l’outil Carte de chaleur (estimation par noyau) de QGIS inclut plusieurs fonctions kernel alors que l’outil Kernel Density d’ArcGIS Pro utilise uniquement la fonction quadratique. Finalement, la fonction density.ppp du package spatstat intègre quatre fonctions.\nInfluence de la forme du noyau (fonction kernel)\nExcepté la fonction uniforme, la plupart des auteurs s’entendent sur le fait que le choix de fonctions kernel a relativement peu d’influence sur le résultat final comparativement au choix du rayon d’influence.\n\n\n\n\nTableau 3.4: Fonctions kernel disponibles selon différents logiciels\n\nFonction\nQGis\nArcGIS Pro\nCrimeStat\nR (density.ppp)\n\n\n\nGaussienne\n\n\nX\nX\n\n\nTriangulaire\nX\n\nX\n\n\n\nQuadratique\nX\nX\nX\nX\n\n\nUniforme\nX\n\nX\nX\n\n\nCubique\nX\n\n\n\n\n\nEpanechnikov\nX\n\n\nX\n\n\nExponentielle\n\n\nX\n\n\n\n\n\n\n\n\n\n\n3.4.2.3 Mise en œuvre de la KDE dans R\nPour calculer la densité dans une maille régulière (KDE), vous pouvez utiliser deux packages :\n\n\nSpatialKDE, un package proposé récemment qui reprend le code de l’outil Carte de chaleur (estimation par noyau) de QGIS (Caha 2023). Vous obtiendrez donc les mêmes fonctionnalités et résultats qu’avec QGIS. Pour un exemple d’utilisation, consultez cette vignette.\n\nspatstat, soit le package le plus complet pour réaliser des analyses de répartition ponctuelle (Baddeley et Turner 2005; Baddeley, Rubak et Turner 2015). Par conséquent, nous privilégions son utilisation dans les exemples ci-dessous.\n\nAfin d’expliquer comment réaliser une KDE dans R, nous utilisons les accidents survenus dans l’arrondissement des Nations de la ville de Sherbrooke. Notez que nous retenons uniquement cet arrondissement et non l’ensemble de la ville afin d’accélérer les temps de calculs.\n\nlibrary(sf)\nlibrary(spatstat)\nlibrary(tmap)\nlibrary(terra)\n## Importation des données \nArrondissements &lt;-  st_read(dsn = \"data/chap03/Arrondissements.shp\", quiet=TRUE)\nIncidents &lt;- st_read(dsn = \"data/chap03/IncidentsSecuritePublique.shp\", quiet=TRUE)\nArrondissements &lt;- st_transform(Arrondissements, crs = 3798) \nIncidents &lt;- st_transform(Incidents, crs = 3798)\n## Couche pour les accidents\nAccidents &lt;- subset(Incidents, Incidents$DESCRIPTIO %in% \n                      c(\"Accident avec blessés\", \"Accident mortel\"))\n## Pour accélérer les calculs, nous retenons uniquement l'arrondissement des Nations\n# Couche pour l'arrondissement des Nations\nArrDesNations &lt;- subset(Arrondissements, NOM == \"Arrondissement des Nations\")\n# Sélection des accidents localisés dans l'arrondissement des Nations\nRequeteSpatiale &lt;- st_intersects(Accidents, ArrDesNations, sparse = FALSE)\nAccidents$Nations &lt;- RequeteSpatiale[, 1]\nAccNations &lt;- subset(Accidents, Accidents$Nations == TRUE)\n# Une couche par année\nAccNations2019 &lt;- subset(AccNations, AccNations$AN == 2019)\nAccNations2020 &lt;- subset(AccNations, AccNations$AN == 2020)\nAccNations2021 &lt;- subset(AccNations, AccNations$AN == 2021)\nAccNations2022 &lt;- subset(AccNations, AccNations$AN == 2022)\n\nNous convertissons les points dans le format ppp utilisé par le package spatstat.\n\n## Conversion des données sf dans le format de spatstat\n# la fonction as.owin est utilisée pour définir la fenêtre de travail\nfenetre &lt;- as.owin(ArrDesNations)\n## Conversion des points au format ppp pour les différentes années\n# 2019 \nAccN2019.ppp &lt;- ppp(x = st_coordinates(AccNations2019)[,1],\n                    y = st_coordinates(AccNations2019)[,2],\n                    window = fenetre, check = T)\n# 2020 \nAccN2020.ppp &lt;- ppp(x = st_coordinates(AccNations2020)[,1],\n                    y = st_coordinates(AccNations2020)[,2],\n                    window = fenetre, check = T) \n# 2021 \nAccN2021.ppp &lt;- ppp(x = st_coordinates(AccNations2021)[,1],\n                    y = st_coordinates(AccNations2021)[,2],\n                    window = fenetre, check = T)\n# 2022\nAccN2022.ppp &lt;- ppp(x = st_coordinates(AccNations2022)[,1],\n                    y = st_coordinates(AccNations2022)[,2],\n                    window = fenetre, check = T)\n\nPuis, il faut définir la taille des pixels et le rayon d’influence (bandwidth en anglais) :\n\nPlus la taille des pixels est réduite, plus la taille de l’image résultante est importante et les calculs longs à réaliser.\nLe choix du rayon est aussi délicat. Avec un rayon trop petit, les résultats sont trop détaillés avec des valeurs très faibles. À l’inverse, un rayon trop grand a comme effet de lisser les résultats et de masquer des disparités locales.\n\n\n\n\n\n\nRatio entre la taille du pixel et la taille du rayon d’influence\n\n\nAssurez-vous que la taille du pixel soit bien inférieure (au moins dix fois plus petite) à celle du rayon d’influence afin d’obtenir des résultats précis. À notre connaissance, il n’existe pas de règle pour optimiser la valeur ratio entre la taille du pixel et la taille du rayon d’influence.\n\n\nPlusieurs algorithmes permettant de sélectionner la valeur optimale du rayon d’influence sont implémentés dans le package spatstat avec les fonctions bw.diggle, bw.ppl, bw.scott et bw.CvL. À la lecture des résultats ci-dessous, la valeur du rayon proposée par l’algorithme de Diggle semble bien trop petite et celle du critère de Cronie et van Lieshout bien trop grande. Le rayon optimal est certainement compris entre 700 et 750 mètres. Par conséquent, nous retenons une taille de pixel de 50 mètres et la valeur du rayon optimisée par la fonction bw.ppl.\n\ncat(\"\\nDiggle :\", bw.diggle(AccN2020.ppp),\n    \"\\nMaximum de vraissemblance :\", bw.ppl(AccN2020.ppp),\n    \"\\nCritère de Scott 1 :\", bw.scott(AccN2020.ppp)[1],\n    \"\\nCritère  de Scott 2 :\", bw.scott(AccN2020.ppp)[2],\n    \"\\nCritère de Cronie et van Lieshout :\", bw.CvL(AccN2020.ppp))\n\n\nDiggle : 124.196 \nMaximum de vraissemblance : 494.5408 \nCritère de Scott 1 : 971.3489 \nCritère  de Scott 2 : 514.2688 \nCritère de Cronie et van Lieshout : 1644.489\n\n## Taille des pixels et du rayon en mètres\npixel_m &lt;- 50\nRayonOpt &lt;- bw.ppl(AccN2020.ppp)\n\n\n3.4.2.3.1 Comparaison des fonctions kernel\n\nLe paramètre kernel de la fonction density.ppp permet de sélectionner l’une des quatre fonctions kernel (gaussian, quartic, epanechnikov et disc). Pour la dernière, le même poids est attribué aux points compris dans le rayon d’influence; elle correspond ainsi à un kernel uniforme ou simple. Excepté ce dernier kernel, la figure 3.21 démontre que les résultats sont très semblables d’un kernel à l’autre.\n\n## Calcul de la KDE avec différentes fonctions kernel\n# kernel gaussien\nkdeG &lt;- density.ppp(AccN2020.ppp, sigma=RayonOpt, eps=pixel_m, kernel=\"gaussian\")\n# kernel quadratique\nkdeQ &lt;- density.ppp(AccN2020.ppp, sigma=RayonOpt, eps=pixel_m, kernel=\"quartic\")\n# kernel Epanechnikov\nkdeE &lt;- density.ppp(AccN2020.ppp, sigma=RayonOpt, eps=pixel_m, kernel=\"epanechnikov\")\n# kernel disc (uniforme ou simple)\n# le même poids est accordé à chaque point dans le rayon\nkdeD &lt;- density.ppp(AccN2020.ppp, sigma=RayonOpt, eps=pixel_m, kernel=\"disc\")\n## Conversion en raster \n# étant donné que les valeurs sont exprimées en nombre de points par m2,\n# nous les multiplions par 1 000 000 pour obtenir une densité au km2.\nRkdeG &lt;- terra::rast(kdeG)*1000000\nRkdeQ &lt;- terra::rast(kdeQ)*1000000\nRkdeE &lt;- terra::rast(kdeE)*1000000\nRkdeD &lt;- terra::rast(kdeD)*1000000\n## Projection cartographique\ncrs(RkdeG) &lt;- \"epsg:3798\"\ncrs(RkdeQ) &lt;- \"epsg:3798\" \ncrs(RkdeE) &lt;- \"epsg:3798\" \ncrs(RkdeD) &lt;- \"epsg:3798\"\n## Visualisation des résultats\ntmap_mode(\"plot\")\nCarte1 &lt;-\n  tm_shape(RkdeG) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"Gaussien\")+\n  tm_shape(AccNations2020) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_layout(frame = FALSE)\nCarte2 &lt;-\n  tm_shape(RkdeQ) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"Quadratique\")+\n  tm_shape(AccNations2020) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_layout(frame = FALSE)\nCarte3 &lt;-\n  tm_shape(RkdeE) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"Epanechnikov\")+\n  tm_shape(AccNations2020) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_layout(frame = FALSE)\nCarte4 &lt;-\n  tm_shape(RkdeD) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"Uniforme\")+\n  tm_shape(AccNations2020) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_scale_bar(breaks  = c(0, 1, 2))+tm_layout(frame = FALSE)\ntmap_arrange(Carte1, Carte2, Carte3, Carte4,\n             ncol = 2, nrow = 2)\n\n\n\nFigure 3.21: Comparaison des résultats de la KDE pour différents kernels\n\n\n\n\n3.4.2.3.2 Évaluation de l’impact du rayon d’influence\nLe code R permet de réaliser des KDE avec des rayons de 250, 500, 750 et 1000 mètres. La figure 3.21 démontre bien que plus le rayon est grand, plus la carte est lissée.\n\nkdeQ250  &lt;- density.ppp(AccN2020.ppp, sigma=250,  eps=50, kernel=\"quartic\")\nkdeQ500  &lt;- density.ppp(AccN2020.ppp, sigma=500,  eps=50, kernel=\"quartic\")\nkdeQ750  &lt;- density.ppp(AccN2020.ppp, sigma=750,  eps=50, kernel=\"quartic\")\nkdeQ1000 &lt;- density.ppp(AccN2020.ppp, sigma=1000, eps=50, kernel=\"quartic\")\nRkdeQ250  &lt;- terra::rast(kdeQ250)*1000000\nRkdeQ500  &lt;- terra::rast(kdeQ500)*1000000\nRkdeQ750  &lt;- terra::rast(kdeQ750)*1000000\nRkdeQ1000 &lt;- terra::rast(kdeQ1000)*1000000\ncrs(RkdeQ250) &lt;- \"epsg:3798\"\ncrs(RkdeQ500) &lt;- \"epsg:3798\" \ncrs(RkdeQ750) &lt;- \"epsg:3798\" \ncrs(RkdeQ1000) &lt;- \"epsg:3798\"\n\n## Visualisation des résultats\ntmap_mode(\"plot\")\nCarte1 &lt;-\n  tm_shape(RkdeQ250) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"r = 250 m\")+\n  tm_shape(AccNations2020) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_layout(frame = FALSE)\nCarte2 &lt;-\n  tm_shape(RkdeQ500) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"r = 500 m\")+\n  tm_shape(AccNations2020) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_layout(frame = FALSE)\nCarte3 &lt;-\n  tm_shape(RkdeQ750) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"r = 750 m\")+\n  tm_shape(AccNations2020) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 3)+\n  tm_layout(frame = FALSE)\nCarte4 &lt;-\n  tm_shape(RkdeQ1000) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"r = 1000 m\")+\n  tm_shape(AccNations2020) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_scale_bar(breaks  = c(0, 1, 2))+\n  tm_layout(frame = FALSE)\ntmap_arrange(Carte1, Carte2, Carte3, Carte4,\n             ncol = 2, nrow = 2)\n\n\n\nFigure 3.22: Comparaison des résultats de la KDE selon le rayon d’influence\n\n\n\n\n3.4.2.3.3 Comparaison entre différentes années\nBien entendu, pour un même jeu de données, il est possible de représenter la densité pour différentes années (figure 3.23).\n\nkde2019 &lt;- density.ppp(AccN2019.ppp, sigma=500, eps=50, kernel=\"quartic\")\nkde2020 &lt;- density.ppp(AccN2020.ppp, sigma=500, eps=50, kernel=\"quartic\")\nkde2021 &lt;- density.ppp(AccN2021.ppp, sigma=500, eps=50, kernel=\"quartic\")\nkde2022 &lt;- density.ppp(AccN2022.ppp, sigma=500, eps=50, kernel=\"quartic\")\nRkde2019 &lt;- terra::rast(kde2019)*1000000\nRkde2020 &lt;- terra::rast(kde2020)*1000000\nRkde2021 &lt;- terra::rast(kde2020)*1000000\nRkde2022 &lt;- terra::rast(kde2021)*1000000\ncrs(Rkde2019) &lt;- \"epsg:3798\"\ncrs(Rkde2020) &lt;- \"epsg:3798\" \ncrs(Rkde2021) &lt;- \"epsg:3798\" \ncrs(Rkde2022) &lt;- \"epsg:3798\"\n\nCarte1 &lt;-\n  tm_shape(Rkde2019) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"2019\")+\n  tm_shape(AccNations2019) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_layout(frame = FALSE)\nCarte2 &lt;-\n  tm_shape(Rkde2020) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"2021\")+\n  tm_shape(AccNations2020) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_layout(frame = FALSE)\nCarte3 &lt;-\n  tm_shape(Rkde2021) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"2022\")+\n  tm_shape(AccNations2021) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_layout(frame = FALSE)\nCarte4 &lt;-\n  tm_shape(Rkde2022) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"2023\")+\n  tm_shape(AccNations2022) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 2)+\n  tm_scale_bar(breaks  = c(0, 1, 2))+tm_layout(frame = FALSE)\ntmap_arrange(Carte1, Carte2, Carte3, Carte4,\n             ncol = 2, nrow = 2)\n\n\n\nFigure 3.23: Comparaison des résultats de la KDE pour différentes années\n\n\n\n\n3.4.3 Densité spatio-temporelle dans une maille régulière\n\n3.4.3.1 Description de la méthode STKDE\nDans les sections précédentes, nous avons vu que l’analyse de densité dans une maille régulière avec la méthode KDE consiste à répartir la masse des évènements étudiés dans un certain rayon d’influence (bandwidth) autour de chaque évènement selon une fonction décroissante (kernel).\nCette méthode peut être étendue assez facilement au contexte spatio-temporel. En effet, lorsque les évènements étudiés sont caractérisés par une date d’occurrence, il est possible de répartir leur masse à la fois dans l’espace et dans le temps. Plus exactement, il s’agit d’un kernel bivarié comprenant une bandwidth spatiale et une bandwidth temporelle. La densité locale est alors estimée avec le produit de la densité spatiale et de la densité temporelle de chaque évènement (équation 3.31).\n\\[\n\\lambda(s) = \\sum_{i=1}^n \\frac{1}{\\pi bw_s^2 \\times bw_t} k\\biggl(\\frac{d_{si}}{bw_s}\\biggl) k\\biggl(\\frac{t_{si}}{bw_t}\\biggl)\\text{ avec }\n\\tag{3.31}\\]\n\n\n\\(bw_t\\) et \\(bw_s\\) les rayons d’influence (bandwidth) temporel et spatial.\n\n\\(t_{si}\\) et \\(d_{si}\\) les distances temporelles et spatiales entre l’évènement i et un point dans l’espace et le temps s.\n\n\\(k\\), fonction kernel.\n\nLa figure 3.24 illustre le principe de la STKDE (Space-Time Kernel Density Estimatation). Un évènement a lieu à un moment spécifique et en un lieu spécifique. Plus nous sommes proches de l’évènement dans le temps et dans l’espace, plus la densité est élevée. Hors du bandwidth dans le temps ou dans l’espace, la densité est nulle.\n\n\nFigure 3.24: Visualisation de la STKDE\n\nUne représentation particulièrement efficace de la STKDE est une carte animée permettant de visualiser l’évolution spatiale de la densité dans le temps. À la figure 3.25, nous visualisons la densité spatio-temporelle produite par deux évènements ayant eu lieu à des moments et des localisations différentes.\n\n\nFigure 3.25: Visualisation animée de la STKDE\n\nEn résumé, la seule différence avec la KDE et la SKDE est que cette dernière nécessite de déterminer une bandwidth temporelle (en plus de la bandwidth spatiale).\n\n3.4.3.2 Mise en œuvre de la STKDE dans R\nPour mettre en œuvre la STKDE sur les données de collisions de la ville de Sherbrooke, nous utilisons le package sparr (Davies, Marshall et Hazelton 2018). À la figure 3.26, nous visualisons aisément les périodes durant lesquelles les accidents ont été les plus nombreux.\n\nlibrary(spatstat)\nlibrary(terra)\nlibrary(gifski)\nlibrary(sf)\nlibrary(tmap)\nlibrary(ggplot2)\nlibrary(sparr)\nlibrary(lubridate)\nlibrary(classInt)\nlibrary(viridis)\n\nroutes &lt;- st_read('data/chap01/shp/Segments_de_rue.shp', quiet = TRUE)\ncollisions &lt;- st_read('data/chap04/DataAccidentsSherb.shp', quiet = TRUE)\n## Reprojection dans le même système\nroutes &lt;- st_transform(routes, 2949)\ncollisions &lt;- st_transform(collisions, 2949)\n## Visualisation de la densité temporelle\ncollisions$dt &lt;- as_date(collisions$DATEINCIDE)\ncollisions$dt_num &lt;- as.numeric(collisions$dt - min(collisions$dt))\nggplot(collisions, aes(x=dt)) + \n  geom_density(bw = 'sj', color = \"blue\", lwd = 1) + \n  labs(y= \"Densité\", x = \"Date\")+\n  scale_x_date(date_breaks = \"6 months\", date_labels =  \"%b %Y\")+\n  theme_bw()\n\n\n\nFigure 3.26: Distribution temporelle de la densité des accidents\n\n\n\nPour calculer la STKDE, nous devons déterminer les valeurs des deux bandwidths, réalisé ici avec une approche dite de validation croisée par maximum de vraisemblance.\n\n## Préparation des données de collisions\nfenetre &lt;- as.owin(st_buffer(collisions,500))\nXY &lt;- st_coordinates(collisions)\ncollisions.ppp &lt;- ppp(x = XY[,1],\n                    y = XY[,2],\n                    window = fenetre, check = T)\n## Estimation des deux meilleures bandwidths\nscores_bw &lt;- LIK.spattemp(collisions.ppp, \n                          tt = collisions$dt_num,\n                          tlim = c(0,1100),\n                          parallelise = NA,verbose = FALSE)\nprint(scores_bw)\n\n       h   lambda \n736.8287 147.4328 \n\n\nLes valeurs optimales obtenues avec l’approche dite de validation croisée par maximum de vraisemblance sont de 736 mètres pour la bandwidth spatiale et 147 jours pour la bandwidth temporelle. Cette dernière est vraisemblablement trop grande. Par conséquent, nous utilisons des bandwidths de 750 mètres et 14 jours. Notez que les valeurs de densité sont multipliées par 10 000, car leur étalement dans l’espace et le temps conduit à des valeurs trop petites pour le package tmap. Les résultats sont présentés sous la forme d’une animation à la figure 3.27.\n\n# Calcul des valeurs de densités\ndens_vals &lt;- spattemp.density(collisions.ppp,\n                              h = 750,\n                              lambda = 14,\n                              tt = collisions$dt_num,\n                              tlim = c(0,1100),\n                              sres = 500,\n                              tres = 150, verbose = FALSE)\n\n## Extraction des rasters à chaque période\nall_rasts &lt;- lapply(dens_vals$z, function(x){\n  my_rast &lt;- terra::rast(x) * 10000 # petit ajustement pour la carto\n  vals &lt;- terra::values(my_rast)\n  vals &lt;- ifelse(is.na(vals), 0, vals)\n  terra::values(my_rast) &lt;- vals\n  terra::crs(my_rast) &lt;- 'epsg:32188'\n  return(my_rast)\n})\n## Extraction des valeurs pour créer une échelle commune de couleur\nall_densities &lt;- do.call(c, lapply(all_rasts, function(x){\n  sample(terra::values(x),size = 100,replace = FALSE)\n}))\ncolor_breaks &lt;- classIntervals(all_densities, n = 10, style = \"kmeans\")\n## Préparation des dates\ntimestamps &lt;- round(as.numeric(names(dens_vals$z)))\ntime_frames &lt;- min(collisions$dt) + days(timestamps)\n## Compilation des cartes\nall_maps &lt;- lapply(1:length(time_frames), function(i){\n  tm_shape(all_rasts[[i]]) +\n    tm_raster(breaks = color_breaks$brks, palette = viridis(10)) +\n    tm_layout(legend.show = FALSE, frame = FALSE, title = time_frames[[i]])\n})\n\n# Création d'une animation pour produire la carte animée\ntmap_animation(all_maps, filename = \"images/Chap03/animated_STKDE_sherbrooke.gif\", \n               width = 500, height = 500, dpi = 150, delay = 25)\n\n\n\nFigure 3.27: Distribution spatio-temporelle de la densité des accidents"
  },
  {
    "objectID": "03-MethodesRepartitionPonctuelle.html#sec-035",
    "href": "03-MethodesRepartitionPonctuelle.html#sec-035",
    "title": "3  Méthodes de répartition ponctuelle",
    "section": "\n3.5 Quiz de révision du chapitre",
    "text": "3.5 Quiz de révision du chapitre\n\n\n\n\n\nQuelle est la différence entre le centre point et le point central?\n\n\nRelisez au besoin la section 3.2.1.2.\n\n\n\n\n\n\nLe centre moyen peut être pondéré contrairement au point central.\n\n\n\n\n\n\n\nLe point central est un point qui fait partie du semis de points initial.\n\n\n\n\n\n\n\n\n\n\nAssocier la distance standard à chacun des trois semis de points suivants :\n\n\nRelisez au besoin la section 3.2.2.\n\n\n\n\n\n\nA = 219, B = 347, C = 685\n\n\n\n\n\n\n\nA = 685, B = 219, C = 347\n\n\n\n\n\n\n\nA = 685, B = 347, C = 219\n\n\n\n\n\n\n\n\n\n\nQuelles sont les quatre principales manières de représenter graphiquement la dispersion d’un semis de points?\n\n\nRelisez au besoin la section 3.2.3.1.\n\n\n\n\n\n\nEnveloppe convexe\n\n\n\n\n\n\n\nCentre moyen et point central\n\n\n\n\n\n\n\nRectangle construit avec les déviations standards des X et des Y\n\n\n\n\n\n\n\nDistance standard\n\n\n\n\n\n\n\nEllipse de déviation de distance standard\n\n\n\n\n\n\n\n\n\n\nQuelles sont les trois principales distributions d’un semis de points?\n\n\nRelisez au besoin la section 3.3.\n\n\n\n\n\n\nDistribution dispersée\n\n\n\n\n\n\n\nDistribution aléatoire\n\n\n\n\n\n\n\nDistribution concentrée\n\n\n\n\n\n\n\nDistribution normale\n\n\n\n\n\n\n\n\n\n\nÀ la lecture des résultats de la méthode du plus proche voisin, quelle année a la distribution spatiale la plus concentrée?\n\n\nRelisez au besoin la section 3.3.1.\n\n\n\n\n\n\n2019\n\n\n\n\n\n\n\n2020\n\n\n\n\n\n\n\n2021\n\n\n\n\n\n\n\n2022\n\n\n\n\n\n\n\n\n\n\nEn analyse des quadrats, quelles sont les trois principales formes utilisées?\n\n\nRelisez au besoin la section 3.3.2.1.\n\n\n\n\n\n\nTriangle\n\n\n\n\n\n\n\nCarré\n\n\n\n\n\n\n\nHexagone\n\n\n\n\n\n\n\nCercle\n\n\n\n\n\n\n\n\n\n\nQuel est le paramètre influençant le plus les résultats de la méthode KDE?\n\n\nRelisez au besoin la section 3.4.2.1.\n\n\n\n\n\n\nLa fonction kernel utilisée\n\n\n\n\n\n\n\nLa taille du rayon d’influence\n\n\n\n\n\n\n\n\n\n\nQuelles sont les principales fonctions kernel?\n\n\nRelisez au besoin la section 3.4.2.1.\n\n\n\n\n\n\nGaussienne\n\n\n\n\n\n\n\nQuadratique\n\n\n\n\n\n\n\nUniforme\n\n\n\n\n\n\n\nManhattan\n\n\n\n\n\n\n\nEpanechnikov\n\n\n\n\n\n\n\n\n\nVérifier votre résultat"
  },
  {
    "objectID": "03-MethodesRepartitionPonctuelle.html#sec-036",
    "href": "03-MethodesRepartitionPonctuelle.html#sec-036",
    "title": "3  Méthodes de répartition ponctuelle",
    "section": "\n3.6 Exercices de révision",
    "text": "3.6 Exercices de révision\n\n\n\n\n\nExercice 1. Calcul du centre moyen et de la distance standard pour les accidents\n\n\nComplétez le code ci-dessous.\n\nlibrary(sf)\nlibrary(tmap)\n## Importation des données \nArrondissements &lt;-  st_read(dsn = \"data/chap03/Arrondissements.shp\", quiet=TRUE)\nIncidents &lt;- st_read(dsn = \"data/chap03/IncidentsSecuritePublique.shp\", quiet=TRUE)\n## Changement de projection\nArrondissements &lt;- st_transform(Arrondissements, crs = 3798) \nIncidents &lt;- st_transform(Incidents, crs = 3798)\n## Couche pour les accidents\nAccidents &lt;- subset(Incidents, Incidents$DESCRIPTIO %in% \n                      c(\"Accident avec blessés\", \"Accident mortel\"))\n## Coordonnées et projection cartographique\nxy &lt;- À compléter\nProjCarto &lt;- À compléter\n## Centre moyen\nCentreMoyen &lt;- data.frame(À compléter)\nCentreMoyen &lt;- st_as_sf(CentreMoyen, coords = c(\"X\", \"Y\"), crs = À compléter)\n# Distance standard combinée\nCentreMoyen$DS &lt;- À compléter\nCercleDS &lt;- À compléter\nhead(CercleDS)\n\nCorrection à la section 9.3.1.\n\n\n\n\n\n\n\nExercice 2. Calcul et cartographie de la densité des accidents dans un maillage irrégulier\n\n\nPour l’année 2021, complétez le code ci-dessous.\n\nlibrary(sf)\nlibrary(tmap)\n## Importation des données \nSR &lt;- st_read(dsn = \"data/chap03/Recen2021Sherbrooke.gpkg\",\n              layer = \"DR_SherbSRDonnees2021\", quiet=TRUE)\n## Couche pour les accidents pour l'année 2021\nAcc2021 &lt;- subset(Incidents, Incidents$DESCRIPTIO %in% \n                    c(\"Accident avec blessés\", \"Accident mortel\")\n                  & ANNEE==2021)\n## Nous nous assurons que les deux couches ont la même projection cartographique\nSR &lt;- st_transform(SR, st_crs(Acc2021))\n## Calcul du nombre d'incidents par SR \nSR$Acc2021 &lt;- À compléter\n## Calcul du nombre de méfaits pour 1000 habitants\nSR$DensiteMAcc2021Hab &lt;- À compléter\n## Cartographie\ntm_shape(SR)+\n  tm_polygons(col= À compléter, style=\"pretty\", \n              title=\"Nombre pour 1000 habitants\",\n              border.col = \"black\", lwd = 1)+\n  tm_bubbles(size = À compléter, border.col = \"black\", alpha = .5,\n             col = \"aquamarine3\", title.size = \"Nombre\", scale = 1.5)+ \n  tm_layout(frame = FALSE)+tm_scale_bar(text.size = .5, c(0, 5, 10))\n\nCorrection à la section 9.3.2.\n\n\n\n\n\n\n\nExercice 3. Calcul et cartographie de la densité des accidents dans un maillage régulier\n\n\nPour l’année 2021, complétez le code ci-dessous.\n\nlibrary(sf)\nlibrary(spatstat)\nlibrary(tmap)\nlibrary(terra)\n\n## Importation des données \nArrondissements &lt;-  st_read(dsn = \"data/chap03/Arrondissements.shp\", quiet=TRUE)\nIncidents &lt;- st_read(dsn = \"data/chap03/IncidentsSecuritePublique.shp\", quiet=TRUE)\n## Changement de projection\nArrondissements &lt;- st_transform(Arrondissements, crs = 3798) \nIncidents &lt;- st_transform(Incidents, crs = 3798)\n## Couche pour les méfaits pour l'année 2021\nM2021 &lt;- subset(Incidents, DESCRIPTIO == \"Méfait\" & ANNEE==2021)\n## Pour accélérer les calculs, nous retenons uniquement l'arrondissement des Nations\n# Couche pour l'arrondissement des Nations\nArrDesNations &lt;- subset(Arrondissements, NOM == \"Arrondissement des Nations\")\n# Sélection des accidents localisés dans l'arrondissement Des Nations\nRequeteSpatiale &lt;- st_intersects(M2021, ArrDesNations, sparse = FALSE)\nM2021$Nations &lt;- RequeteSpatiale[, 1]\nM2021Nations &lt;- subset(M2021, M2021$Nations == TRUE)\n\n## Conversion des données sf dans le format de spatstat\n# la fonction as.owin est utilisée pour définir la fenêtre de travail\nfenetre &lt;- à complétér\n## Conversion des points au format ppp pour les différentes années\nM2021.ppp &lt;- à complétér\n\n## Kernel quadratique avec un rayon de 500 mètres et une taille de pixel de 50 mètres\nkdeQ &lt;- density.ppp(M2021.ppp, sigma=500, eps=50, kernel=\"quartic\")\n## Conversion en raster\nRkdeQ &lt;- terra::rast(kdeQ)*1000000\n## Projection cartographique\ncrs(RkdeQ) &lt;- \"epsg:3857\"\n## Visualisation des résultats\ntmap_mode(\"plot\")\n  À compléter\n\nCorrection à la section 9.3.3.\n\n\n\n\n\n\nApparicio, Philippe et Jérémy Gelb. 2022. Méthodes quantitatives en sciences sociales : un grand bol d’R. FabriqueREL, Licence CC BY-SA. https://laeq.github.io/LivreMethoQuantBolR/.\n\n\nBaddeley, Adrian, Ege Rubak et Rolf Turner. 2015. Spatial point patterns: methodology and applications with R. CRC press.\n\n\nBaddeley, Adrian et Rolf Turner. 2005. « spatstat: An R Package for Analyzing Spatial Point Patterns. » Journal of Statistical Software 12 (6): 1‑42. https://doi.org/10.18637/jss.v012.i06.\n\n\nBarbonne, Rémy, Paul Villeneuve et Marius Thériault. 2007. « La dynamique spatiale des marchés locaux de l’emploi au sein du champ métropolitain de Québec, 1981–2001. » The Canadian Geographer/Le Géographe canadien 51 (3). Wiley Online Library: 303‑322. https://doi.org/10.1111/j.1541-0064.2007.00180.x.\n\n\nCaha, Jan. 2023. SpatialKDE: Kernel Density Estimation for Spatial Data. s.n. https://CRAN.R-project.org/package=SpatialKDE.\n\n\nDavies, T. M., J. C. Marshall et M. L. Hazelton. 2018. « Tutorial on kernel estimation of continuous spatial and spatiotemporal relative risk. » Statistics in Medicine 37 (7): 1191‑1221.\n\n\nLevine, Ned. 2006. « Crime mapping and the Crimestat program. » Geographical analysis 38 (1): 41‑56. https://doi.org/10.1111/j.0016-7363.2005.00673.x.\n\n\n———. 2021. « CrimeStat IV. » In The Encyclopedia of Research Methods in Criminology and Criminal Justice, sous la dir. de JC Barnes et David R Forde, 1:28‑32. John Wiley & Sons. https://doi.org/10.1002/9781119111931.ch6.\n\n\nLópez Castro, Marco Antonio, Marius Thériault et Marie-Hélène Vandersmissen. 2015. « Évolution de la mobilité des membres de familles monoparentales dans la région métropolitaine de Québec de 1996 à 2006: comparaison entre les ménages matricentriques et patricentriques. » Cahiers de géographie du Québec 59 (167): 209‑250. https://doi.org/10.7202/1036355ar.\n\n\nMitchel, Andy. 2005. The ESRI Guide to GIS analysis, Volume 2: Spartial measurements and statistics. ESRI press.\n\n\nTveite, Håvard. 2020. « The QGIS Standard Deviational Ellipse Plugin. » http://plugins.qgis.org/plugins/SDEllipse/.\n\n\nWang, Bin, Wenzhong Shi et Zelang Miao. 2015. « Confidence analysis of standard deviational ellipse and its extension into higher dimensional Euclidean space. » PloS one 10 (3): e0118537. https://doi.org/10.1371%2Fjournal.pone.0118537.\n\n\nWong, David WS. 1999. « Geostatistics as measures of spatial segregation. » Urban geography 20 (7): 635‑647. https://doi.org/10.2747/0272-3638.20.7.635.\n\n\nWong, David WS et Jay Lee. 2005. Statistical analysis of geographic information with ArcView GIS and ArcGIS. Wiley."
  },
  {
    "objectID": "04-AgregatsSpatiaux.html#sec-041",
    "href": "04-AgregatsSpatiaux.html#sec-041",
    "title": "4  Méthodes de détection d’agrégats spatiaux et spatio-temporels",
    "section": "\n4.1 Agrégats d’entités spatiales ponctuelles",
    "text": "4.1 Agrégats d’entités spatiales ponctuelles\n\n4.1.1 DBSCAN : agrégats spatiaux\n\n\n\n\n\nPourquoi utiliser DBSCAN ?\n\n\nDans le chapitre précédent, portant sur les méthodes de répartition ponctuelles, nous avons abordé la méthode KDE permettant de cartographier la densité de points dans une maille régulière (section 3.4.2). La carte de chaleur obtenue avec la KDE représente les valeurs de densité (variable continue) pour les pixels couvrant le territoire à l’étude.\nAvec l’algorithme DBSCAN (Ester et al. 1996), l’objectif est différent : il s’agit d’identifier des agrégats spatiaux d’évènements ponctuels dans un territoire donné (par exemple, des cas de maladies, d’accidents, d’espèces fauniques ou végétales, de crimes, etc.). Autrement dit, il s’agit d’identifier plusieurs régions du territoire à l’étude dans lesquelles la densité de points est forte.\nConcrètement, si la méthode KDE renvoie une variable continue pour l’ensemble du territoire, l’algorithme DBSCAN renvoie une variable qualitative uniquement pour les points du jeu de données.\n\n\n\n4.1.1.1 Fonctionnement de DBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) est un algorithme de classification non supervisée qui regroupe des observations en fonction de leur densité dans un espace à deux, trois ou n dimensions (Ester et al. 1996). Comme pour toute autre méthode de classification non supervisée, ces dimensions sont des variables. Par conséquent, en appliquant DBSCAN sur les coordonnées géographiques d’entités ponctuelles 2D (x, y) ou 3D (x, y, z), nous classifions les points du jeu de données.\nPrenons un jeu de données fictives (figure 4.1, a). À l’œil nu, nous identifions clairement cinq régions distinctes avec une forte densité de points et des zones de faible densité; ces dernières étant représentées par les points noirs avec DBSCAN (figure 4.1, b).\n\n\n\n\nFigure 4.1: Jeu de données fictives et classification DBSCAN avec cinq classes\n\n\n\nL’intérêt majeur de l’algorithme DBSCAN est qu’il est basé sur la densité des points et non sur la distance entre les points comme les algorithmes classiques de classification non supervisée que sont les k-moyennes, k-médianes, k-médoïdes ou la classification ascendante hiérarchique. Tel qu’illustré à la figure 4.2, l’utilisation de la distance pour identifier cinq groupes de points renvoie des résultats peu convaincants. D’une part, tous les points appartiennent à une classe, sans séparer les régions de fortes et de faibles densités. D’autre part, les algorithmes classiques basés sur la distance ne parviennent pas à bien identifier les deux agrégats circulaires (bleu et rouge à la figure 4.1, b) et parfois linéaires (vert et mauve à la figure 4.1, b).\n\n\n\n\nFigure 4.2: Classification avec d’autres algorithmes basés sur la distance\n\n\n\nL’algorithme DBSCAN comprend deux paramètres qui doivent être définis par la personne utilisatrice :\n\nLe rayon de recherche, dénommé \\(\\epsilon\\) (epsilon), habituellement basé sur la distance euclidienne. Les distances de Manhattan ou réticulaires peuvent aussi être utilisées.\nLe nombre minimum de points, dénommé \\(MinPts\\), requis pour qu’un point, incluant lui-même, soit considéré comme un point central et appartienne à un agrégat, un regroupement (cluster en anglais).\n\n\n\n\n\n\nAvantage de DBSCAN : nul besoin de spécifier le nombre d’agrégats (clusters)!\n\n\nComparativement à d’autres méthodes de classification non supervisées comme les k-moyennes, k-médianes et k-médoïdes, DBSCAN ne requiert pas de spécifier le nombre de classes à identifier dans le jeu de données. Autrement dit, appliqué à des géométries ponctuelles, l’algorithme DBSCAN détecte autant d’agrégats spatiaux que nécessaire en fonction des valeurs des deux paramètres (\\(\\epsilon\\) et \\(MinPts\\)).\n\n\nÀ la figure 4.3, nous appliquons l’algorithme DBSCAN à un semis de points avec un rayon de recherche de 500 mètres (\\(\\epsilon=500\\)) et un nombre minimum de cinq points (\\(MinPts = 5\\)). Dans un premier temps, l’algorithme distingue trois types de points :\n\nDes points centraux (core points en anglais) qui ont au moins cinq points (incluant eux-mêmes) dans un rayon de 500 mètres (points rouges).\nDes points frontières (border points) qui ont moins de cinq points (incluant eux-mêmes) dans un rayon de 500 mètres, mais qui sont inclus dans la zone tampon de 500 mètres d’un point central (points bleus).\nDes points aberrants (noise points) qui ont moins de cinq points (incluant eux-mêmes) dans un rayon de 500 mètres et qui ne sont pas inclus dans la zone tampon d’un point central (points noirs).\n\n\n\nFigure 4.3: Trois types de points identifiés par l’algorithme DBSCAN\n\nPar la suite, les étapes de l’algorithme sont les suivantes :\n\n\nÉtape 1. Formation du premier agrégat\n\nNous tirons au hasard un point central et l’assignons au premier agrégat (groupe ou cluster).\nPuis, les points compris dans la zone tampon du premier point central sont ajoutés à ce premier agrégat.\nDe façon itérative, nous étendons l’agrégat avec les points centraux ou frontières qui sont compris dans les zones tampons des points ajoutés précédemment.\n\n\n\nÉtape 2. Formation d’autres agrégats\n\nLorsque le premier agrégat est complété, nous tirons au hasard un autre point central n’appartenant pas au premier agrégat.\nNous appliquons la même démarche qu’à l’étape 1 pour étendre et compléter cet autre agrégat.\nLes deux sous-étapes ci-dessus sont répétées jusqu’à ce que tous les points centraux et frontières soient assignés à un agrégat.\n\n\n\nNous obtenons ainsi k agrégats (valeurs de 1 à k) tandis que les points aberrants sont affectés à la même classe (valeur de 0 habituellement). Appliqué au semis de points, DBSCAN a détecté deux agrégats et quatre points aberrants (figure 4.4).\n\n\nFigure 4.4: Résultats de l’algorithme DBSCAN\n\n\n4.1.1.2 Sensibilité et optimisation des paramètres de DBSCAN\nLes résultats de l’algorithme de DBSCAN varient en fonction de ses deux paramètres, soit le rayon de recherche (\\(\\epsilon\\)) et le nombre minimum de points (\\(MinPts\\)).\nConcernant le paramètre \\(\\epsilon\\), plus sa valeur est réduite, plus le nombre de points identifiés comme aberrants est important. Inversement, plus elle est grande, plus le nombre d’agrégats diminue. En guise d’illustration, faisons varier la valeur du rayon en maintenant à cinq le nombre minimum de points :\n\nAvec un rayon de 250 mètres, cinq agrégats sont identifiés tandis que 29 points sont considérés comme du bruit (figure 4.5, a).\nAvec un rayon de 500 mètres, la solution est plus optimale avec deux agrégats et cinq points aberrants (figure 4.5, b).\nAvec un rayon de 1000 mètres, deux agrégats sont aussi identifiés. Par contre, il ne reste plus qu’un point aberrant. Par conséquent, quatre points qui, à l’œil nu, sont très éloignés d’un agrégat y sont pourtant affectés (figure 4.5, c).\nAvec un rayon de 1500 mètres, tous les points sont affectés à un et un seul agrégat (figure 4.5, d).\n\n\n\nFigure 4.5: Variations de résultats de l’algorithme DBSCAN selon la taille du rayon\n\nConcernant le paramètre \\(MinPts\\), plusieurs stratégies ont été proposées pour fixer sa valeur :\n\n\n\\(MinPts \\geq dim(D) + 1\\), c’est-à-dire que sa valeur doit être minimalement égale au nombre de dimensions (variables) plus un du jeu de données.\n\n\\(MinPts = dim(D) \\times 2\\), c’est-à-dire que le nombre de points devrait être égal à deux fois le nombre de dimensions du tableau (Sander et al. 1998).\n\n\\(MinPts = 4\\) quand le jeu de données ne comprend que deux dimensions (Ester et al. 1996), soit un critère qui s’applique à des géométries ponctuelles 2D.\n\nAprès avoir fixé le nombre minimal de points, nous pouvons optimiser la valeur du rayon de recherche de la façon suivante :\n\nPour chacun des points, calculer la distance au kième point le plus proche.\nTrier les valeurs obtenues pour construire un graphique en courbe.\nDans ce graphique, utiliser le critère du coude pour repérer la ou les valeurs signalant un décrochement dans la courbe. À la lecture de la figure 4.6, les valeurs d’epsilon (\\(\\epsilon\\)) à retenir pourraient être 300, 350, 425 et 450 mètres.\n\n\n\n\n\nFigure 4.6: Optimisation de la valeur d’epsilon\n\n\n\nSi vous repérez plusieurs seuils de distance dans le graphique des distances au kième plus proche voisin, réalisez et comparez les résultats des DBSCAN avec ces valeurs d’epsilon. À la figure 4.7, nous constatons que les résultats avec des seuils de 425 et 450 mètres sont identiques et semblent optimaux. Par contre, la solution avec un rayon de 350 mètres identifie deux points aberrants qui pourraient être intégrés au deuxième agrégat tandis que celle avec un rayon de 300 mètres identifie un agrégat supplémentaire, mais classifie de nombreux points comme aberrants.\n\n\n\n\n\nQuel résultat choisir parmi les quatre solutions?\n\n\nComme pour toute analyse de classification, votre choix peut être objectif et reposer uniquement sur des indicateurs statistiques (ici, le graphique des distances au k plus proche voisin). Il devrait aussi s’appuyer sur vos connaissances du terrain. Par exemple, l’identification d’un troisième agrégat avec une valeur d’epsilon fixée à 300 mètres pourrait refléter selon vous une réalité terrain particulièrement intéressante qui motiverait fortement le choix de cette solution.\n\n\n\n\nFigure 4.7: Comparaison de solutions DBSCAN avec différentes valeurs d’epsilon\n\n\n\n\n\n\nAutres algorithmes de classification non supervisée basée sur la densité\n\n\nBien que DBSCAN soit l’algorithme le plus utilisé, d’autres algorithmes basés sur la densité peuvent être utilisés pour détecter des agrégats spatiaux de points, notamment :\n\nHDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) (Campello, Moulavi et Sander 2013). Brièvement, cette version modifiée de DBSCAN permet d’obtenir une hiérarchie de partitions, comme dans une classification ascendante hiérarchique.\nOPTICS (Ordering Points To Identify the Clustering Structure) (Campello, Moulavi et Sander 2013). Avec OPTICS, la distance de voisinage (\\(\\epsilon\\)) n’a pas besoin d’être spécifiée. Succinctement, pour chaque point du jeu de données, il utilise la distance au \\(k\\) (\\(MinPts\\)) plus proche voisin.\n\nApplication à des évènements localisés sur un réseau de rues\nLorsque les évènements sont localisés sur un réseau de rues (des accidents par exemple), il convient d’utiliser une autre métrique que la distance euclidienne pour le rayon de recherche (\\(\\epsilon\\)), soit la distance du chemin le plus court à travers le réseau de rues, ce que nous verrons au chapitre suivant (section 6.4). Geoff Boeing a aussi proposé un un code Python basé sur la librairie OSMnx.\n\n\n\n4.1.2 ST-DBSCAN : agrégats spatio-temporels\nDerya Birant et Alp Kut (2007) ont proposé une modification de l’algorithme de DBSCAN afin qu’il puisse s’appliquer à des données spatio-temporelles (\\(x\\), \\(y\\), \\(d\\)) avec \\(d\\) étant la date de l’évènement. Dénommé ST-DBSCAN, l’algorithme comprend toujours les deux paramètres de DBSCAN (\\(MinPts\\) et \\(\\epsilon\\)), auxquels s’ajoute un autre paramètre \\(\\epsilon\\) pour le temps (défini en heure, jour, semaine, mois ou année). Autrement dit, deux paramètres de distance sont utilisés : \\(\\epsilon_1\\) pour la proximité spatiale (comme avec DBSCAN) et \\(\\epsilon_2\\) pour la proximité temporelle (Birant et Kut 2007). De la sorte, deux points sont considérés comme voisins si la distance spatiale et la distance temporelle sont toutes deux inférieures aux seuils fixés.\n\n\n\n\n\nFenêtre temporelle des points formant un agrégat\n\n\nAttention, les points formant un agrégat peuvent avoir une fenêtre temporelle bien plus grande que le seuil \\(\\epsilon_2\\) fixé. Par exemple, fixons les valeurs de \\(\\epsilon_1\\) à 500 mètres et de \\(\\epsilon_2\\) à 7 jours. Si le point A (\\(d\\) = 2023-01-15) est distant de 400 mètres du point B (\\(d\\) = 2023-01-20), les deux points sont considérés comme voisins. Par contre, si le point B est distant du point C (\\(d\\) = 2023-01-25) de moins de 500 mètres, il peut être aussi agrégé à l’agrégat puisque l’écart temporel entre B et C est de 5 jours.\nHabituellement, plus la valeur de \\(\\epsilon_2\\) est faible, plus le nombre de points considérés comme aberrants est important.\n\n\n\n4.1.3 Mise en œuvre dans R\n\n4.1.3.1 DBSCAN\nNous utilisons le package dbscan (Hahsler et Piekenbrock 2022; Hahsler, Piekenbrock et Doran 2019) dans lequel sont implémentés plusieurs algorithmes, dont DBSCAN, mais aussi OPTICS et HDBSCAN. La fonction dbscan(x, eps, minPts, weights = NULL) comprend plusieurs paramètres :\n\n\nx: une matrice, un DataFrame, un objet dist ou un objet frNN.\n\neps: le rayon de recherche epsilon (\\(\\epsilon\\)).\n\nminPts: le nombre de points minimum requis pour que chaque point soit considéré comme un point central.\n\nweights: un vecteur numérique optionnel pour pondérer les points.\n\nPour illustrer le fonctionnement de la méthode DBSCAN, nous avons extrait les accidents d’un jeu de données sur les incidents de sécurité publique survenus sur le territoire de la Ville de Sherbrooke de juillet 2019 à juin 2022 (figure 4.8).\n\n\n\n\nFigure 4.8: Accidents survenus entre juillet 2019 et juin 2022, Ville de Sherbrooke\n\n\n\nDans le code ci-dessous, nous réalisons trois étapes préalables au calcul de DBSCAN :\n\nImportation des accidents.\nRécupération des coordonnées (\\(x\\), \\(y\\)) des accidents et stockage dans une matrice.\nConstruction du graphique à partir de la distance au quatrième point le plus proche.\n\nNous n’observons pas de décrochement particulier dans la courbe de la figure 4.9. Par conséquent, nous pourrions tout aussi bien retenir une distance euclidienne de 250, 500, 1000 ou 1500 mètres pour epsilon.\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(dbscan)\nlibrary(ggplot2)\n## Importation des accidents\nAccidents.sf &lt;- st_read(dsn = \"data/chap04/DataAccidentsSherb.shp\", quiet=TRUE)\n## Coordonnées géographiques\nxy &lt;- st_coordinates(Accidents.sf)\n## Graphique pour la distance au quatrième voisin le plus proche\nDistKplusproche &lt;- kNNdist(xy, k = 4)\nDistKplusproche &lt;- as.data.frame(sort(DistKplusproche, decreasing = FALSE))\nnames(DistKplusproche) &lt;- \"distance\"\nggplot(data = DistKplusproche)+\n  geom_path(aes(x = 1:nrow(DistKplusproche), y = distance), size=1)+\n  labs(x = \"Points triés par ordre croissant selon la distance\",\n       y = \"Distance au quatrième point le plus proche\")+\n  geom_hline(yintercept=250, color = \"#08306b\", linetype=\"dashed\", size=1)+\n  geom_hline(yintercept=500, color = \"#00441b\", linetype=\"dashed\", size=1)+\n  geom_hline(yintercept=1000, color = \"#67000d\", linetype=\"dashed\", size=1)+\n  geom_hline(yintercept=1500, color = \"#3f007d\", linetype=\"dashed\", size=1)\n\n\n\nFigure 4.9: Optimisation de la valeur d’epsilon pour les accidents\n\n\n\nAppliquons la méthode DBSCAN avec un minimum de quatre points et les quatre valeurs de distance euclidienne.\n\nset.seed(123456789)\n## DBSCAN avec les quatre distances\ndbscan250  &lt;- dbscan(xy, eps = 250, minPts = 4)\ndbscan500  &lt;- dbscan(xy, eps = 500, minPts = 4)\ndbscan1000 &lt;- dbscan(xy, eps = 1000, minPts = 4)\ndbscan1500 &lt;- dbscan(xy, eps = 1500, minPts = 4)\n## Affichage des résultats\ndbscan250\n\nDBSCAN clustering for 1106 objects.\nParameters: eps = 250, minPts = 4\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 45 cluster(s) and 353 noise points.\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19 \n353   4   6   7   5  15   5   4  22   7   5   5  19 295   4  18   5   4   7   8 \n 20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39 \n  4   6  49   4  11   4   4  41   5   4   8  31  25  10   6  23   4   5  18  15 \n 40  41  42  43  44  45 \n  6   4   6   4   7   4 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints\n\ndbscan500\n\nDBSCAN clustering for 1106 objects.\nParameters: eps = 500, minPts = 4\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 33 cluster(s) and 143 noise points.\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19 \n143   7   6  14   4   5   5   4   6 734  18   8   6   6   5   9  16   9   5   4 \n 20  21  22  23  24  25  26  27  28  29  30  31  32  33 \n 23   5   3   4   6   5   4   9   6   9   4   4   4   6 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints\n\ndbscan1000\n\nDBSCAN clustering for 1106 objects.\nParameters: eps = 1000, minPts = 4\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 10 cluster(s) and 42 noise points.\n\n  0   1   2   3   4   5   6   7   8   9  10 \n 42   8   6  37 962   8   4   6   5  12  16 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints\n\ndbscan1500\n\nDBSCAN clustering for 1106 objects.\nParameters: eps = 1500, minPts = 4\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 3 cluster(s) and 7 noise points.\n\n   0    1    2    3 \n   7 1047   12   40 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints\n\n\nPour les 1106 accidents du jeu de données, les résultats des quatre DBSCAN ci-dessus sont les suivants :\n\nAvec \\(\\epsilon = 250\\), 45 agrégats et 353 points aberrants (bruit).\nAvec \\(\\epsilon = 500\\), 33 agrégats et 143 points aberrants.\nAvec \\(\\epsilon = 1000\\), 10 agrégats et 42 points aberrants.\nAvec \\(\\epsilon = 1500\\), 3 agrégats et 7 points aberrants.\n\nPour les n points du jeu de données, l’appartenance à un agrégat est enregistrée dans un vecteur numérique avec des valeurs de 0 à k agrégats (ResultatDbscan$cluster). Notez que la valeur de 0 est attribuée aux points aberrants. Avec ce vecteur, nous enregistrons les résultats dans un nouveau champ de la couche de points sf.\n\n## Enregistrement des résultats de DBSCAN dans la couche de points sf\nAccidents.sf$Dbscan250  &lt;- as.character(dbscan250$cluster)\nAccidents.sf$Dbscan500  &lt;- as.character(dbscan500$cluster)\nAccidents.sf$Dbscan1000 &lt;- as.character(dbscan1000$cluster)\nAccidents.sf$Dbscan1500 &lt;- as.character(dbscan1500$cluster)\n\nAccidents.sf$Dbscan250 &lt;- ifelse(nchar(Accidents.sf$Dbscan250) == 1,\n                                        paste0(\"0\", Accidents.sf$Dbscan250),\n                                        Accidents.sf$Dbscan250)\nAccidents.sf$Dbscan500 &lt;- ifelse(nchar(Accidents.sf$Dbscan500) == 1,\n                                        paste0(\"0\", Accidents.sf$Dbscan500),\n                                        Accidents.sf$Dbscan500)\nAccidents.sf$Dbscan1000 &lt;- ifelse(nchar(Accidents.sf$Dbscan1000) == 1,\n                                        paste0(\"0\", Accidents.sf$Dbscan1000),\n                                        Accidents.sf$Dbscan1000)\nAccidents.sf$Dbscan1500 &lt;- ifelse(nchar(Accidents.sf$Dbscan1500) == 1,\n                                        paste0(\"0\", Accidents.sf$Dbscan1500),\n                                        Accidents.sf$Dbscan1500)\n\nNous cartographions finalement les résultats pour les quatre solutions.\n\ntmap_mode(\"plot\")\ntm_shape(Accidents.sf)+tm_dots(col=\"Dbscan250\", title = \"DBSCAN 250\", size = .5)\n\n\n\ntm_shape(Accidents.sf)+tm_dots(col=\"Dbscan500\", title = \"DBSCAN 500\", size = .5)\n\n\n\ntm_shape(Accidents.sf)+tm_dots(col=\"Dbscan1000\", title = \"DBSCAN 1000\", size = .5)\n\n\n\ntm_shape(Accidents.sf)+tm_dots(col=\"Dbscan1500\", title = \"DBSCAN 1500\", size = .5)\n\n\n\n\n\n4.1.3.2 ST-DBSCAN\nPour l’algorithme ST-DBSCAN, nous utilisons une fonction R proposée par Colin Kerouanton.\n\nsource(\"code_complementaire/stdbscan.R\")\n\nCalculons ST-DBSCAN avec une distance spatiale de 1000 mètres et une distance temporelle de 21 jours. Nous obtenons 26 agrégats et 584 points identifiés comme aberrants.\n\n## Importation des accidents\nAccidents.sf &lt;- st_read(dsn = \"data/chap04/DataAccidentsSherb.shp\", quiet=TRUE)\n## Coordonnées géographiques\nxy &lt;- st_coordinates(Accidents.sf)\nAccidents.sf$x &lt;- xy[,1]\nAccidents.sf$y &lt;- xy[,2]\n## Vérifions que le champ DATEINCIDE est bien au format date\nstr(Accidents.sf$DATEINCIDE)\n\n Date[1:1106], format: \"2021-12-11\" \"2022-05-16\" \"2021-08-12\" \"2019-08-02\" \"2020-03-02\" ...\n\n## Calcul de st-dbscan avec une distance de 1000 mètres et 21 jours\nResultats.stdbscan &lt;- stdbscan(x = Accidents.sf$x,\n                              y = Accidents.sf$y,\n                              time = Accidents.sf$DATEINCIDE,\n                              eps1 = 1000,\n                              eps2 = 21,\n                              minpts = 4)\n## Enregistrement des résultats de ST-DBSCAN dans la couche de points sf\nAccidents.sf$stdbscan.1000_21 &lt;- as.character(Resultats.stdbscan$cluster)\nAccidents.sf$stdbscan.1000_21 &lt;- ifelse(nchar(Accidents.sf$stdbscan.1000_21) == 1,\n                                        paste0(\"0\", Accidents.sf$stdbscan.1000_21),\n                                        Accidents.sf$stdbscan.1000_21)\n## Nombre de points par agrégat\ntable(Accidents.sf$stdbscan.1000_21)\n\n\n 00  01  02  03  04  05  06  07  08  09  10  11  12  13  14  15  16  17  18  19 \n584   4   6   5   4   4   7   7   4 178 156  13  17   4  22  32   6   7   8   8 \n 20  21  22  23  24  25  26 \n  5   6   4   3   5   4   3 \n\n\nPour faciliter l’analyse des résultats de ST-DBSCAN, nous conseillons de :\n\nConstruire un tableau récapitulatif pour les agrégats avec le nombre de points, les dates de début et de fin et l’intervalle temporel.\nConstruire un graphique avec les agrégats (axe des y) et la dimension temporelle (axe des x).\nCartographier les résultats.\n\nLe code ci-dessous génère le tableau récapitulatif. Nous constatons ainsi que les agrégats 9 et 10 incluent respectivement 178 et 156 points avec des intervalles temporels importants (respectivement 251 et 319 jours).\n\nlibrary(dplyr)\n## Sélection des points appartenant à un agrégat\nAgregats &lt;- subset(Accidents.sf, \n                   Accidents.sf$stdbscan.1000_21 != \"00\")\n## Conversion de la date au format POSIXct\nAgregats$dtPOSIXct &lt;- as.POSIXct(Agregats$DATEINCIDE, format = \"%Y/%m/%d\")\n## Tableau récapitulatif\nlibrary(\"dplyr\")  \nTableau.stdbscan &lt;-\n    st_drop_geometry(Agregats) %&gt;%\n      group_by(stdbscan.1000_21) %&gt;% \n      summarize(points = n(),\n                date.min = min(DATEINCIDE),\n                date.max = max(DATEINCIDE),\n                intervalle.jours = as.numeric(max(DATEINCIDE)-min(DATEINCIDE)))\n## Affichage du tableau\nprint(Tableau.stdbscan, n = nrow(Tableau.stdbscan))\n\n# A tibble: 26 × 5\n   stdbscan.1000_21 points date.min   date.max   intervalle.jours\n   &lt;chr&gt;             &lt;int&gt; &lt;date&gt;     &lt;date&gt;                &lt;dbl&gt;\n 1 01                    4 2019-08-08 2019-09-04               27\n 2 02                    6 2021-12-15 2022-01-25               41\n 3 03                    5 2019-07-21 2019-08-30               40\n 4 04                    4 2020-11-10 2020-12-12               32\n 5 05                    4 2022-01-08 2022-02-13               36\n 6 06                    7 2021-06-09 2021-07-02               23\n 7 07                    7 2020-06-23 2020-08-07               45\n 8 08                    4 2021-09-30 2021-10-27               27\n 9 09                  178 2019-07-02 2020-03-09              251\n10 10                  156 2021-03-13 2022-01-26              319\n11 11                   13 2021-07-24 2021-09-11               49\n12 12                   17 2021-10-21 2022-01-12               83\n13 13                    4 2021-06-16 2021-07-07               21\n14 14                   22 2022-04-11 2022-06-27               77\n15 15                   32 2020-09-11 2020-12-18               98\n16 16                    6 2020-01-17 2020-02-08               22\n17 17                    7 2022-05-07 2022-05-30               23\n18 18                    8 2021-04-01 2021-05-27               56\n19 19                    8 2020-07-15 2020-09-11               58\n20 20                    5 2019-07-05 2019-07-31               26\n21 21                    6 2022-01-15 2022-03-02               46\n22 22                    4 2020-06-17 2020-06-30               13\n23 23                    3 2022-03-12 2022-03-18                6\n24 24                    5 2021-06-29 2021-07-21               22\n25 25                    4 2021-09-20 2021-10-25               35\n26 26                    3 2021-04-13 2021-04-26               13\n\n\nLa figure 4.10 présente les points et l’étendue temporelle de chaque agrégat.\n\n## Construction du graphique\nggplot(Agregats) + \n  geom_point(aes(x = dtPOSIXct, \n                 y = stdbscan.1000_21, \n                 color = stdbscan.1000_21),\n             show.legend = FALSE) +\n  scale_x_datetime(date_labels = \"%Y/%m\")+\n  labs(x= \"Temps\",\n       y= \"Identifiant de l'agrégat\",\n       title = \"ST-DBSCAN avec Esp1 = 1000, Esp2 = 21 et MinPts = 4\")\n\n\n\nFigure 4.10: Intervalles temporels des agrégats ST-DBSCAN\n\n\n\nLa cartographie des agrégats est présentée à la figure 4.11 avec en noir les points aberrants.\n\n## Création de deux couches : l'une pour les agrégats, l'autre pour les points aberrants\nstdbcan.Agregats &lt;- subset(Accidents.sf, Accidents.sf$stdbscan.1000_21 != \"00\")\nstdbcan.Bruit    &lt;- subset(Accidents.sf, Accidents.sf$stdbscan.1000_21 == \"00\")\n## Cartographie\ntmap_mode(\"plot\")\ntm_shape(Arrondiss)+tm_fill(col=\"#f7f7f7\")+tm_borders(col=\"black\")+\ntm_shape(stdbcan.Bruit)+\n  tm_dots(shape = 21, col=\"black\", size=.2)+\n  tm_shape(stdbcan.Agregats)+\n  tm_dots(shape = 21, col=\"stdbscan.1000_21\", size=.2, title = \"Agrégat\")+\ntm_layout(frame = FALSE, legend.position = c(\"center\", \"bottom\"),\n          legend.text.size = .85, legend.outside = TRUE)\n\n\n\nFigure 4.11: Agrégats identifiés avec ST-DBSCAN"
  },
  {
    "objectID": "04-AgregatsSpatiaux.html#sec-042",
    "href": "04-AgregatsSpatiaux.html#sec-042",
    "title": "4  Méthodes de détection d’agrégats spatiaux et spatio-temporels",
    "section": "\n4.2 Méthodes de balayage de Kulldorff",
    "text": "4.2 Méthodes de balayage de Kulldorff\n\n4.2.1 Objectifs de la méthode, types d’analyses, de modèles et d’agrégats\n\n4.2.2 Principes de base de la méthode\n\n4.2.2.1 Type de balayage (cercle ou ellipse)\n\n4.2.2.2 Variable de contrôle\n\n4.2.3 Mise en œuvre dans R\n\n4.2.3.1 Agrégats temporels, spatiaux et spatio-temporels\n\n4.2.3.2 Introduction de variables de contrôle\n\n4.2.3.3 Exploration d’autres types de modèles"
  },
  {
    "objectID": "04-AgregatsSpatiaux.html#sec-044",
    "href": "04-AgregatsSpatiaux.html#sec-044",
    "title": "4  Méthodes de détection d’agrégats spatiaux et spatio-temporels",
    "section": "\n4.3 Quiz de révision du chapitre",
    "text": "4.3 Quiz de révision du chapitre\n\n\n\n\n\nL’algorithme DBSCAN est basé sur :\n\n\nRelisez au besoin la section 4.1.1.1.\n\n\n\n\n\n\nLa distance entre les points\n\n\n\n\n\n\n\nLa densité des points\n\n\n\n\n\n\n\n\n\n\nAvec l’algorithme DBSCAN, vous devez spécifier le nombre de groupes (agrégats).\n\n\nRelisez au besoin la section 4.1.1.1.\n\n\n\n\n\n\nVrai\n\n\n\n\n\n\n\nFaux\n\n\n\n\n\n\n\n\n\n\nQuels sont les trois types de points identifiés par DBSCAN?\n\n\nRelisez au besoin l’encadré à la section 4.1.1.1.\n\n\n\n\n\n\nPoints centraux\n\n\n\n\n\n\n\nPoints médians\n\n\n\n\n\n\n\nPoints frontières\n\n\n\n\n\n\n\nPoints aberrants\n\n\n\n\n\n\n\n\n\n\nQuels sont les deux paramètres de l’algorithme DBSCAN?\n\n\nRelisez au besoin la section 4.1.1.1.\n\n\n\n\n\n\nLe nombre de points minimum pour identifier les points centraux (MinPts)\n\n\n\n\n\n\n\nLe rayon de recherche (epsilon)\n\n\n\n\n\n\n\nLa distance standard (DS)\n\n\n\n\n\n\n\n\n\n\nPlus la valeur d’epsilon est grande, plus le nombre d’agrégats diminue.\n\n\nRelisez au besoin la section 4.1.1.2.\n\n\n\n\n\n\nVrai\n\n\n\n\n\n\n\nFaux\n\n\n\n\n\n\n\n\n\n\nQuels sont les trois paramètres de l’algorithme ST-DBSCAN?\n\n\nRelisez au besoin la section 4.1.2.\n\n\n\n\n\n\nLe nombre de points minimum pour identifier les points centraux (MinPts)\n\n\n\n\n\n\n\nLe rayon de recherche pour la proximité spatiale (eps1)\n\n\n\n\n\n\n\nLe rayon de recherche pour la proximité temporelle (eps2)\n\n\n\n\n\n\n\nLa région d’étude\n\n\n\n\n\n\n\n\n\nVérifier votre résultat"
  },
  {
    "objectID": "04-AgregatsSpatiaux.html#sec-045",
    "href": "04-AgregatsSpatiaux.html#sec-045",
    "title": "4  Méthodes de détection d’agrégats spatiaux et spatio-temporels",
    "section": "\n4.4 Exercices de révision",
    "text": "4.4 Exercices de révision\n\n\n\n\n\nExercice 1. Application de l’algorithme DBSCAN\n\n\nL’objectif est d’appliquer cet algorithme sur des accidents impliquant des personnes à vélo sur l’île de Montréal (voir la section 4.1.3.1). Ces données ouvertes sur les collisions routières et leur documentation sont disponibles au lien suivant.\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(dbscan)\nlibrary(ggplot2)\n## Importation des données\nCollissions &lt;- st_read(dsn = \"data/chap04/collisions.gpkg\", \n                       layer = \"CollisionsRoutieres\", \n                       quiet = TRUE)\n## Collisions impliquant au moins une personne à vélo en 2020 et 2021\nColl.Velo &lt;- subset(Collissions,\n                    Collissions$NB_VICTIMES_VELO &gt; 0 &\n                      Collissions$AN %in% c(2020, 2021))\n## Coordonnées géographiques\nxy &lt;- st_coordinates(Coll.Velo)\n## Graphique pour la distance au quatrième voisin le plus proche\nDistKplusproche &lt;- kNNdist(xy, k = 4)\nDistKplusproche &lt;- as.data.frame(sort(DistKplusproche, decreasing = FALSE))\nnames(DistKplusproche) &lt;- \"distance\"\nggplot(à compléter)+\n  geom_path(à compléter)+\n  labs(à compléter)+\n  geom_hline(yintercept=250, color = \"#08306b\", linetype=\"dashed\", size=1)+\n  geom_hline(yintercept=500, color = \"#00441b\", linetype=\"dashed\", size=1)+\n  geom_hline(yintercept=1000, color = \"#67000d\", linetype=\"dashed\", size=1)\n## DBSCAN avec les quatre distances\nset.seed(123456789)\ndbscan250  &lt;- à compléter\ndbscan500  &lt;- à compléter\ndbscan1000 &lt;- à compléter\n## Affichage des résultats\ndbscan250\ndbscan500\ndbscan1000\n## Enregistrement dans la couche de points sf Coll.Velo\nColl.Velo$dbscan250 &lt;- à compléter\nColl.Velo$dbscan500 &lt;- à compléter\nColl.Velo$dbscan1000 &lt;- à compléter\n\nColl.Velo$dbscan250 &lt;- ifelse(nchar(Coll.Velo$dbscan250) == 1,\n                              paste0(\"0\", Coll.Velo$dbscan250),\n                              Coll.Velo$dbscan250)\nColl.Velo$dbscan500 &lt;- ifelse(nchar(Coll.Velo$dbscan500) == 1,\n                               paste0(\"0\", Coll.Velo$dbscan500),\n                               Coll.Velo$dbscan500)\nColl.Velo$dbscan1000 &lt;- ifelse(nchar(Coll.Velo$dbscan1000) == 1,\n                               paste0(\"0\", Coll.Velo$dbscan1000),\n                               Coll.Velo$dbscan1000)\n# Extraction des agrégats\nAgregats.dbscan250  &lt;- subset(Coll.Velo, dbscan250 != \"00\")\nAgregats.dbscan500  &lt;- subset(Coll.Velo, dbscan500 != \"00\")\nAgregats.dbscan1000 &lt;- subset(Coll.Velo, dbscan1000 != \"00\")\n## Cartographie\ntmap_mode(\"plot\")\nà compléter\n\nCorrection à la section 9.4.1.\n\n\n\n\n\n\n\nExercice 2. Application de l’algorithme ST-DBSCAN\n\n\nAvec le même jeu de données, réaliser un ST-DBSCAN avec les paramètres suivants : distance spatiale de 500 mètres, distance temporelle de 30 jours et quatre points minimum (voir la section 4.1.3.2).\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(dbscan)\nlibrary(ggplot2)\n## Importation des données\nCollissions &lt;- st_read(dsn = \"data/chap04/collisions.gpkg\", \n                       layer = \"CollisionsRoutieres\",\n                       quiet = TRUE)\n## Collisions impliquant au moins une personne à vélo en 2020 et 2021\nColl.Velo &lt;- subset(Collissions,\n                    Collissions$NB_VICTIMES_VELO &gt; 0 &\n                      Collissions$AN %in% c(2020, 2021))\n## Coordonnées géographiques\nxy &lt;- st_coordinates(Coll.Velo)\nColl.Velo$x &lt;- xy[,1]\nColl.Velo$y &lt;- xy[,2]\n## Conversion du champ DT_ACCDN au format Date\nColl.Velo$DT_ACCDN &lt;- as.Date(Coll.Velo$DT_ACCDN)\n## ST-DBSCAN avec eps1 = 500, esp2 = 30 et minpts = 4\nResultats.stdbscan &lt;- stdbscan(À compléter)\n## Enregistrement des résultats ST-DBSCAN dans la couche de points sf\nColl.Velo$stdbscan &lt;- as.character(Resultats.stdbscan$cluster)\nColl.Velo$stdbscan &lt;- ifelse(nchar(Coll.Velo$stdbscan) == 1,\n                             paste0(\"0\", Coll.Velo$stdbscan),\n                             Coll.Velo$stdbscan)\n## Nombre de points par agrégat avec la fonction table\ntable(Coll.Velo$stdbscan)\n## Sélection des points appartenant à un agrégat avec la fonction subset\nAgregats &lt;- subset(Coll.Velo, stdbscan != \"00\")\n## Conversion de la date au format POSIXct\nAgregats$dtPOSIXct &lt;- as.POSIXct(Agregats$DT_ACCDN, format = \"%Y/%m/%d\")\n## Tableau récapitulatif\nlibrary(\"dplyr\")  \nTableau.stdbscan &lt;- À compléter\n## Affichage du tableau\nprint(Tableau.stdbscan, n = nrow(Tableau.stdbscan))\n## Construction du graphique\nÀ compléter\n## Création d'une couche pour les agrégats\nstdbcan.Agregats &lt;- subset(Coll.Velo, stdbscan != \"00\")\n## Cartographie\nÀ compléter\n\nCorrection à la section 9.4.2.\n\n\n\n\n\n\nBirant, Derya et Alp Kut. 2007. « ST-DBSCAN: An algorithm for clustering spatial–temporal data. » Data & knowledge engineering 60 (1): 208‑221. https://doi.org/10.1016/j.datak.2006.01.013.\n\n\nCampello, Ricardo JGB, Davoud Moulavi et Jörg Sander. 2013. « Density-based clustering based on hierarchical density estimates. » In Advances in Knowledge Discovery and Data Mining: 17th Pacific-Asia Conference, PAKDD 2013, Gold Coast, Australia, April 14-17, 2013, Proceedings, Part II 17, 160‑172. s.n. http://dx.doi.org/10.1007/978-3-642-37456-2_14.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander et Xiaowei Xu. 1996. « A density-based algorithm for discovering clusters in large spatial databases with noise. » In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), 96:226‑231. 34. s.n. https://dl.acm.org/doi/10.5555/3001460.3001507.\n\n\nHahsler, Michael et Matthew Piekenbrock. 2022. dbscan: Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Related Algorithms. s.n. https://CRAN.R-project.org/package=dbscan.\n\n\nHahsler, Michael, Matthew Piekenbrock et Derek Doran. 2019. « dbscan: Fast Density-Based Clustering with R. » Journal of Statistical Software 91 (1): 1‑30. https://doi.org/10.18637/jss.v091.i01.\n\n\nKulldorff, Martin. 1997. « A spatial scan statistic. » Communications in Statistics-Theory and methods 26 (6): 1481‑1496. https://doi.org/10.1080/03610929708831995.\n\n\nSander, Jörg, Martin Ester, Hans-Peter Kriegel et Xiaowei Xu. 1998. « Density-based clustering in spatial databases: The algorithm gdbscan and its applications. » Data mining and knowledge discovery 2: 169‑194. https://doi.org/10.1023/A:1009745219419."
  },
  {
    "objectID": "05-AnalyseReseau.html#sec-051",
    "href": "05-AnalyseReseau.html#sec-051",
    "title": "5  Mesures d’accessibilité spatiale selon différents modes de transport",
    "section": "\n5.1 Notions relatives à l’analyse de réseau",
    "text": "5.1 Notions relatives à l’analyse de réseau\n\n5.1.1 Définition d’un réseau\nUn réseau est un ensemble de lignes connectées par des nœuds – voies ferrées, voies routières, canalisations d’eau ou de gaz, fleuves et affluents drainant une région, etc. – reliant un territoire (figure 5.1). Pour un réseau routier, l’information sémantique rattachée tant aux lignes (sens de circulation, vitesse autorisée, rues piétonnières, pistes cyclables, etc.) qu’aux nœuds (types d’intersection, autorisation de virage à gauche, etc.) est utilisée pour modéliser un réseau.\n\n\nFigure 5.1: Réseau : un ensemble de lignes connectées par des nœuds\n\n\n5.1.2 Principaux problèmes résolus en analyse de réseau\nL’analyse de réseau permet de résoudre trois principaux problèmes (figure 5.2) :\n\nTrouver le trajet le plus court ou le plus rapide entre deux points, basé sur l’algorithme de Dijkstra (1959).\nTrouver la route optimale comprenant plusieurs arrêts (problème du voyageur de commerce).\nDéfinir des zones de desserte autour d’une origine, basé aussi sur l’algorithme de Dijkstra.\n\n\n\nFigure 5.2: Trois principaux problèmes résolus en analyse de réseau\n\nÀ cela s’ajoutent quatre autres problèmes :\n\nTrouver les k services les plus proches à partir d’une origine (figure 5.3, a).\nConstruire une matrice de distance origines-destinations (figure 5.3, b), dont l’intérêt principal est de permettre par la suite le calcul de n’importe quelle mesure d’accessibilité.\nRésoudre le problème de tournées de véhicules dont l’objectif est d’optimiser les tournées d’une flotte de véhicules en fonction des ressources disponibles, de la localisation des clients, des lieux de dépôt de marchandises, etc.\nRéaliser un modèle localisation-affectation dont l’objectif est d’optimiser la localisation d’un ou plusieurs nouveaux équipements en fonction de la demande, et ce, en minimisant la distance agrégée entre les points d’offre et de demande. Par exemple, une région ayant 15 hôpitaux desservant deux millions d’habitants souhaiterait ajouter trois autres établissements. En fonction de l’offre existante (hôpitaux pondérés par le nombre de lits), de la distribution spatiale de la population et de la localisation des sites potentiels des trois hôpitaux, il s’agit de choisir ceux qui minimisent la distance entre les points d’offre et de demande.\n\n\n\nFigure 5.3: Autres problèmes résolus en analyse de réseau\n\nCes problèmes peuvent être résolus selon différents modes de transport, à savoir le chemin le plus rapide en véhicule motorisé, à pied, en vélo et en transport en commun (figure 5.4).\n\n\nFigure 5.4: Chemin le plus rapide selon différents modes de transport\n\n\n5.1.3 Analyse de réseau et entités polygonales\nVous avez compris que les problèmes présentés plus haut sont réalisés à partir d’entités spatiales ponctuelles. Pour estimer le trajet le plus court ou le plus rapide entre un point et un polygone (un parc urbain par exemple), il faut préalablement le convertir en point. Plusieurs solutions sont envisageables (Apparicio et Séguin 2006) :\n\nCalculer le trajet entre le point et le centroïde du parc (figure 5.5, a). Le centroïde est alors rattaché au tronçon de rue le plus proche. Cette solution est peu précise : plus la superficie du polygone est grande, plus l’imprécision augmente.\nSi le parc a plusieurs entrées, il suffit de les positionner le long du périmètre et de calculer les trajets à partir de ces points.\nSi le parc n’a pas d’entrée, il convient de positionner des points le long du périmètre, espacés d’une distance prédéterminée (figure 5.5, b). Bien qu’elle soit longue à calculer, cette solution est bien plus précise. Par exemple, avec des points espacés de 20 mètres le long du périmètre du parc, l’erreur maximale est de 10 mètres.\n\n\n\nFigure 5.5: Méthode pour déterminer le trajet le plus court entre une entité ponctuelle et une entité polygonale\n\n\n\n\n\n\nModélisation d’un réseau dans un logiciel SIG (systèmes d’information géographique)\n\n\nIl est aussi possible de construire un réseau dans un logiciel SIG (QGIS et ArcGIS Pro par exemple). Pour une description détaillée de la construction d’un réseau selon différents modes de transport (voiture, marche, vélo et transport en commun) dans un SIG, consultez l’article d’Apparicio et al. (2017)."
  },
  {
    "objectID": "05-AnalyseReseau.html#sec-052",
    "href": "05-AnalyseReseau.html#sec-052",
    "title": "5  Mesures d’accessibilité spatiale selon différents modes de transport",
    "section": "\n5.2 Construction d’un réseau avec R5R",
    "text": "5.2 Construction d’un réseau avec R5R\nPour construire un réseau pour différents modes de transport dans R, nous utilisons le package R5R (Pereira et al. 2021). Il existe d’autres packages, notamment opentripplanner (Morgan et al. 2019) qui a été largement utilisé ces dernières années. Étant plus rapide, R5R s’impose actuellement comme la solution la plus efficace pour calculer des trajets à travers un réseau de rues selon différents modes de transport.\n\n\n\n\n\nDocumentation du packageR5R\n\n\nNous vous conseillons vivement de lire la documentation de R5R sur le site de CRAN, notamment les nombreuses vignettes présentant des exemples d’analyses avec du code R très bien documenté.\n\n\n\n5.2.1 Extraction des données spatiales pour R5R\nPour construire un réseau multimode, R5R a besoin de trois fichiers qui doivent être localisés dans le même dossier (figure 5.6) :\n\nun fichier pbf (Protocolbuffer Binary Format) pour les données d’OpenStreetMap.\nun ou plusieurs fichiers GTFS (General Transit Feed Specification) pour les flux relatifs aux transports en commun.\nun fichier GeoTIFF d’élévation.\n\nNotez que ce dernier fichier est optionnel. Toutefois, pour calculer des trajets à pied ou à vélo, il est préférable de tenir compte de la pente, surtout dans une ville comme Sherbrooke!\n\n\nFigure 5.6: Trois types de données nécessaires pour modéliser un réseau dans R5R\n\n\n5.2.1.1 Extraction d’un fichier OpenStreetMap\nPour récupérer un fichier OpenStreetMap (OSM, format pbf), nous utilisons deux fonctions du package osmextract (Gilardi et Lovelace 2022) :\n\n\noe_match pour repérer le fichier OSM pour la région de l’Estrie.\n\noe_download pour télécharger le fichier OSM pour la région de l’Estrie.\n\n\nlibrary(osmextract)\n## Identification du fichier OSM (format pbf) pour l'Estrie\nEstrie = oe_match(place=\"Estrie\", provider = \"openstreetmap_fr\")\n## Téléchargement du fichier OSM (format pbf) pour l'Estrie\noe_download(\n  file_url = Estrie$url,\n  file_size = Estrie$file_size,\n  provider = \"openstreetmap_fr\",\n  download_directory = \"data/chap05/__TempOSM\")\n\nLe fichier OSM téléchargé plus haut couvre la région de l’Estrie. À notre connaissance, il n’existe pas de solution pour découper un fichier pbf dans R. Par conséquent, nous récupérons les coordonnées minimales et maximales de l’enveloppe d’une zone tampon de 5000 mètres autour de la couche de la ville de Sherbrooke. Puis, pour découper le fichier pbf, nous utilisons l’outil osmconvert64-0.8.8p.exe. Le code ci-dessous renvoie les quatre coordonnées de l’enveloppe.\n\nlibrary(sf)\nlibrary(tmap)\n## Importation du polygone pour la ville de Sherbrooke avec sf\nSherbrooke &lt;- st_read(dsn = \"data/chap05/AutresDonnees/Sherbrooke.shp\", quiet=TRUE)\n## Création d'une zone tampon de 5 km\nzone &lt;- st_buffer(Sherbrooke, dist = 5000)\n## Changement de la projection en 4326\nzone &lt;- st_transform(zone, 4326)\n## Création de l'enveloppe autour de la couche\nenveloppe = st_bbox(zone)\n## Visualisation des coordonnées minimales et maximales\ncat(paste0(\"Minimum longitude : \", round(enveloppe[1],4),\n           \"\\n Minimum latitude : \", round(enveloppe[2],4),\n           \"\\n Maximum longitude : \", round(enveloppe[3],4),\n           \"\\n Maximum latitude : \", round(enveloppe[4],4)\n    ))\n\nMinimum longitude : -72.1537\n Minimum latitude : 45.2683\n Maximum longitude : -71.7576\n Maximum latitude : 45.5554\n\n\nL’application Osmconvert est disponible pour Windows et Linux. Pour découper un fichier avec les coordonnées latitude/longitude minimales et maximales avec Osmconvert, nous écrivons une commande système et l’exécutons avec la fonction system. Pour utiliser le code ci-dessous, vous devez avoir préalablement téléchargé Osmconvert et connaitre son emplacement.\n\n## Préparation des chemins\npath_to_osm_convert &lt;- paste0(getwd(),'/data/chap05/__TempOSM/osmconvert64-0.8.8p.exe')\npath_to_big_osm &lt;- paste0(getwd(),'/data/chap05/__TempOSM/openstreetmap_fr_estrie-latest.osm.pbf')\npath_to_small_osm &lt;- paste0(getwd(),'/data/chap05/__TempOSM/openstreetmap_fr_estrie-latest.osm_01.pbf')\n## Écriture de la commande\ncommande &lt;- paste0('\"',path_to_osm_convert, '\" \"',\n                   path_to_big_osm, '\" -b=',paste0(enveloppe[1:4], collapse = ','), \n                   ' -o=\"',path_to_small_osm,'\"')\n\nUne fois que la commande est écrite, nous l’exécutons.\n\nsystem(commande)\n\nNotez qu’il est possible d’obtenir le même résultat avec l’application Osmosis. Il s’agit d’un outil rédigé en Java et qui dispose de fonctionnalités similaires à celles de Osmconvert. Puisqu’il est rédigé en Java, il peut être utilisé sur davantage de plateformes qu’Osmconvert.\n\n5.2.1.2 Construction d’un fichier GeoTIFF pour l’élévation\nPour créer un fichier GeoTIFF pour l’élévation, nous utilisons les modèles numériques de terrain (MNT) du ministère des Ressources naturelles et des Forêts.\n\nlibrary(terra)\n## Couche polygonale sf pour la ville de Sherbrooke\n# Shapefile pour les régions du Québec au 1/20000\n# https://www.donneesquebec.ca/recherche/dataset/decoupages-administratifs/resource/b368d470-71d6-40a2-8457-e4419de2f9c0\nSherbrooke &lt;- st_read(dsn = \"data/chap05/AutresDonnees/Sherbrooke.shp\", quiet=TRUE)\nSherbrooke &lt;- st_buffer(Sherbrooke, dist = 5000)\n## Feuillets pour les MNT au 1/20000\n## Importation du shapefile des feuillets\n## (https://www.donneesquebec.ca/recherche/dataset/modeles-numeriques-d-altitude-a-l-echelle-de-1-20-000/resource/2df157af-74cf-4b53-af9d-1d3ccee0d6e1)\nFeuillets &lt;- st_read(dsn = \"data/chap05/AutresDonnees/Index_MNT20k.shp\", quiet=TRUE)\n### Nous nous assurons que les deux couches ont la même projection, soit EPSG 4326\nSherbrooke &lt;- st_transform(Sherbrooke, st_crs(Feuillets))\n### Sélection des feuillets qui intersectent le polygone de l'Estrie\nRequeteSpatiale &lt;- st_intersects(Feuillets, Sherbrooke, sparse = FALSE)\nFeuillets$Intersect &lt;-  RequeteSpatiale[, 1]\nFeuilletsSherbrooke &lt;- subset(Feuillets,Intersect == TRUE)\n\n## Téléchargement des GRIDS\n### Création d'un dossier temporaire pour les MNT\ndir.create(paste0(\"data/chap05/AutresDonnees/MNT\"))\ngrids &lt;- FeuilletsSherbrooke$GRID\ni = 0\nfor (e in grids) {\n  i = i+1\n  # Téléchargement\n  Fichier &lt;- substr(e, 88, nchar(e))\n  Chemin &lt;-  \"data/chap05/AutresDonnees/MNT\"\n  CheminFichier &lt;- paste0(Chemin, \"/\", Fichier)\n  download.file(e, destfile = CheminFichier)\n  # Décompression du fichier zip\n  unzip(CheminFichier, exdir = Chemin)\n  # suppression du zip\n  unlink(CheminFichier)\n}\n\n## Importation des GRIDS avec le package Terra\nFichier1 &lt;- substr(grids, 88, 98)\nFichier2 &lt;- paste0(substr(tolower(Fichier1), 1, 7),\n                    substr(Fichier1, 9, 11))\nNomsFichiers &lt;- paste0(\"data/chap05/AutresDonnees/MNT/\",\n                        Fichier1, \"/\",\n                        Fichier2)\nrlist &lt;- list()\nfor(e in NomsFichiers) {\n print(e)\n rasterGrid &lt;- terra::rast(e)\n rlist[[length(rlist)+1]] &lt;- rasterGrid\n}\n## Création d'une mosaique avec les GRIDS\nrsrc &lt;- terra::sprc(rlist)\nMosaicSherbrooke &lt;- mosaic(rsrc)\nMosaicSherbrooke\n\n# Pour réduire la taille du fichier d’élévation, nous arrondissons les valeurs au mètre\nMosaicSherbrooke &lt;- round(MosaicSherbrooke)\n\n## Exporter en GeoTIFF\nterra::writeRaster(MosaicSherbrooke, \n                  \"data/chap05/_DataReseau/Elevation.tif\",\n                  filetype = \"GTiff\", \n                  datatype = 'INT2U',\n                  overwrite = TRUE)\n\n## Suppression des GRIDS\ndossier &lt;- \"data/chap05/AutresDonnees/MNT/\"\nf &lt;- list.files(dossier, include.dirs = FALSE, full.names = TRUE, recursive = TRUE)\nfile.remove(f)\nd &lt;- list.dirs(dossier)\nunlink(d, recursive = TRUE)\n\nLe fichier d’élévation ainsi construit est présenté à la figure 5.7.\n\nMosaicSherbrooke &lt;- terra::rast(\"data/chap05/_DataReseau/Elevation.tif\")\nterra::plot(MosaicSherbrooke)\n\n\n\nFigure 5.7: Modèle numérique d’élévation au 1/20000 pour la région de Sherbrooke\n\n\n\n\n\n\n\n\nPackage elevatr pour extraire des images d’élévation pour une région donnée\n\n\nPlus haut, nous avons utilisé des modèles numériques de terrain (MNT) du ministère des Ressources naturelles et des Forêts pour construire un fichier GeoTIFF pour l’élévation pour la ville de Sherbrooke (Québec). Pour d’autres régions du monde, vous pouvez aussi utiliser le package elevatr (Hollister et al. 2023) pour extraire une image d’élévation à partir de l’API OpenTopography.\nPour plus d’information, consultez cette vignette,\n\n\n\n5.2.1.3 Extraction et validation d’un fichier GTFS\nLe fichier GTFS pour la Société de Transport de Sherbrooke est disponible à l’adresse suivante : https://gtfs.sts.qc.ca:8443/gtfs/client/GTFS_clients.zip. Pour le télécharger, nous utilisons la fonction download.file.\n\nurl &lt;- \"https://gtfs.sts.qc.ca:8443/gtfs/client/GTFS_clients.zip\"\ndestfile &lt;- \"data/chap05/_DataReseau/GTFS_clients.zip\"\ndownload.file(url, destfile)\n\nPour s’assurer du bon fonctionnement d’un GTFS dans r5r, il faut préalablement valider la structure du fichier. Dans un premier temps, vous pouvez valider la structure générale de votre GTFS en utilisant l’outil en ligne gtfs-validator proposé par l’organisation MobilityData. Dans un second temps, il convient de s’assurer que les types de lignes utilisés font partie des types standards définis par Google, soit avec l’un des numéros suivants : 0, 1, 2, 3, 4, 5, 6, 7, 11, 12. Si votre GTFS contient des lignes de transport en commun provenant par exemple des types étendus, r5r renverra une erreur.\nNous vous proposons ci-dessous une fonction qui remplace les types problématiques dans un GTFS. Le plus simple est de les remplacer par un type bus (3), ce qui n’affectera pas les trajets estimés.\n\nlibrary(gtfstools)\n## Fonction\nclean_gtfs_types &lt;- function(gtfs_file, replace_by = 3){\n  # Lecture du GTFS\n  seed &lt;- gtfstools::read_gtfs(gtfs_file)\n  print(unique(seed$routes$route_type))\n  # Vérification des types de routes\n  seed$routes$route_type &lt;- ifelse(\n    seed$routes$route_type %in% c(0:7,11,12),\n    seed$routes$route_type,\n    replace_by\n  )\n  # Réécriture du fichier GTFS avec la modification\n  gtfstools::write_gtfs(seed, gtfs_file)\n}\n## Appel de la fonction\ndestfile &lt;- \"data/chap05/_DataReseau/GTFS_clients.zip\"\nclean_gtfs_types(destfile)\n\n[1] 3\n\n\nPour ce fichier, le seul type de ligne utilisé est le numéro 3, soit des lignes de bus.\n\n5.2.2 Construction du réseau avec R5R\n\n\n\n\n\nR5R et JDK Java et allocation de la mémoire\n\n\nLe package R5R utilise la version 21 de la JDK de Java (Java Development Kit). Vous devez préalablement la télécharger et l’installer sur votre ordinateur. Les deux lignes de code ci-dessous permettent de vérifier si elle est bien installée sur votre ordinateur.\n\n## Vérification que la JDK version 21 est bien installée\nrJava::.jinit()\nrJava::.jcall(\"java.lang.System\", \"S\", \"getProperty\", \"java.version\")\n\n[1] \"21.0.3\"\n\n\nIl est fortement conseillé d’augmenter la mémoire allouée au processus JAVA, surtout si vous souhaitez calculer des matrices origines-destinations de grande taille. Par exemple, la commande options(java.parameters = \"-Xmx2G\") permet d’augmenter la mémoire disponible pour JAVA à deux gigaoctets. Si votre ordinateur ne manque pas de mémoire vive (16, 32 ou 64 gigaoctets), n’hésitez pas à augmenter ce paramètre (par exemple, options(java.parameters = \"-Xmx8G\")).\n\n\nNous utilisons la fonction setup_r5 pour construire un réseau multimode à partir des trois fichiers :\n\nLe fichier OSM (openstreetmap_fr_sherbrooke.pbf).\nLe fichier GTFS pour la Société de Transport de Sherbrooke (GTFS_clients.zip).\nLe fichier d’élévation pour la région de Sherbrooke (Elevation.tif).\n\nNotez les paramètres suivants pour la fonction setup_r5 :\n\ndata_path pour définir le dossier dans lequel sont présents les trois fichiers.\nelevation = \"TOBLER\" pour utiliser la fonction d’impédance de Tobler pour la marche et le vélo qui prend en compte la pente.\noverwrite = FALSE pour ne pas écraser le réseau s’il est déjà construit. La construction d’un réseau peut être très longue dépendamment de la taille des trois fichiers (OSM, GTFS, élévation). Par conséquent, n’oubliez pas de spécifier cette option si votre réseau a déjà été construit.\n\n\nlibrary(r5r)\n## Allocation de la mémoire pour Java\noptions(java.parameters = \"-Xmx2G\")\n## Construction du réseau R5R\nr5r_core &lt;- setup_r5(data_path = \"data/chap05/_DataReseau/\",\n                     elevation = \"TOBLER\",\n                     verbose = FALSE, overwrite = FALSE)\n\nLa fonction setup_r5 a créé deux nouveaux fichiers (network.dat et network_settings.json) qui sont utilisés par le package r5r pour les analyses de réseau.\n\n## Liste des fichiers dans le dossier \nlist.files(\"data/chap05/_DataReseau/\")\n\n[1] \"Elevation.tif\"                          \n[2] \"GTFS_clients.zip\"                       \n[3] \"network.dat\"                            \n[4] \"network_settings.json\"                  \n[5] \"openstreetmap_fr_sherbrooke.pbf\"        \n[6] \"openstreetmap_fr_sherbrooke.pbf.mapdb\"  \n[7] \"openstreetmap_fr_sherbrooke.pbf.mapdb.p\"\n\n\n\n5.2.3 Calcul d’itinéraires avec R5R selon le mode de transport\nPour calculer un trajet, nous utilisons la fonction detailed_itineraries dont les paramètres sont décrits dans l’encadré ci-dessous.\n\n\n\n\n\nParamètres de la fonction detailed_itineraries\n\n\n\n\nr5r_core: le réseau créé avec la fonction setup_r5() décrite plus haut.\n\norigins: un point sf projeté en WGS84 ou un data.frame comprenant les colonnes id, lon et lat.\n\ndestinations: un point sf projeté en WGS84 ou un data.frame comprenant les colonnes id, lon et lat.\n\nmode: un vecteur de type caractères définissant les modes de transport dont les principaux sont :\n\n\n\"WALK\" pour la marche.\n\n\"BICYCLE\" pour le vélo.\n\n\"CAR\" pour l’automobile.\n\n\"c(\"WALK\",TRANSIT\") pour la marche et le transport en commun.\n\n\n\ndeparture_datetime: un objet POSIXct définissant la date et l’heure de départ à utiliser si vous souhaitez calculer un trajet en transport en commun.\n\nmax_walk_time=inf: un nombre entier définissant le temps de marche maximal en minutes pour chaque segment du trajet. La valeur par défaut est sans limite (Inf).\n\nmax_bike_time = Inf: un nombre entier définissant le temps maximal à vélo en minutes.\n\nmax_car_time = Inf: un nombre entier définissant le temps maximal en automobile en minutes.\n\nmax_trip_duration = 120: un nombre entier définissant le temps maximal du trajet qui est fixé à 120 minutes par défaut. Par conséquent, tout trajet d’une durée supérieure à ce seuil ne sera pas calculé.\n\nwalk_speed = 3.6: une valeur numérique définissant la vitesse moyenne de marche qui est fixée par défaut à 3,6 km/h. Ce seuil est très conservateur et pourrait être augmenté à 4,5 km/h.\n\nbike_speed = 12: une valeur numérique définissant la vitesse moyenne à vélo qui est fixée par défaut à 12 km/h. Ce seuil est aussi très conservateur et pourrait être augmenté à 15 ou 16 km/h.\n\nmax_lts = 2: un nombre entier de 1 à 4 qui indique le niveau de stress lié à la circulation que les cyclistes sont prêts à tolérer. Une valeur de 1 signifie que les cyclistes n’emprunteront que les rues les plus calmes, tandis qu’une valeur de 4 indique que les cyclistes peuvent emprunter n’importe quelle route.\n\n\nmax_lts = 1: tolérable pour les enfants.\n\nmax_lts = 2: tolérable pour la population adulte en général.\n\nmax_lts = 3: tolérable pour les cyclistes enthousiastes et confiants.\n\nmax_lts = 4: tolérable uniquement pour les cyclistes intrépides.\n\n\n\ndrop_geometry = FALSE: si ce paramètre est fixé à TRUE, la géométrie du trajet ne sera pas incluse.\n\n\n\n\n5.2.3.1 Calcul d’itinéraires selon les modes de transport actif\nDans un premier temps, nous calculons des trajets à vélo entre les deux localisations suivantes :\n\nLe point Pt.W situé à l’intersection de la rue Wellington et de la Côte de l’Acadie (45,38947; -71,88393).\nLe point Pt.D situé à l’intersection des rues Darche et Dorval (45,38353; -71,89169).\n\n\n## Création d'une couche sf avec les deux points\nPts &lt;- data.frame(id = c(\"Rue Wellington S.\", \"Rue Darche\"),\n                         lat = c( 45.38947,  45.38353),\n                         lon = c(-71.88393, -71.89169)\n                         )\nPts &lt;- st_as_sf(Pts, coords = c(\"lon\",\"lat\"), crs = 4326)\nPt.W &lt;- Pts[1,]\nPt.D &lt;- Pts[2,]\n\nAvec la fonction detailed_itineraries, les durées sont estimées à respectivement six et huit minutes (figure 5.8). Bien que le chemin emprunté soit le même, cet écart s’explique par la Côte de l’Acadie, soit l’un des tronçons de rue les plus pentus de la ville de Sherbrooke.\n\n## Trajets en vélo\nvelo.1 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                               origins = Pt.W,\n                               destinations = Pt.D,\n                               mode = \"BICYCLE\", # Vélo\n                               bike_speed = 12,\n                               shortest_path = FALSE,\n                               drop_geometry = FALSE)\nvelo.2 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                               origins = Pt.D,\n                               destinations = Pt.W,\n                               mode = \"BICYCLE\", # Vélo\n                               bike_speed = 12,\n                               shortest_path = FALSE,\n                               drop_geometry = FALSE)\nvelo.1\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -71.89191 ymin: 45.3835 xmax: -71.88387 ymax: 45.38955\nGeodetic CRS:  WGS 84\n            from_id from_lat  from_lon      to_id   to_lat    to_lon option\n1 Rue Wellington S. 45.38947 -71.88393 Rue Darche 45.38353 -71.89169      1\n  departure_time total_duration total_distance segment    mode segment_duration\n1       15:18:25            8.1           1215       1 BICYCLE              8.1\n  wait distance route                       geometry\n1    0     1215       LINESTRING (-71.88396 45.38...\n\nvelo.2\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -71.89191 ymin: 45.3835 xmax: -71.88387 ymax: 45.38955\nGeodetic CRS:  WGS 84\n     from_id from_lat  from_lon             to_id   to_lat    to_lon option\n1 Rue Darche 45.38353 -71.89169 Rue Wellington S. 45.38947 -71.88393      1\n  departure_time total_duration total_distance segment    mode segment_duration\n1       15:18:25            6.8           1215       1 BICYCLE              6.8\n  wait distance route                       geometry\n1    0     1215       LINESTRING (-71.89191 45.38...\n\n\nPour visualiser les trajets, nous utilisons le package tmap en mode view. Notez qu’un clic sur le trajet fait apparaître une fenêtre surgissante.\n\nlibrary(tmap)\n# Cartographie des trajets avec tmap\ntmap_mode(\"view\")\nCarte1 &lt;- tm_shape(velo.1)+\n  tm_lines(col=\"black\", lwd = 2, \n           popup.vars = c(\"mode\", \"from_id\", \"to_id\", \"segment_duration\", \"distance\"))+\n  tm_shape(Pt.W)+tm_dots(col=\"green\", size = .15)+\n  tm_shape(Pt.D)+tm_dots(col=\"red\", size = .15)\nCarte2 &lt;- tm_shape(velo.2)+\n  tm_lines(col=\"black\", lwd = 2,\n           popup.vars = c(\"mode\", \"from_id\", \"to_id\", \"segment_duration\", \"distance\"))+\n  tm_shape(Pt.D)+tm_dots(col=\"green\", size = .15)+\n  tm_shape(Pt.W)+tm_dots(col=\"red\", size = .15)\ntmap_arrange(Carte1, Carte2, ncol = 2)\n\n\n\nFigure 5.8: Trajets à vélo entre les deux destinations\n\nDans un second temps, nous calculons les trajets à pied avec les deux mêmes localisations qui sont estimées à 16 et 21 minutes (figure 5.9).\n\n\nFigure 5.9: Trajets à pied entre les deux destinations\n\n\n5.2.3.2 Calcul d’itinéraires en automobile\nNous calculons ici l’itinéraire entre le campus principal de l’Université de Sherbrooke et une localisation (45,4220308; -71,881828). Nous fixons alors mode = \"CAR\" pour la fonction detailed_itineraries. Les trajets sont respectivement estimés à 21,7 et 18,8 minutes à destination et au départ du campus principal.\n\nUDeS &lt;- data.frame(id = \"Campus principal\", lon = -71.929526, lat = 45.378017)\nUDeS &lt;- st_as_sf(UDeS, coords = c(\"lon\",\"lat\"), crs = 4326)\nPoint &lt;- data.frame(id = \"Départ\", lon = -71.881828, lat = 45.4220308)\nPoint &lt;- st_as_sf(Point, coords = c(\"lon\",\"lat\"), crs = 4326)\n\nAuto.Aller &lt;- detailed_itineraries(r5r_core = r5r_core,\n                                 origins = Point,\n                                 destinations = UDeS,\n                                 mode = \"CAR\", # Automobile\n                                 shortest_path = FALSE,\n                                 drop_geometry = FALSE)\nAuto.Retour &lt;- detailed_itineraries(r5r_core = r5r_core,\n                               origins = UDeS,\n                               destinations = Point,\n                               mode = \"CAR\", # Automobile\n                               shortest_path = FALSE,\n                               drop_geometry = FALSE)\nAuto.Aller\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -71.93373 ymin: 45.37722 xmax: -71.88129 ymax: 45.42365\nGeodetic CRS:  WGS 84\n  from_id from_lat  from_lon            to_id   to_lat    to_lon option\n1  Départ 45.42203 -71.88183 Campus principal 45.37802 -71.92953      1\n  departure_time total_duration total_distance segment mode segment_duration\n1       15:18:26             22          11352       1  CAR               22\n  wait distance route                       geometry\n1    0    11352       LINESTRING (-71.88129 45.42...\n\nAuto.Retour\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -71.93312 ymin: 45.37759 xmax: -71.87607 ymax: 45.42234\nGeodetic CRS:  WGS 84\n           from_id from_lat  from_lon  to_id   to_lat    to_lon option\n1 Campus principal 45.37802 -71.92953 Départ 45.42203 -71.88183      1\n  departure_time total_duration total_distance segment mode segment_duration\n1       15:18:26           19.2           9685       1  CAR             19.2\n  wait distance route                       geometry\n1    0     9685       LINESTRING (-71.92952 45.37...\n\n\nBien entendu, les deux trajets sont différents en raison des sens de circulation (figure 5.10).\n\n# Cartographie des trajets avec tmap\nCarte1 &lt;- tm_shape(Auto.Aller)+\n  tm_lines(col=\"black\", lwd = 2,\n           popup.vars = c(\"mode\", \"from_id\", \"to_id\", \"segment_duration\", \"distance\"))+\n  tm_shape(Point)+tm_dots(col=\"green\", size = .15)+\n  tm_shape(UDeS)+tm_dots(col=\"red\", size = .15)\nCarte2 &lt;- tm_shape(Auto.Retour)+\n  tm_lines(col=\"black\", lwd = 2,\n           popup.vars = c(\"mode\", \"from_id\", \"to_id\", \"segment_duration\", \"distance\"))+\n  tm_shape(Point)+tm_dots(col=\"red\", size = .15)+\n  tm_shape(UDeS)+tm_dots(col=\"green\", size = .15)\n# Figure avec les deux cartes\ntmap_arrange(Carte1, Carte2)\n\n\n\nFigure 5.10: Trajets en voiture entre les deux destinations\n\n\n5.2.3.3 Calcul d’itinéraires en transport en commun\nPour le calcul d’itinéraires en transport en commun, nous devons fixer une heure de départ et un temps de marche maximal pour chaque segment du trajet réalisé à pied, soit :\n\ndu domicile à l’arrêt de bus le plus proche;\nentre deux arrêts de bus de lignes différentes;\ndu dernier arrêt de bus à la destination finale.\n\nDans le code ci-dessous, nous fixons les heures de départ et d’arrivée à 8 h et à 18 h pour le 28 février 2024 et un temps de marche maximal de 20 minutes.\n\n### Définition de la journée et de l'heure de départ\ndateheure.matin &lt;- as.POSIXct(\"28-02-2024 08:05:00\",\n                              format = \"%d-%m-%Y %H:%M:%S\")\n\ndateheure.soir  &lt;- as.POSIXct(\"28-02-2024 18:00:00\",\n                              format = \"%d-%m-%Y %H:%M:%S\")\n### Définition du temps de marche maximal\nminutes_marches_max &lt;- 20\n\nToujours avec la fonction detailed_itineraries, nous modifions les paramètres comme suit :\n\nmode = c(\"WALK\", \"TRANSIT\") pour le transport en commun.\nwalk_speed = 4.5 pour une vitesse moyenne à la marche de 4,5 km/h.\ndeparture_datetime = dateheure.matin pour un départ le 28 février 2024 à 8 h.\ndeparture_datetime = dateheure.soir pour un départ le 28 février 2024 à 18 h.\nmax_walk_time = minutes_marches_max pour un temps maximal de marche de 20 minutes.\n\n\nTC.Aller &lt;- detailed_itineraries(r5r_core = r5r_core,\n                                   origins = Point,\n                                   destinations = UDeS,\n                                   mode = c(\"WALK\", \"TRANSIT\"),\n                                   max_walk_time = minutes_marches_max,\n                                   walk_speed = 4.5,\n                                   departure_datetime = dateheure.matin,\n                                   shortest_path = FALSE,\n                                   drop_geometry = FALSE)\nTC.Retour &lt;- detailed_itineraries(r5r_core = r5r_core,\n                                    origins = UDeS,\n                                    destinations = Point,\n                                    mode = c(\"WALK\", \"TRANSIT\"),\n                                    max_walk_time = minutes_marches_max,\n                                    walk_speed = 4.5,\n                                    departure_datetime = dateheure.soir,\n                                    shortest_path = FALSE,\n                                    drop_geometry = FALSE)\n\nPour l’option 1, la durée du trajet à 8 h vers l’Université de Sherbrooke est estimée à 50,8 minutes avec trois segments :\n\nUn premier segment de 8,6 minutes et 661 mètres à pied.\nUn second de 39,0 minutes et 10,862 km en autobus.\nUn troisième de 1,8 minutes et 127 mètres à pied.\n\n\nTC.Aller\n\nSimple feature collection with 0 features and 16 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n [1] from_id          from_lat         from_lon         to_id           \n [5] to_lat           to_lon           option           departure_time  \n [9] total_duration   total_distance   segment          mode            \n[13] segment_duration wait             distance         route           \n[17] geometry        \n&lt;0 lignes&gt; (ou 'row.names' de longueur nulle)\n\n\nPour l’option 1, la durée du trajet à 18 h au départ de l’Université de Sherbrooke est estimée à 53,5 minutes avec trois segments :\n\nUn premier segment de 1,3 minutes et 102 mètres à pied.\nUn second de 19,8 minutes et 7,060 km en autobus.\nUn troisième de 18,4 minutes et 1,183 km à pied.\n\n\nTC.Retour\n\nSimple feature collection with 0 features and 16 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n [1] from_id          from_lat         from_lon         to_id           \n [5] to_lat           to_lon           option           departure_time  \n [9] total_duration   total_distance   segment          mode            \n[13] segment_duration wait             distance         route           \n[17] geometry        \n&lt;0 lignes&gt; (ou 'row.names' de longueur nulle)\n\n\nLes deux trajets en transport en commun sont représentés à la figure 5.11.\n\ntmap_mode(\"view\")\nCarte1 &lt;- tm_shape(subset(TC.Aller, option == 1))+\n            tm_lines(col=\"mode\", lwd = 3,\n                     popup.vars = c(\"mode\", \"from_id\", \"to_id\", \n                                    \"segment_duration\", \"distance\",\n                                    \"total_duration\", \"total_distance\"))+\n  tm_shape(Point)+tm_dots(col=\"green\", size = .15)+\n  tm_shape(UDeS)+tm_dots(col=\"red\", size = .15)+\n  tm_view(view.legend.position = c(\"left\", \"top\"))\n\nCarte2 &lt;- tm_shape(subset(TC.Retour, option == 1))+\n              tm_lines(col=\"mode\", lwd = 3,\n                       popup.vars = c(\"mode\", \"from_id\", \"to_id\", \n                                      \"segment_duration\", \"distance\",\n                                      \"total_duration\", \"total_distance\"))+\n  tm_shape(Point)+tm_dots(col=\"red\", size = .15)+\n  tm_shape(UDeS)+tm_dots(col=\"green\", size = .15)+\n  tm_view(view.legend.position = c(\"left\", \"top\"))\n\ntmap_arrange(Carte1, Carte2, ncol = 2)\n\n\n\nFigure 5.11: Trajets en transport en commun entre les deux destinations\n\n\n5.2.4 Délimitation d’isochrones avec R5R selon le mode de transport\nLe fonction isochrone du package R5R permet de délimiter des zones de desserte selon la distance-temps et différents modes de transport. Ses paramètres sont d’ailleurs les mêmes que ceux de la fonction detailed_itineraries, à l’exception de :\n\nIl n’y a pas de paramètre destinations puisque l’isochrone est uniquement délimité à partir de points d’origines (origins).\nLe paramètre cutoffs = c(0, 15, 30) permet de définir différentes isochrones en minutes.\nLe paramètre sample_size est utilisé pour tracer les isochrones. Variant de 0,2 à 1, sa valeur par défaut de 0,8 signifie que 80 % des nœuds du réseau de transport sont utilisés pour tracer l’isochrone. Plus cette valeur est proche de 1, plus l’isochrone est précise, mais plus sa vitesse de calcul est longue.\n\nDans l’exemple ci-dessous, nous calculons plusieurs isochrones à partir du campus principal de l’Université de Sherbrooke en fonction du mode transport (figure 5.12) :\n\nTrois isochrones de 10, 20 et 30 minutes à pied.\nTrois isochrones de 10, 20 et 30 minutes en vélo.\nTrois isochrones de 5, 10 et 20 minutes en automobile.\nDeux isochrone de 20 et 40 minutes en transport en commun le 28 février 2024 à 18 h avec un durée maximale de marche de 15 minutes.\n\n\n## Point pour l'université de Sherbrooke\nUDeS &lt;- data.frame(id = \"Campus principal\", lon = -71.929526, lat = 45.378017)\nUDeS &lt;- st_as_sf(UDeS, coords = c(\"lon\",\"lat\"), crs = 4326)\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\n## Trois isochrones à pied de 10, 20 et 30 minutes\nIsochrome.marche &lt;- isochrone(r5r_core,\n                      origins = UDeS,\n                      mode = \"WALK\",\n                      cutoffs = c(10, 20, 30),\n                      sample_size = .8,\n                      time_window = 120,\n                      progress = FALSE)\n\nCarte.Marche &lt;- tm_shape(Isochrome.marche)+\n                    tm_fill(col=\"isochrone\", \n                            alpha = .4, \n                            breaks = c(0, 10, 20, 30),\n                            title =\"Marche\",\n                            legend.format = list(text.separator = \"à\"))+\n                    tm_shape(UDeS)+tm_dots(col=\"darkred\", size = .25)\n\n## Trois isochrones à vélo de 10, 20 et 30 minutes\nIsochrome.velo &lt;- isochrone(r5r_core,\n                      origins = UDeS,\n                      mode = \"BICYCLE\",\n                      cutoffs = c(10, 20, 30),\n                      sample_size = .8,\n                      time_window = 120,\n                      progress = FALSE)\n\nCarto.Velo &lt;- tm_shape(Isochrome.velo)+\n                    tm_fill(col=\"isochrone\", \n                            alpha = .4, \n                            breaks = c(0, 10, 20, 30),\n                            title =\"Vélo\",\n                            legend.format = list(text.separator = \"à\"))+\n                    tm_shape(UDeS)+tm_dots(col=\"darkred\", size = .25)\n\n## Trois isochrones en auto de 5, 10 et 20 minutes\nIsochrome.auto &lt;- isochrone(r5r_core,\n                      origins = UDeS,\n                      mode = \"CAR\",\n                      cutoffs = c(5, 10, 20),\n                      sample_size = 1,\n                      time_window = 120,\n                      progress = FALSE)\n\nCarto.Auto &lt;- tm_shape(Isochrome.auto)+\n                    tm_fill(col=\"isochrone\", \n                            alpha = .4, \n                            breaks = c(0, 5, 10, 20),\n                            title =\"Automobile\",\n                            legend.format = list(text.separator = \"à\"))+\n                    tm_shape(UDeS)+tm_dots(col=\"darkred\", size = .25)\n\n## Deux isochrones en transport en commun de 20, 40 et 60 minutes\ndateheure.soir  &lt;- as.POSIXct(\"28-02-2024 18:00:00\",\n                              format = \"%d-%m-%Y %H:%M:%S\")\nIsochrome.tc &lt;- isochrone(r5r_core,\n                      origins = UDeS,\n                      mode = c(\"WALK\", \"TRANSIT\"),\n                      max_walk_time = 15,\n                      departure_datetime = dateheure.soir,\n                      cutoffs = c(20, 40),\n                      sample_size = 1,\n                      time_window = 120,\n                      progress = FALSE)\n\nCarto.TC &lt;- tm_shape(Isochrome.tc)+\n                    tm_fill(col=\"isochrone\", \n                            alpha = .4, \n                            breaks = c(0, 20, 40),\n                            title =\"Transport en commun\",\n                            legend.format = list(text.separator = \"à\"))+\n                    tm_shape(UDeS)+tm_dots(col=\"darkred\", size = .25)\n## Figure avec les quatre cartes\ntmap_arrange(Carte.Marche, Carto.Velo,\n             Carto.Auto, Carto.TC,\n             ncol = 2, nrow = 2)\n\n\n\nFigure 5.12: Isochrones selon les quatre modes de transport\n\n\n5.2.5 Calcul de matrices OD selon différents modes de transport\nPour calculer des matrices origines-destinations selon différents modes de transport, nous utilisons la fonction travel_time_matrix dont les paramètres sont quasi les mêmes que la detailed_itineraries (section 5.2.3). Dans le code ci-dessous, nous importons 284 adresses tirées aléatoirement et les supermarchés présents sur le territoire de la ville de Sherbrooke.\n\n## Importation des couches\nAdresses &lt;- st_read(dsn = \"data/Chap05/AutresDonnees/Commerces.gpkg\",\n                        layer = \"AdressesAleatoires\", quiet = TRUE)\nsupermarches &lt;- st_read(dsn = \"data/Chap05/AutresDonnees/Commerces.gpkg\",\n                        layer = \"supermarche\", quiet = TRUE)\n## Nombre de distances à calculer\nnO = nrow(Adresses)\nnD = nrow(supermarches)\nNOD = nO * nD\ncat(\"Origines (O) :\", nO, \"adresses\",\n    \"\\n Destinations (D) :\", nD, \"supermarchés\",\n    \"\\n Distances OD à calculer = \", NOD)\n\nOrigines (O) : 184 adresses \n Destinations (D) : 27 supermarchés \n Distances OD à calculer =  4968\n\n## Origines et destinations\nOrigines &lt;- Adresses\nOrigines$id &lt;- as.character(Adresses$id)\nOrigines$lat &lt;- st_coordinates(Adresses)[,2]\nOrigines$lon &lt;- st_coordinates(Adresses)[,1]\nDestinations &lt;- supermarches\nDestinations$id &lt;- supermarches$osm_id\nDestinations$lat &lt;- st_coordinates(supermarches)[,2]\nDestinations$lon &lt;- st_coordinates(supermarches)[,1]\nnames(Destinations)[1] &lt;- \"id\"\n\nPar la suite, nous calculons les différentes matrices OD :\n\nmatriceOD.Auto avec mode = \"CAR\".\nmatriceOD.Marche avec mode = \"WALK\", walk_speed = 4.5 et max_trip_duration = 60. La durée du trajet est limitée à 60 minutes avec une vitesse moyenne de 4,5 km/h.\nmatriceOD.Velo avec mode = \"BICYCLE\", bike_speed = 15 et max_trip_duration = 60. La durée du trajet est limitée à 60 minutes avec une vitesse moyenne de 15 km/h.\nmatriceOD.TC avec mode = \"c(\"WALK\", \"TRANSIT\")\", walk_speed = 4.5, max_walk_time = 30, max_trip_duration = 120 et departure_datetime = dateheure.soir. La durée du trajet est limitée à 60 minutes avec une vitesse moyenne de marche de 4,5 km/h et une durée maximale de 30 minutes pour chaque segment à la marche. L’heure de départ a été fixée comme suit : dateheure.soir  &lt;- as.POSIXct(\"04-05-2023 18:00:00\", format = \"%d-%m-%Y %H:%M:%S\").\n\n\n## Matrice OD en voiture\nt1 &lt;-Sys.time()\nmatriceOD.Auto &lt;- travel_time_matrix(r5r_core = r5r_core,\n                                     origins = Origines,\n                                     destinations = Destinations,\n                                     mode = \"CAR\")\nt2 &lt;-Sys.time()\nduree.auto = as.numeric(difftime(t2, t1), units = \"secs\")\n## Matrice OD à la marche\nt1 &lt;-Sys.time()\nmatriceOD.Marche &lt;- travel_time_matrix(r5r_core = r5r_core,\n                                       origins = Origines,\n                                       destinations = Destinations,\n                                       mode = \"WALK\",\n                                       walk_speed = 4.5,  # valeur par défaut 3.6\n                                       max_trip_duration = 60, # 1 heure de marche maximum\n                                       max_walk_time = Inf)\nt2 &lt;-Sys.time()\nduree.marche = as.numeric(difftime(t2, t1), units = \"secs\")\n## Matrice OD en vélo\nt1 &lt;-Sys.time()\nmatriceOD.Velo &lt;- travel_time_matrix(r5r_core = r5r_core,\n                                     origins = Origines,\n                                     destinations = Destinations,\n                                     mode = \"BICYCLE\",\n                                     bike_speed = 15,\n                                     max_trip_duration = 60,\n                                     max_bike_time = Inf)\nt2 &lt;-Sys.time()\nduree.velo = as.numeric(difftime(t2, t1), units = \"secs\")\n## Matrice OD en transport en commun\ndateheure.soir  &lt;- as.POSIXct(\"04-05-2023 18:00:00\",\n                              format = \"%d-%m-%Y %H:%M:%S\")\nt1 &lt;-Sys.time()\nmatriceOD.TC &lt;- travel_time_matrix(r5r_core = r5r_core,\n                                   origins = Origines,\n                                   destinations = Destinations,\n                                   mode = c(\"WALK\", \"TRANSIT\"),\n                                   walk_speed = 4.5,\n                                   max_walk_time = 30,\n                                   max_trip_duration = 120,\n                                   departure_datetime = dateheure.soir)\nt2 &lt;-Sys.time()\nduree.tc = as.numeric(difftime(t2, t1), units = \"secs\")\n\nLes temps de calcul des différentes matrices sont reportés ci-dessous.\n\ncat(\"Temps de calcul des matrices :\", \n     \"\\n Voiture : \", round(duree.auto,2), \"secondes\",\n     \"\\n Marche : \", round(duree.marche,2), \"secondes\",\n     \"\\n Vélo : \", round(duree.velo,2), \"secondes\",\n     \"\\n Transport en commun : \", round(duree.tc,2), \"secondes\")\n\nTemps de calcul des matrices : \n Voiture :  8.62 secondes \n Marche :  0.33 secondes \n Vélo :  2.88 secondes \n Transport en commun :  0.26 secondes\n\n\nUne fois les matrices obtenues, nous les enregistrons dans un fichier Rdata.\n\nsave(matriceOD.Auto, matriceOD.Marche, \n     matriceOD.Velo, matriceOD.TC,\n     file=\"data/chap05/Resultats/MatricesOD.Rdata\")\n\n\n\n\n\n\nLibération de la mémoire allouée à JAVA\n\n\nUne fois les calculs avec R5R terminés, il convient de détruire l’objet r5r_core et d’arrêter le processus JAVA avec les deux lignes de code ci-dessous.\n\nr5r::stop_r5(r5r_core)\nrJava::.jgc(R.gc = TRUE)\n\n\n\nÀ partir de ces matrices, nous extrayons la valeur minimale pour chacune des adresses pour les quatre modes de transport. Puis, nous opérons une jointure attributaire avec la couche des adresses aléatoires avec la fonction merge.\n\n## Création d'un vecteur pour la distance au supermarché le plus proche \nSupermarchePlusProche.Auto &lt;- aggregate(travel_time_p50 ~ from_id, matriceOD.Auto, min)\nSupermarchePlusProche.Pied &lt;- aggregate(travel_time_p50 ~ from_id, matriceOD.Marche, min)\nSupermarchePlusProche.Velo &lt;- aggregate(travel_time_p50 ~ from_id, matriceOD.Velo, min)\nSupermarchePlusProche.tc   &lt;- aggregate(travel_time_p50 ~ from_id, matriceOD.TC, min)\n## Changement des noms des champs\nnames(SupermarchePlusProche.Auto) &lt;- c(\"id\", \"SupPlusProcheAuto\")\nnames(SupermarchePlusProche.Pied) &lt;- c(\"id\", \"SupPlusProchePied\")\nnames(SupermarchePlusProche.Velo) &lt;- c(\"id\", \"SupPlusProcheVelo\")\nnames(SupermarchePlusProche.tc)   &lt;- c(\"id\", \"SupPlusProcheTC\")\n## Jointure avec la couche des adresses\nAdresses &lt;- merge(Adresses, SupermarchePlusProche.Auto, by =\"id\", all.x=TRUE)\nAdresses &lt;- merge(Adresses, SupermarchePlusProche.Pied, by =\"id\", all.x=TRUE)\nAdresses &lt;- merge(Adresses, SupermarchePlusProche.Velo, by =\"id\", all.x=TRUE)\nAdresses &lt;- merge(Adresses, SupermarchePlusProche.tc, by =\"id\", all.x=TRUE)\n\nFinalement, nous utilisons le package tmap pour cartographier les résultats et réaliser une figure avec la fonction tmap_arrange.\n\n## Importation des arrondissements de la ville de Sherbrooke\narrondissements &lt;- st_read(dsn = \"data/Chap05/AutresDonnees/Arrondissements.shp\", \n                           quiet=TRUE)\n## Construction des cartes\ntmap_mode(\"plot\")\nmax.auto &lt;- max(Adresses$SupPlusProcheAuto,na.rm=TRUE)\nCarte1 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(Adresses)+\n            tm_dots(col=\"SupPlusProcheAuto\", \n                    border.lwd = 1,\n                    style = \"fixed\",\n                    breaks = c(0,5,10,max.auto),\n                    palette=\"YlOrRd\",\n                    size = .2, \n                    legend.format = list(text.separator = \"à\"),\n                    title=\"Voiture\")+\n          tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_layout(legend.format = list(text.separator = \"à\"),\n                    frame = FALSE, legend.outside = TRUE)\n\nmax.pied &lt;- max(Adresses$SupPlusProchePied,na.rm=TRUE)\nCarte2 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(Adresses)+\n            tm_dots(col=\"SupPlusProchePied\", \n                    border.lwd = 1,\n                    style = \"fixed\",\n                    breaks = c(0,5,10,15,20,30,45, max.pied),\n                    palette=\"YlOrRd\",\n                    size = .2, \n                    title=\"Marche\",\n                    textNA = \"Plus de 60\")+\n          tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_layout(legend.format = list(text.separator = \"à\"),\n                    frame = FALSE, legend.outside = TRUE)\n\nmax.velo &lt;- max(Adresses$SupPlusProcheVelo,na.rm=TRUE)\nCarte3 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(Adresses)+\n            tm_dots(col=\"SupPlusProcheVelo\", \n                    border.lwd = 1,\n                    style = \"fixed\",\n                    breaks = c(0,5,10,20,30,45,max.velo),\n                    palette=\"YlOrRd\",\n                    size = .2, \n                    title=\"Vélo\",\n                    textNA = \"Plus de 60\")+\n          tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_layout(legend.format = list(text.separator = \"à\"),\n                    frame = FALSE, legend.outside = TRUE)\n\nmax.tc &lt;- max(Adresses$SupPlusProcheTC,na.rm=TRUE)\nCarte4 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(Adresses)+\n            tm_dots(col=\"SupPlusProcheTC\", \n                    border.lwd = 1,\n                    style = \"fixed\",\n                    breaks = c(0,10,15,20,25,max.tc),\n                    palette=\"YlOrRd\",\n                    size = .2, \n                    title=\"Bus\",\n                    textNA = \"Plus de 60\")+\n          tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_layout(legend.format = list(text.separator = \"à\"),\n                    frame = FALSE, legend.outside = TRUE)\n\n## Figure avec les quatre cartes\ntmap_arrange(Carte1, Carte2, Carte3, Carte4)\n\n\n\nFigure 5.13: Supermarché le plus proche en minutes selon le mode de transport"
  },
  {
    "objectID": "05-AnalyseReseau.html#sec-053",
    "href": "05-AnalyseReseau.html#sec-053",
    "title": "5  Mesures d’accessibilité spatiale selon différents modes de transport",
    "section": "\n5.3 Mesures d’accessibilité",
    "text": "5.3 Mesures d’accessibilité\n\n5.3.1 Notion d’accessibilité\nDans un article fondateur intitulé The concept of access: definition and relationship to consumer satisfaction, Roy Penchansky et William Thomas (1981) ont identifié cinq dimensions fondamentales au concept d’accessibilité aux services de santé :\n\nL’accessibilité spatiale (accessibility) renvoie à la proximité géographique du service par rapport à la population.\nLa disponibilité (availability) renvoie à la quantité et aux types de services offerts selon les besoins des individus.\nL’organisation (accommodation) renvoie au fonctionnement du service (horaires, délais d’attente, prises de rendez-vous, etc.).\nL’accessibilité financière (affordability) renvoie aux coûts du service qui peuvent constituer une barrière financière pour les personnes défavorisées.\nL’accessibilité socioculturelle (acceptability) renvoie à l’acceptation et à l’adaptation des services aux différences sociales, culturelles ou linguistiques des personnes.\n\n\n\n\n\n\nLes cinq dimensions de l’accessibilité et le type de service analysé\n\n\nL’importance accordée à chacune des cinq dimensions identifiées par Roy Penchansky et William Thomas (1981) varie en fonction du type de service à l’étude. Prenons l’exemple des parcs urbains :\n\nL’accessibilité spatiale, soit la proximité géographique est sans aucun doute une dimension très importante.\nLa disponibilité (availability) renvoie à différents équipements (aires de jeu pour enfants, terrains de sports, etc.) présents dans le parc.\nLa dimension de l’organisation (accommodation) risque d’être moins importante puisque les heures d’ouverture et les modalités de réservation de certains types de terrain de sport (comme un terrain de tennis) ne varient habituellement pas d’un parc à l’autre au sein d’une même ville.\nLa dimension de l’accessibilité financière (affordability) risque aussi d’être moins importante puisque l’accès au parc et à ses équipements est gratuit, à l’exception de certains terrains de sport très spécialisés.\nL’accessibilité socioculturelle (acceptability) peut être une dimension très importante et renvoie à l’acceptation des différences sociales, générationnelles et ethnoculturelles des personnes utilisatrices des parcs.\n\n\n\nPlus récemment, la notion d’accessibilité à un service a été définie selon deux dimensions, soit réelle (ou révélée) ou potentielle et spatiale ou aspatiale (Guagliardo 2004; Luo et Wang 2003; Khan 1992) :\n\nL’accessibilité réelle renvoie à l’utilisation effective des services tandis que l’accessibilité potentielle renvoie à leur utilisation probable.\nL’accessibilité spatiale renvoie à l’importance de la séparation spatiale entre l’offre et la demande de services en tant que barrière ou facilitateur, et l’accessibilité aspatiale (dimensions financière, socioculturelle, organisationnelle) se concentre sur les barrières ou facilitateurs non géographiques (Luo et Wang 2003; Ngui et Apparicio 2011).\n\nPar conséquent, la notion d’accessibilité aux services de santé englobe quatre catégories principales : l’accessibilité spatiale réelle, l’accessibilité aspatiale réelle, l’accessibilité spatiale potentielle et l’accessibilité aspatiale potentielle (Khan 1992). Par exemple, si nous questionnons un groupe de personnes sur la localisation des parcs qu’elles fréquentent habituellement, nous pourrions dresser un portrait sur leur accessibilité spatiale réelle aux parcs. Par contre, si nous calculons le nombre d’hectares de parcs présents dans un rayon de 20 minutes de marche autour de leur domicile, nous pourrions évaluer leur accessibilité spatiale potentielle aux parcs.\n\n5.3.2 Accessibilité spatiale potentielle\nDans le cadre de cette section, nous abordons l’accessibilité spatiale potentielle qui suppose de paramétrer quatre éléments : l’unité spatiale de référence, la méthode d’agrégation, la ou les mesures d’accessibilité et le type de distance (Apparicio et al. 2017).\n\n5.3.2.1 Unité spatiale de référence\nL’entité spatiale de référence correspond aux entités spatiales pour lesquelles l’accessibilité sera évaluée et cartographiée qui pourrait être :\n\nLes centroïdes des bâtiments résidentiels d’une ville donnée.\nDes entités polygonales représentant des zones résidentielles comme des aires de diffusion (comprenant de 400 à 700 habitants) ou des secteurs de recensement (de 2500 à 8000 habitants).\n\nL’aire de diffusion et surtout le secteur de recensement sont souvent choisis puisqu’une panoplie de variables socioéconomiques, sociodémographiques et relatives au logement sont rattachées à ces entités spatiales pour les différents recensements de Statistique Canada. La sélection de ces entités spatiales (aire de diffusion ou secteur de recensement) permet alors d’évaluer les relations entre les mesures d’accessibilité et les variables socioéconomiques ou sociodémographiques. Néanmoins, cela nécessite de recourir à méthodes d’agrégation afin de limiter les erreurs dans la mesure de l’accessibilité spatiale potentielle (Apparicio et al. 2017; Hewko, Smoyer-Tomic et Hodgson 2002).\n\n5.3.2.2 Méthodes d’agrégation\nDans un article méthodologique sur la comparaison des approches pour évaluer l’accessibilité spatiale potentielle, Apparicio et al. (2017) ont répertorié quatre principales méthodes d’agrégation pour évaluer une mesure d’accessibilité pour les secteurs de recensement. Ces approches, de la moins à la plus précise, sont les suivantes :\n\nLa première approche consiste à calculer la distance entre le centroïde du secteur de recensement et le service (figure 5.14, a). Plus la taille du secteur de recensement est grande, plus l’erreur d’agrégation (l’imprécision de la mesure d’accessibilité) risque d’être importante puisqu’elle ne tient pas compte de la distribution spatiale de la population à l’intérieur du secteur de recensement. Autrement dit, cette approche revient à supposer que toute la population réside en son centroïde.\nLa seconde approche consiste à calculer la distance entre les services et les centroïdes d’entités spatiales entièrement incluses dans les secteurs de recensement, puis à calculer la moyenne de ces distances pondérée par la population totale de chaque entité spatiale. Cette approche est réalisée avec les aires de diffusion et les îlots inclus dans les secteurs de recensement (figure 5.14, b et c). Bien entendu, les résultats sont plus précis avec les îlots de diffusion que les aires de diffusion puisqu’ils sont de taille plus réduite.\nLa troisième approche consiste à ajuster la localisation des centroïdes des îlots en ne retenant que la partie résidentielle avec une carte d’occupation du sol (figure 5.14, d).\nFinalement, la quatrième approche consiste à utiliser le rôle d’évaluation foncière. Nous calculons alors les distances entre chaque unité d’évaluation foncière et les services, puis la moyenne pondérée de ces distances par le nombre de logements. Cette approche est sans aucun doute la plus précise, mais elle est bien plus chronophage. En effet, à la figure 5.14, nous avons respectivement 4 secteurs de recensement (a), 23 aires de diffusion (b), 69 îlots (c et d) et 3497 unités d’évaluation foncière (e).\n\n\n\nFigure 5.14: Méthodes d’agrégation et erreurs potentielles\n\n\n5.3.2.3 Mesures d’accessibilité\nDifférentes mesures renvoyant à différentes conceptualisations de l’accessibilité peuvent être utilisées pour évaluer l’accessibilité spatiale potentielle; les principales sont reportées au tableau 5.1. Pour une description détaillée de ces mesures et de leurs formules, consultez l’article d’Apparicio et al. (2017).\n\n\n\n\nTableau 5.1: Liste des formats avec le package sf (st_drivers)\n\n\n\n\n\nConceptualisation\nMesures d’accessibilité\n\n\n\nProximité immédiate\nDistance entre l’origine et le service le plus proche\n\n\nOffre de services dans l’environnement immédiat\nNombre de services présents à moins de n mètres ou minutes\n\n\nCoût moyen pour atteindre tous les services\nDistance moyenne entre une origine et tous les services\n\n\nCoût moyen pour atteindre toutes les n destinations\nDistance moyenne entre une origine et n services\n\n\nAccessibilité en fonction de l’offre et la demande\nModèles gravitaires et méthodes two-step floating catchment area (2SFCA)\n\n\n\n\n\n\nSource : Apparicio et al. (2017).\nPour poser un diagnostic d’accessibilité spatiale potentielle à un service pour un territoire donné, plusieurs chercheuses et chercheurs recommandent d’utiliser plusieurs mesures d’accessibilité.\nPar exemple, dans une étude sur les déserts alimentaires à Montréal, Apparicio et al. (2007) utilisent trois mesures d’accessibilité : la distance au supermarché le plus proche (proximité immédiate), le nombre de supermarchés dans un rayon de 1000 mètres (offre dans l’environnement immédiat) et la distance moyenne aux trois supermarchés d’enseignes différentes (diversité en termes d’offre et de prix) à travers le réseau de rues.\nDans une autre étude portant sur l’accessibilité spatiale potentielle aux parcs urbains à Montréal, Jepson et al. (2022) utilisent deux mesures d’accessibilité : la distance au parc le plus proche (proximité immédiate) et la mesure E2FCA (congestion potentielle en fonction de l’offre et la demande) calculées pour les aires de diffusion de la Communauté métropolitaine de Montréal (figure 5.15). Concernant la proximité immédiate, le niveau d’accessibilité est bien élevé sur l’île de Montréal et inversement, plus faible à Laval et dans la Rive-Nord et la Rive-Sud (figure 5.15, a). En effet, la quasi-totalité des aires de diffusion de l’île de Montréal a un parc à moins de 200 mètres de marche. Concernant la congestion potentielle des parcs, le portrait est tout autre : le niveau de congestion potentielle est faible dans les zones suburbaines (Laval et les deux Rives) tandis qu’il est élevé dans les quartiers centraux de l’île de Montréal (figure 5.15, b). Autrement dit, les habitants des quartiers centraux de la ville de Montréal vivent plus près d’un parc, mais ce dernier est potentiellement plus congestionné. Or, une surutilisation des parcs peut entraîner une dégradation accélérée des équipements (aires de jeu, terrains de sports, etc.), voire décourager certaines personnes à visiter un parc durant les périodes plus achalandées.\n\n\nFigure 5.15: Deux mesures d’accessibilité spatiale potentielle aux parcs, aires de diffusion de la Communauté métropolitaine de Montréal, 2016\n\n\n\n\n\n\nLe 2SFCA et ses fonctions de pondération\n\n\nLes mesures appartenant à la famille des Two Step Floating Catchement Area (TSFCA) permettent d’évaluer l’accessibilité à des services en tenant comptant à la fois de l’offre (taille du service, par exemple le nombre de lits dans un hôpital) et de la demande (population résidant à la proximité des services).\nTel le nom l’indique, le calcul des mesures du TSFCA comprend deux étapes. Si nous reprenons l’exemple des parcs, la première étape consiste à calculer un ratio \\(R_j\\) pour chaque parc, indiquant le nombre d’hectares de parcs pour 1000 personnes résidant dans un rayon de n mètres ou minutes du parc :\n\\[\nR_{j} = \\frac{S_j}{\\sum_{j \\in \\{ d_{ij} \\leq d_0\\} }{P_i \\times W(d_{ij}) }}\n\\tag{5.1}\\]\navec :\n\n\n\\(j\\), un parc.\n\n\\(S_j\\) la capacité du parc \\(j\\) (ici, la superficie en hectares).\n\n\\(i\\), une entité géographique (ici, une aire de diffusion).\n\n\\(d_{ij}\\), la distance entre l’entité géographique \\(i\\) et le parc \\(j\\).\n\n\\(d_0\\), le seuil de distance maximale (par exemple un kilomètre ou trente minutes).\n\n\\(P_i\\), la population totale résidant dans \\(i\\). Cette population est habituellement exprimée en milliers d’habitants, soit \\(P_i/1000\\).\n\n\\(W\\), une fonction de pondération permettant de contrôler le fait qu’un parc plus éloigné est moins attractif et contribue moins à la valeur finale de la mesure d’accessibilité.\n\nEn résumé, \\(R_j\\) est le ratio entre la superficie du parc et la population ayant accès au parc dans un rayon n mètres ou minutes (\\(d_0\\)).\nLa deuxième étape consiste à calculer la somme pour chaque aire de diffusion \\(i\\) de la disponibilité des parcs à proximité.\n\\[\nA_{i} = \\sum^{m}_{j=1}{R_j \\times W(d_{ji}) }\n\\tag{5.2}\\]\nNous obtenons au final \\(A_i\\), soit pour chaque aire de diffusion le nombre total d’hectares de parcs disponible pour 1000 habitants.\nNotez que la fonction \\(W\\) joue un rôle très important dans la formulation du 2SFCA. Elle peut prendre plusieurs formes, la plus simple étant une fonction binaire donnant un poids de 1 aux parcs situés en-dessous d’une certaine distance. Par exemple, la fonction R ci-dessous donne un poids de 1 à des parcs situés à moins de 500 mètres et 0 au-delà.\n\nw_binaire &lt;- function(d){ifelse(d &lt;= 500, 1, 0)}\n\nLe problème de cette formulation est qu’elle implique que l’accessibilité au parc est la même que l’on habite à 25 ou 500 mètres du parc. Il a donc été proposé d’améliorer la méthode en ajoutant une fonction de pondération par palier qui accorde un poids différent selon des seuils de distances prédéfinis. Les fonctions R ci-dessous font ainsi varier le poids dans trois catégories de distance selon une pondération appelée slow decay et fast decay (Luo et Qi 2009) et des paliers de 150, 300 et 500 mètres.\n\nw_slow_decay &lt;- function(d){\n  w &lt;- case_when(\n    d &lt;= 150 ~ 1,\n    d &gt; 150 & d &lt;= 300 ~ 0.68,\n    d &gt; 300 & d &lt;= 500 ~ 0.22,\n    d &gt; 500 ~ 0\n  )\n  return(w)\n}\n\nw_fast_decay &lt;- function(d){\n  w &lt;- case_when(\n    d &lt;= 150 ~ 1,\n    d &gt; 150 & d &lt;= 300 ~ 0.42,\n    d &gt; 300 & d &lt;= 500 ~ 0.09,\n    d &gt; 500 ~ 0\n  )\n  return(w)\n}\n\nFinalement, il a aussi été proposé d’utiliser des fonctions de pondération continues et décroissantes. Les fonctions R ci-dessous illustrent une fonction incluant un effet de palier tel que proposé par McGrail (2012) et une fonction logistique décroissante.\n\n# palier à 125 mètres, maximum à 500 mètres\nW_cont_palier &lt;- function(x){\n  w &lt;- ifelse(x &lt; 125, 1,((500 - x) / (500 - 125))**1.5)\n  w[x  &gt; 500] &lt;- 0\n  return(w)\n}\n\nlogit_fun &lt;- function(x,a,b){\n  (1 + exp(-(a*pi)/(b*sqrt(3)))) / (1 + exp(((x-a)*pi)/(b*sqrt(3))))\n}\nw_logit1 &lt;- function(x){logit_fun(x, 250, 80)}\nw_logit2 &lt;- function(x){logit_fun(x, 250, 40)}\n\nConcernant la fonction logistique, elle comporte deux paramètre, \\(\\alpha\\) et \\(\\beta\\). Le premier paramètre permet de contrôler la distance à laquelle la pondération atteindra la valeur de 0,5. Le second contrôle la vitesse de décroissance de la courbe. La figure ci-dessous illustre les poids obtenus selon chacune de ces fonctions de pondération.\n\nlibrary(dplyr, quietly = TRUE)\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr, quietly = TRUE)\nlibrary(ggplot2, quietly = TRUE)\n\n\nd &lt;- 1:600\ndf &lt;- data.frame(\n  distance = d,\n  binaire = w_binaire(d),\n  slow_decay = w_slow_decay(d),\n  fast_decay = w_fast_decay(d),\n  palier = W_cont_palier(d),\n  logistique1 = w_logit1(d),\n  logistique2 = w_logit2(d)\n)\npond_cols &lt;- names(df)[names(df) != 'distance']\n\ndf2 &lt;- df %&gt;% pivot_longer(cols = all_of(pond_cols))\n\nggplot(df2) + \n  geom_line(aes(x = distance, y = value, color = name, group = name)) + \n  labs(x = 'distance (m)', \n       y = 'pondération',\n       color = 'fonction'\n       ) + \n  theme_minimal()\n\n\n\n\nLe choix d’une fonction de pondération doit avant tout reposer sur des considérations théoriques et représenter correctement la décroissance de l’attractivité d’un service en fonction de la distance d’éloignement. Idéalement, ces fonctions devraient être ajustées en fonction de données sur les habitudes de déplacement des individus.\n\n\n\n5.3.2.4 Types de distance\nTel que décrit à la section 5.1.2, les mesures d’accessibilité peuvent être calculées en fonction de trajets les plus rapides selon différents modes de transport, soit en automobile, à pied, en vélo et en transport en commun (figure 5.4)."
  },
  {
    "objectID": "05-AnalyseReseau.html#sec-054",
    "href": "05-AnalyseReseau.html#sec-054",
    "title": "5  Mesures d’accessibilité spatiale selon différents modes de transport",
    "section": "\n5.4 Mesures d’accessibilité spatiale potentielle dans R",
    "text": "5.4 Mesures d’accessibilité spatiale potentielle dans R\n\n5.4.1 Accessibilité spatiale potentielle aux supermarchés\nDans ce premier exemple applicatif dans R, nous élaborons un diagnostic de l’accessibilité spatiale potentielle aux supermarchés dans la ville de Sherbrooke avec les quatre paramètres suivants :\n\nUnité spatiale de référence : aires de diffusion (AD) de 2021 de la ville de Sherbrooke.\nMéthode d’agrégation : calcul des moyennes pondérées par le nombre de logements des immeubles du rôle d’évaluation foncière compris dans les AD.\nTrois mesures d’accessibilité : le supermarché le plus proche (en minutes), le nombre de supermarchés à 30 minutes et moins, la distance moyenne aux trois supermarchés les plus proches.\nType de distance : chemin le plus rapide à la marche.\n\nÉtape 1. Importation des trois couches géographiques.\n\n## Unités d'évaluation foncière\nRole &lt;- st_read(dsn = \"data/chap05/AutresDonnees/Role2022Sherb.gdb\",\n                layer = \"rol_unite_Sherbrooke\", quiet = TRUE)\n## Aires de diffusion\nAD &lt;- st_read(dsn = \"data/chap05/AutresDonnees/Commerces.gpkg\",\n                        layer = \"AD_Sherbrooke\", quiet=TRUE)\n## Supermarchés\nSupermarches &lt;- st_read(dsn = \"data/chap05/AutresDonnees/Commerces.gpkg\",\n                        layer = \"supermarche\", quiet=TRUE)\n## Changement de projection\nSupermarches &lt;- st_transform(Supermarches, crs = 4326)\nRole &lt;- st_transform(Role, crs = 4326)\nAD &lt;- st_transform(AD, crs = 4326)\n\nÉtape 2. Réalisation d’une jointure spatiale pour attribuer à chaque unité d’évaluation l’identifiant de l’aire de diffusion (champ ADIDU) dans laquelle elle est comprise.\n\n## Jointure spatiale entre le Role et les AD\nRole &lt;- st_join(Role, AD[,\"ADIDU\"], join = st_intersects)\nRole &lt;- subset(Role, is.na(ADIDU)==FALSE)\n## Nombre de distances à calculer\nnO = nrow(Role)\nnD = nrow(Supermarches)\nNOD = nO * nD\ncat(\"Origines (O) :\", nO, \"îlots\",\n    \"\\n Destinations (D) :\", nD, \"supermarchés\",\n    \"\\n Distances OD = \", NOD)\n\nOrigines (O) : 46534 îlots \n Destinations (D) : 27 supermarchés \n Distances OD =  1256418\n\n\nÉtape 3. Création des points d’origine et de destination.\n\n\n\n\n\nRattacher les points aux tronçons de rue\n\n\nPuisque nous utilisons le trajet le plus court à pied, les points d’origine et de destination doivent être rattachés à des tronçons de rues qui ne sont pas des autoroutes ou tout autre tronçon interdit à la marche. Pour ce faire, le package R5R dispose d’une fonction très intéressante :\nfind_snap(r5r_core, points, mode = \"WALK\").\nSans le recours à cette fonction, un point d’origine ou de destination risque d’être rattaché à un tronçon autoroutier, faisant en sorte que le trajet ne pourra être calculé à la marche.\n\n\n\n## Origines\nOrigines &lt;- Role\nOrigines$lat &lt;- st_coordinates(Origines)[,2]\nOrigines$lon &lt;- st_coordinates(Origines)[,1]\nOrigines$id &lt;- Origines$mat18\nOrigines &lt;- find_snap(r5r_core, Origines, mode = \"WALK\")\nOrigines$lat &lt;- Origines$snap_lat\nOrigines$lon &lt;- Origines$snap_lon\nOrigines &lt;- Origines[, c(\"point_id\", \"lat\", \"lon\", \"distance\")]\nnames(Origines) &lt;- c(\"id\", \"lat\", \"lon\", \"distance\")\n## Destinations\nDestinations &lt;- Supermarches\nDestinations$lat &lt;- st_coordinates(Destinations)[,2]\nDestinations$lon &lt;- st_coordinates(Destinations)[,1]\nnames(Destinations)[1] &lt;- \"id\"\nDestinations &lt;- find_snap(r5r_core, Destinations, mode = \"WALK\")\nDestinations$lat &lt;- Destinations$snap_lat\nDestinations$lon &lt;- Destinations$snap_lon\nDestinations &lt;- Destinations[, c(\"point_id\", \"lat\", \"lon\", \"distance\")]\nnames(Destinations) &lt;- c(\"id\", \"lat\", \"lon\", \"distance\")\n\nÉtape 4. Construction de la matrice origines-destinations avec la fonction travel_time_matrix et sauvegarde dans un fichier Rdata.\n\n## Matrice OD à la marche\nt1 &lt;-Sys.time()\nmatriceOD.Marche &lt;- travel_time_matrix(r5r_core = r5r_core,\n                                       origins = Origines,\n                                       destinations = Destinations,\n                                       mode = \"WALK\",\n                                       walk_speed = 4.5,  # valeur par défaut 3.6\n                                       max_trip_duration = 240,\n                                       max_walk_time = Inf)\nt2 &lt;-Sys.time()\nduree.marche = as.numeric(difftime(t2, t1), units = \"mins\")\ncat(\"Temps de calcul :\", round(duree.marche,2), \"minutes\")\n## Enregistrement des résultats dans un fichier Rdata\nsave(duree.marche, matriceOD.Marche,\n     file=\"data/chap05/Resultats/MatricesMarcheRoleSupermarche.Rdata\")\n\nÉtape 5. Calcul des trois mesures d’accessibilité pour les unités d’évaluation.\n\n## Chargement du fichier Rdata\nload(\"data/chap05/Resultats/MatricesMarcheRoleSupermarche.Rdata\")\ncat(\"Temps de calcul pour la matrice :\", round(duree.marche,2), \"minutes\")\n\nTemps de calcul pour la matrice : 4.53 minutes\n\n## Supermarché le plus proche\nSupermarche.PlusProche &lt;- matriceOD.Marche %&gt;% \n  group_by(from_id) %&gt;% \n  summarise(\n    plus_proche = min(travel_time_p50))\n\n## Nombre de supermarchés à moins de 30 minutes\nSupermarche.N30mins &lt;- matriceOD.Marche %&gt;% \n  filter(travel_time_p50 &lt;= 30) %&gt;% \n  group_by(from_id) %&gt;% \n  summarise(\n    nb_30_min = n())\n\n# Distance moyenne aux trois supermarchés les plus proches\nSupermarche.3 &lt;- matriceOD.Marche %&gt;% \n  group_by(from_id) %&gt;% \n  mutate(dist_rank = order(order(travel_time_p50, decreasing=FALSE))) %&gt;%\n  filter(dist_rank &lt;= 3) %&gt;%\n  summarise(nb = n(),\n            sum_dist = sum(travel_time_p50))\n\n# Notez ici que nous pouvons avoir des origines pour lesquelles\n# n'avons pas trois supermarchés à moins de 240 minutes.\n# Pour ces quelques cas, nous ajoutons des temps de 240 minutes pour les \n# origines avec des supermarchés manquants.\nSupermarche.3$sum_dist &lt;- Supermarche.3$sum_dist + 240 * (3 - Supermarche.3$nb)\nSupermarche.3$mean_n3 &lt;- Supermarche.3$sum_dist / 3\nSupermarche.3$nb &lt;- NULL\nSupermarche.3$sum_dist &lt;- NULL\n\n# Nous pouvons à présent fusionner nos différent dataframes avec les \n# indicateurs d'accessibilité\nRole &lt;- Role %&gt;% \n  left_join(Supermarche.PlusProche, by = c('mat18' = 'from_id')) %&gt;%\n  left_join(Supermarche.N30mins, by = c('mat18' = 'from_id')) %&gt;%\n  left_join(Supermarche.3, by = c('mat18' = 'from_id')) %&gt;%\n  rename(\n      SupPlusProcheMin = 'plus_proche',\n      SupN30min = 'nb_30_min',\n      Moy3Sup = 'mean_n3'\n  )\n\n# Certaines observations n'ont pas de supermarchés à 30 minutes\n# Nous mettons alors les valeurs à 0\nRole$SupN30min[is.na(Role$SupN30min)] &lt;- 0\n\nÉtape 6. Calcul des moyennes pondérées par le nombre de logements (champ Logements) pour les aires de diffusion avec le package dplyr.\n\nlibrary(dplyr)\n## Création d'un DataFrame temporaire sans la géométrie\nRole.Temp &lt;- st_drop_geometry(Role)\n## Moyennes pondérées pour les trois indicateurs d'accessibilité\nMesureAcc &lt;- Role.Temp %&gt;%\n                group_by(ADIDU) %&gt;%\n                summarize(\n                  SupPlusProcheMin = weighted.mean(SupPlusProcheMin, Logements),\n                  SupN30min = weighted.mean(SupN30min, Logements),\n                  Moy3Sup = weighted.mean(Moy3Sup, Logements)\n                )\n\n## Fusion avec la couche des AD\nAD &lt;- left_join(AD, MesureAcc, by=\"ADIDU\")\n\nÉtape 7. Cartographie des résultats avec le package tmap.\nTout d’abord, nous analysons les statistiques univariées pour repérer les valeurs minimales et maximales pour les trois mesures d’accessibilité avec la fonction summary.\n\nTroisMesures &lt;- c(\"SupPlusProcheMin\",\"SupN30min\",\"Moy3Sup\")\nsummary(AD[, TroisMesures])\n\n SupPlusProcheMin    SupN30min        Moy3Sup                   geom    \n Min.   :  4.528   Min.   :0.000   Min.   :  6.149   MULTIPOLYGON :254  \n 1st Qu.: 10.995   1st Qu.:1.000   1st Qu.: 17.416   epsg:4326    :  0  \n Median : 16.244   Median :2.536   Median : 23.555   +proj=long...:  0  \n Mean   : 21.903   Mean   :2.774   Mean   : 32.007                      \n 3rd Qu.: 24.413   3rd Qu.:4.518   3rd Qu.: 33.837                      \n Max.   :130.615   Max.   :7.333   Max.   :139.209                      \n NA's   :2                         NA's   :2                            \n\n\nUne fois les valeurs maximales et minimales analysées, réalisons les cartes (figure 5.16).\n\n## Importation des arrondissements de la ville de Sherbrooke\narrondissements &lt;- st_read(dsn = \"data/Chap05/AutresDonnees/Arrondissements.shp\", \n                           quiet=TRUE)\n## Construction des cartes\ntmap_mode(\"plot\")\n# Carte pour les supermarchés\nCarte0 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(Supermarches)+\n              tm_dots(col=\"red\", size=0.25)+\n          tm_layout(frame = FALSE, legend.outside = TRUE, \n                    legend.format = list(text.separator = \"à\"),\n                    title = \"Supermarché\",\n                    title.size = 1)\n# Carte pour le supermarché le plus proche\nmax.acc1 &lt;- max(AD$SupPlusProcheMin,na.rm=TRUE)\nCarte1 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(AD)+\n            tm_fill(col=\"SupPlusProcheMin\", \n                    border.lwd = 1,\n                    style = \"fixed\",\n                    breaks = c(0,10,20,30,40,60,max.acc1),\n                    palette=\"-YlOrRd\",\n                    size = .2, \n                              legend.format = list(text.separator = \"à\"),\n                    textNA = \"Sans données\",\n                              title=\"Plus proche\")+\n          tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_layout(frame = FALSE, legend.outside = TRUE)\n# Carte pour le nombre de supermarchés à 30 minutes ou moins\nmax.acc2 &lt;- max(AD$SupN30min,na.rm=TRUE)\nCarte2 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(AD)+\n            tm_fill(col=\"SupN30min\", \n                    border.lwd = 1,\n                    style = \"fixed\",\n                    breaks = c(0,1,2,3,4,5,max.acc2),\n                    palette=\"YlOrRd\",\n                    size = .2, \n                              legend.format = list(text.separator = \"à\"),\n                            textNA = \"Sans données\",\n                    title=\"Sup. à 30 min\")+\n          tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_layout(frame = FALSE, legend.outside = TRUE)\n# Carte pour la distance moyenne aux trois supermarchés les plus proches\nmax.acc3 &lt;- max(AD$Moy3Sup,na.rm=TRUE)\nCarte3 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(AD)+\n            tm_fill(col=\"Moy3Sup\", \n                    border.lwd = 1,\n                    style = \"fixed\",\n                    breaks = c(5,10,20,30,40,60,max.acc3),\n                    palette=\"-YlOrRd\",\n                    size = .2, \n                            legend.format = list(text.separator = \"à\"),\n                            textNA = \"Sans données\",\n                    title=\"Moy. 3 plus proches\")+\n          tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_layout(frame = FALSE, legend.outside = TRUE)\n## Figure avec les quatre cartes\ntmap_arrange(Carte0, Carte1, Carte2, Carte3)\n\n\n\nFigure 5.16: Accessibilité spatiale potentielle à pied aux supermarchés (en minutes), aires de diffusion de la ville de Sherbrooke, 2021\n\n\n\n\n5.4.2 Accessibilité spatiale potentielle aux patinoires extérieures\nDans ce second exemple applicatif dans R, nous élaborons un diagnostic de l’accessibilité spatiale potentielle aux patinoires extérieures dans la ville de Sherbrooke avec les quatre paramètres suivants :\n\nUnité spatiale de référence : aires de diffusion (AD) de 2021 de la ville de Sherbrooke.\nMéthode d’agrégation : calcul des moyennes pondérées par la population totale des îlots compris dans les AD.\nDeux mesures d’accessibilité : patinoire la plus proche (en minutes); E2SFCA (Enhanced two-step floating catchment area), soit le nombre de patinoires pour 1000 habitants dans un rayon de 30 minutes de marche.\nType de distance : chemin le plus rapide à la marche.\n\nÉtape 1. Importation des trois couches géographiques.\n\n## Unités d'évaluation foncière\nPatinoires &lt;- st_read(dsn = \"data/chap05/AutresDonnees/Patinoires.shp\",\n                      quiet = TRUE)\n## Aires de diffusion\nAD &lt;- st_read(dsn = \"data/chap05/AutresDonnees/Commerces.gpkg\",\n              layer = \"AD_Sherbrooke\", quiet=TRUE)\n## Ilots de recensements\nIlots &lt;- st_read(dsn = \"data/chap05/AutresDonnees/Commerces.gpkg\",\n                 layer = \"Ilots\", quiet=TRUE)\n## Changement de projection\nPatinoires &lt;- st_transform(Patinoires, crs = 4326)\nAD &lt;- st_transform(AD, crs = 4326)\nIlots &lt;- st_transform(Ilots, crs = 4326)\n\nÉtape 2. Réalisation d’une jointure spatiale pour attribuer à chaque îlot l’identifiant de l’aire de diffusion (champ ADIDU) dans laquelle il est compris.\n\n## Jointure spatiale entre le Rôle et les AD\nIlots &lt;- st_join(st_centroid(Ilots), AD[,\"ADIDU\"], join = st_intersects)\nIlots &lt;- Ilots[,c(\"id\",\"pop2021\",\"ADIDU\")]\n\nÉtape 3. Création des points d’origine et de destination.\n\n## Origines\nOrigines &lt;- Ilots\nOrigines$lat &lt;- st_coordinates(Origines)[,2]\nOrigines$lon &lt;- st_coordinates(Origines)[,1]\nOrigines &lt;- find_snap(r5r_core, Origines, mode = \"WALK\")\nOrigines$lat &lt;- Origines$snap_lat\nOrigines$lon &lt;- Origines$snap_lon\nOrigines &lt;- Origines[, c(\"point_id\", \"lat\", \"lon\", \"distance\")]\nnames(Origines) &lt;- c(\"id\", \"lat\", \"lon\", \"distance\")\n## Destinations\nDestinations &lt;- Patinoires\nDestinations$lat &lt;- st_coordinates(Destinations)[,2]\nDestinations$lon &lt;- st_coordinates(Destinations)[,1]\nDestinations$id &lt;- as.character(1:nrow(Destinations))\nDestinations &lt;- find_snap(r5r_core, Destinations, mode = \"WALK\")\nDestinations$lat &lt;- Destinations$snap_lat\nDestinations$lon &lt;- Destinations$snap_lon\nDestinations &lt;- Destinations[, c(\"point_id\", \"lat\", \"lon\", \"distance\")]\nnames(Destinations) &lt;- c(\"id\", \"lat\", \"lon\", \"distance\")\n\nÉtape 4. Construction de la matrice origines-destinations avec la fonction travel_time_matrix et sauvegarde dans un fichier Rdata.\n\n## Matrice OD à la marche\nt1 &lt;-Sys.time()\nmatriceODPatinoire.Marche &lt;- travel_time_matrix(r5r_core = r5r_core,\n                                                origins = Origines,\n                                                destinations = Destinations,\n                                                mode = \"WALK\",\n                                                walk_speed = 4.5,\n                                                max_trip_duration = 240,\n                                                max_walk_time = Inf)\nt2 &lt;-Sys.time()\nduree.marche = as.numeric(difftime(t2, t1), units = \"mins\")\ncat(\"Temps de calcul :\", round(duree.marche,2), \"minutes\")\n## Enregistrement des résultats dans un fichier Rdata\nsave(duree.marche, matriceODPatinoire.Marche,\n     file=\"data/chap05/Resultats/matriceODPatinoire.Rdata\")\n\nÉtape 5. Calcul des deux mesures d’accessibilité pour les îlots.\n\n## Chargement du fichier Rdata\nload(\"data/chap05/Resultats/matriceODPatinoire.Rdata\")\ncat(\"Temps de calcul pour la matrice :\", round(duree.marche,2), \"minutes\")\n\nTemps de calcul pour la matrice : 0.37 minutes\n\n\nLe code ci-dessous permet de calculer la distance à la patinoire la plus proche pour les aires de diffusion.\n\n## Patinoire la plus proche\nPatinoire.PlusProche &lt;- matriceODPatinoire.Marche %&gt;% \n  group_by(from_id) %&gt;%\n  summarise(PatinoirePlusProche = min(travel_time_p50))\n## Fusion avec la couche des îlots\nIlots &lt;- left_join(Ilots, Patinoire.PlusProche, by=c(\"id\" = 'from_id'))\n\nPuis, nous calculons la version de la méthode du E2SFCA avec une fonction de gradient continue (McGrail et Humphreys 2009).\n\nsource(\"code_complementaire/E2SFCA.R\")\n## Ajout des champs de population dans la matrice\nTempIlots &lt;- st_drop_geometry(Ilots)\nmatriceODPatinoire &lt;- merge(matriceODPatinoire.Marche, \n                   TempIlots[,c(\"id\",\"pop2021\")],\n                   by.x = \"from_id\", by.y=\"id\")\nmatriceODPatinoire$Wd &lt;- 1\nnames(matriceODPatinoire) &lt;- c(\"from_id\", \"to_id\", \"Marche\", \"Wo\", \"Wd\")\nhead(matriceODPatinoire, n=2)\n\n       from_id to_id Marche  Wo Wd\n1: 24430011001     1     90 180  1\n2: 24430011001     2    159 180  1\n\n# Utilisation de la pondération de gradient continu du E2SFCA\nWfun &lt;- function(x){\n  w &lt;- ifelse(x &lt; 5, 1,((30 - x) / (30 - 5))**1.5)\n  w[x  &gt; 30] &lt;- 0\n  return(w)\n}\n# Calcul du résultat en milliers d'habitants\nmatriceODPatinoire$Wo &lt;- matriceODPatinoire$Wo / 1000\nMesureE2SFCA &lt;- GTSFCA(dist_mat = matriceODPatinoire,\n                        Wfun = Wfun,\n                        IDorigine = \"from_id\",\n                        IDdestination = \"to_id\",\n                        CoutDistance = \"Marche\",\n                        Wo = \"Wo\",\n                        Wd = \"Wd\",\n                        ChampSortie = \"E2SFCA_G\")\n\nMesureE2SFCA$E2SFCA_G[is.na(MesureE2SFCA$E2SFCA_G)] &lt;- 0\nIlots &lt;- merge(Ilots, MesureE2SFCA, by.x =\"id\", by=\"from_id\", all.x = TRUE)\nIlots$E2SFCA_G[is.na(Ilots$E2SFCA_G)] &lt;- 0\n\nÉtape 6. Calcul des moyennes pondérées par la population des îlots (champ pop2021) pour les aires de diffusion avec le package dplyr.\n\n## Création d'un DataFrame temporaire sans la géométrie\nIlots.Temp &lt;- st_drop_geometry(Ilots)\n## Moyenne pondérées pour la patinoire la plus proche\nMesureAcc1 &lt;- Ilots.Temp %&gt;%\n                group_by(ADIDU) %&gt;%\n                summarize(PatinoirePlusProche = weighted.mean(PatinoirePlusProche, pop2021))\n## Moyenne non pondérée pour le E2SFCA, car la population est déjà prise en compte\nMesureAcc2 &lt;- aggregate(E2SFCA_G ~ ADIDU, Ilots.Temp, FUN = mean)\n## Fusion avec la couche des îlots\nAD &lt;- merge(AD, MesureAcc1, by=\"ADIDU\")\nAD &lt;- merge(AD, MesureAcc2, by=\"ADIDU\")\n\nÉtape 7. Cartographie des résultats avec le package tmap.\nTout d’abord, nous analysons les statistiques univariées pour repérer les valeurs minimales et maximales pour les trois mesures d’accessibilité.\n\nDeuxMesures &lt;- c(\"PatinoirePlusProche\",\"E2SFCA_G\")\nsummary(AD[, DeuxMesures])\n\n PatinoirePlusProche    E2SFCA_G                geometry  \n Min.   : 1.00       Min.   :0.00000   MULTIPOLYGON :254  \n 1st Qu.: 9.28       1st Qu.:0.09836   epsg:4326    :  0  \n Median :14.50       Median :0.19188   +proj=long...:  0  \n Mean   :17.29       Mean   :0.22520                      \n 3rd Qu.:19.83       3rd Qu.:0.30166                      \n Max.   :85.58       Max.   :1.26495                      \n NA's   :2                                                \n\n\nUne fois avoir pris connaissance des valeurs maximales et minimales, nous pouvons réaliser les cartes (figure 5.17).\n\n## Importation des arrondissements de la ville de Sherbrooke\narrondissements &lt;- st_read(dsn = \"data/Chap05/AutresDonnees/Arrondissements.shp\", \n                           quiet=TRUE)\n## Construction des cartes\ntmap_mode(\"plot\")\n# Carte pour les patinoires\nCarte0p &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(Patinoires)+\n              tm_dots(col=\"red\", size=0.25)+\n          tm_layout(frame = FALSE, legend.outside = TRUE, \n                    title = \"Patinoire extérieure\",\n                    title.size = 1)\n# Carte pour la patinoire la plus proche\nmax.acc1 &lt;- max(AD$PatinoirePlusProche,na.rm=TRUE)\nCarte1p &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(AD)+\n            tm_fill(col=\"PatinoirePlusProche\", \n                    border.lwd = 1,\n                    style = \"fixed\",\n                    breaks = c(0,10,20,30,40,60,max.acc1),\n                    palette=\"-YlOrRd\",\n                    size = .2, \n                    legend.format = list(text.separator = \"à\"),\n                    textNA = \"Sans données\",\n                    title=\"Plus proche (min)\")+\n          tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_layout(frame = FALSE, legend.outside = TRUE)\n# Carte pour le 2ESFCA\nAD2 &lt;- subset(AD, E2SFCA_G !=0)\nmin.acc2 &lt;- min(AD2$E2SFCA_G,na.rm=TRUE)\nmax.acc2 &lt;- max(AD2$E2SFCA_G,na.rm=TRUE)\nCarte2p &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_shape(AD)+tm_fill(col=\"gray\")+\n          tm_shape(AD2)+\n            tm_fill(col=\"E2SFCA_G\", \n                    border.lwd = 1,\n                    style = \"fixed\",\n                    breaks = c(min.acc2,0.25,0.50,0.75,1,max.acc2),\n                    palette=\"YlOrRd\",\n                    size = .2, \n                    legend.format = list(text.separator = \"à\"),\n                    title=\"Patinoire pour 1000 hab.\")+\n          tm_shape(arrondissements)+tm_borders(lwd = 2)+\n          tm_layout(frame = FALSE, legend.outside = TRUE)\n## Figure avec les trois cartes\ntmap_arrange(Carte0p, Carte1p, Carte2p,\n             nrow=2, ncol=2)\n## Arrêt de Java\nr5r::stop_r5(r5r_core)\nrJava::.jgc(R.gc = TRUE)\n\n\n\nFigure 5.17: Accessibilité spatiale potentielle à pied aux patinoires extérieures, aires de diffusion de la ville de Sherbrooke, 2021"
  },
  {
    "objectID": "05-AnalyseReseau.html#sec-055",
    "href": "05-AnalyseReseau.html#sec-055",
    "title": "5  Mesures d’accessibilité spatiale selon différents modes de transport",
    "section": "\n5.5 Quiz de révision du chapitre",
    "text": "5.5 Quiz de révision du chapitre\n\n\n\n\n\nQuels sont les trois principaux problèmes résolus en analyse de réseau?\n\n\nRelisez au besoin la section 5.1.2.\n\n\n\n\n\n\nTrouver le trajet le plus court ou le plus rapide entre deux points (algorithme de Dijkstra)\n\n\n\n\n\n\n\nTrouver la route optimale comprenant plusieurs arrêts (problème du voyageur de commerce)\n\n\n\n\n\n\n\nDéfinir des zones de desserte autour d’une origine (algorithme de Dijkstra)\n\n\n\n\n\n\n\nMesurer la dépendance spatiale\n\n\n\n\n\n\n\n\n\n\nQuels sont les quatre autres problèmes résolus en analyse de réseau?\n\n\nRelisez au besoin la section 5.1.2.\n\n\n\n\n\n\nTrouver les k services les plus proches à partir d’une origine\n\n\n\n\n\n\n\nConstruire une matrice de distance origines-destinations\n\n\n\n\n\n\n\nRépartir aléatoirement des points dans un territoire donné\n\n\n\n\n\n\n\nRésoudre le problème de tournées de véhicules\n\n\n\n\n\n\n\nRéaliser un modèle localisation-affectation\n\n\n\n\n\n\n\n\n\n\nQuels sont les trois fichiers nécessaires pour construire un réseau multimode avec R5R?\n\n\nRelisez au besoin le début de la section 5.1.2.\n\n\n\n\n\n\nUn fichier pbf pour les données d’OpenStreetMap\n\n\n\n\n\n\n\nUn ou plusieurs fichiers GTFS\n\n\n\n\n\n\n\nUn fichier pour les bâtiments\n\n\n\n\n\n\n\nUn fichier GeoTIFF d’élévation\n\n\n\n\n\n\n\n\n\n\nQuelle fonction de R5R permet de construire un réseau multimode?\n\n\nRelisez au besoin la section 5.2.3.\n\n\n\n\n\n\ndetailed_itineraries()\n\n\n\n\n\n\n\nsetup_r5()\n\n\n\n\n\n\n\ntravel_time_matrix()\n\n\n\n\n\n\n\n\n\n\nQuelle fonction de R5R permet de construire un itinéraire?\n\n\nRelisez au besoin la section 5.2.3.\n\n\n\n\n\n\ndetailed_itineraries()\n\n\n\n\n\n\n\nsetup_r5()\n\n\n\n\n\n\n\ntravel_time_matrix()\n\n\n\n\n\n\n\n\n\n\nQuelle fonction de R5R permet de construire une matrice origine-destination?\n\n\nRelisez au besoin la section 5.2.3.\n\n\n\n\n\n\ndetailed_itineraries()\n\n\n\n\n\n\n\nsetup_r5()\n\n\n\n\n\n\n\ntravel_time_matrix()\n\n\n\n\n\n\n\n\n\n\nPour évaluer l’accessibilité selon la conceptualisation de la proximité immédiate, quelle mesure d’accessibilité utilisez-vous?\n\n\nRelisez au besoin la section 5.3.2.3.\n\n\n\n\n\n\nDistance entre l’origine et le service le plus proche\n\n\n\n\n\n\n\nNombre de services présents à moins de n mètres ou minutes\n\n\n\n\n\n\n\nDistance moyenne entre une origine et n services\n\n\n\n\n\n\n\nModèles gravitaires et méthodes two-step floating catchment area (2SFCA)\n\n\n\n\n\n\n\n\n\n\nPour évaluer l’accessibilité selon la conceptualisation de l’offre et la demande, quelle mesure d’accessibilité utilisez-vous?\n\n\nRelisez au besoin la section 5.3.2.3.\n\n\n\n\n\n\nDistance entre l’origine et le service le plus proche\n\n\n\n\n\n\n\nNombre de services présents à moins de n mètres ou minutes\n\n\n\n\n\n\n\nDistance moyenne entre une origine et n services\n\n\n\n\n\n\n\nModèles gravitaires et méthodes two-step floating catchment area (2SFCA)\n\n\n\n\n\n\n\n\n\n\nPour évaluer l’accessibilité selon l’offre de services dans l’environnement immédiat, quelle mesure d’accessibilité utilisez-vous?\n\n\nRelisez au besoin la section 5.3.2.3.\n\n\n\n\n\n\nDistance entre l’origine et le service le plus proche\n\n\n\n\n\n\n\nNombre de services présents à moins de n mètres ou minutes\n\n\n\n\n\n\n\nDistance moyenne entre une origine et n services\n\n\n\n\n\n\n\nModèles gravitaires et méthodes two-step floating catchment area (2SFCA)\n\n\n\n\n\n\n\n\n\nVérifier votre résultat"
  },
  {
    "objectID": "05-AnalyseReseau.html#sec-056",
    "href": "05-AnalyseReseau.html#sec-056",
    "title": "5  Mesures d’accessibilité spatiale selon différents modes de transport",
    "section": "\n5.6 Exercices de révision",
    "text": "5.6 Exercices de révision\n\n\n\n\n\nExercice 1. Calcul de trajets selon différents modes de transport\n\n\nComplétez le code ci-dessous pour réaliser les étapes suivantes :\n\nConstruisez un réseau R5R pour la région de Laval avec un fichier OMS (pbf), un fichier d’élévation et un fichier GTFS.\nCréez deux points : l’un pour la station de métro Morency (-73.7199, 45.5585) l’autre pour une adresse fictive (-73.7183, 45.5861).\n\nCalculez les trajets en automobile, à vélo, à pied, et en transport en commun de l’adresse vers la station de métro et l’inverse avec (10 points) :\n\nUne vitesse de 15 km/h pour le vélo.\nUne vitesse de 4,5 km/h pour la marche.\nUn trajet aller le 12-02-2024 à 8h de l’adresse vers la station de métro.\nUn trajet retour le 12-02-2024 à 18h de la station de métro vers l’adresse.\n\n\n\nRéalisez deux figures :\n\nUne figure avec quatre cartes des trajets aller (marche, vélo, auto, transport en commun).\nUne figure avec quatre cartes des trajets retour (marche, vélo, auto, transport en commun).\n\n\nArrêtez java.\n\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(r5r)\n\nsetwd(\"data/chap05/Laval\")\nrJava::.jinit()\noptions(java.parameters = \"-Xmx2G\")\n\n# 1. Construction du réseau\ndossierdata &lt;- paste0(getwd(),\"/_DataReseau\")\nlist.files(dossierdata)\nr5r_core &lt;- setup_r5(à compléter)\n\n# 2. Création de deux points\nPts &lt;- data.frame(id = c(\"Station Morency\", \"Adresse 1\"),\n                  lon = c(à compléter),\n                  lat = c(à compléter))\nPts &lt;- st_as_sf(Pts, coords = c(\"lon\",\"lat\"), crs = 4326)\nStationMorency &lt;- Pts[1,]\nAdresse1 &lt;- Pts[2,]\n\n## 2.1. Trajets en automobile\nAuto.1 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                                   origins = Adresse1,\n                                   destinations = StationMorency,\n                                   mode = \"CAR\",\n                                   shortest_path = FALSE,\n                                   drop_geometry = FALSE)\nAuto.2 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                               origins = StationMorency,\n                               destinations = Adresse1,\n                               mode = \"CAR\",\n                               shortest_path = FALSE,\n                               drop_geometry = FALSE)\n## 2.2. Trajets en vélo\nvelo.1 &lt;- detailed_itineraries(à compléter)\nvelo.2 &lt;- detailed_itineraries(à compléter)\n## 2.3. Trajets à pied\nmarche.1 &lt;- detailed_itineraries(à compléter)\nmarche.2 &lt;- detailed_itineraries(à compléter)\n\n## 2.4. Trajets en transport en commun\ndateheure.matin &lt;- as.POSIXct(\"12-02-2024 08:00:00\",\n                              format = \"%d-%m-%Y %H:%M:%S\")\ndateheure.soir  &lt;- as.POSIXct(\"12-02-2024 18:00:00\",\n                              format = \"%d-%m-%Y %H:%M:%S\")\n### Définir le temps de marche maximal\nminutes_marches_max &lt;- 20\nTC.1 &lt;- detailed_itineraries(à compléter)\nTC.2 &lt;- detailed_itineraries(à compléter)\n\n\n# 4. Cartographie\n  # - Map1.Aller : Marche (de la résidence à la station de métro)\n  # - Map2.Aller : Vélo (de la résidence à la station de métro)\n  # - Map3.Aller : Auto (de la résidence à la station de métro)\n  # - Map4.Aller : Transport en commun (de la résidence à la station de métro)\ntmap_mode(view)\nMap1.Aller &lt;- tm_shape(marche.1)+tm_lines(col=\"mode\", lwd = 3,\n                                      popup.vars = c(\"mode\", \"from_id\", \"to_id\",\n                                                     \"segment_duration\", \"distance\",\n                                                     \"total_duration\", \"total_distance\"))+\n              tm_shape(Adresse1)+tm_dots(col=\"green\", size = .15)+\n              tm_shape(StationMorency)+tm_dots(col=\"red\", size = .15)\n\nMap2.Aller &lt;- à compléter\nMap3.Aller &lt;- à compléter\nMap4.Aller &lt;- à compléter\n\ntmap_arrange(Map1.Aller, Map2.Aller, Map3.Aller, Map4.Aller, ncol = 2, nrow = 2)\n\n## Réaliser une figure avec quatre figures pour les trajets retour :\n  # - Map1.Retour : Marche (de la station de métro à la résidence)\n  # - Map2.Retour : Vélo (de la station de métro à la résidence)\n  # - Map3.Retour : Auto (de la station de métro à la résidence)\n  # - Map4.Retour : Transport en commun (de la station de métro à la résidence)\n\nMap1.Retour &lt;- tm_shape(marche.2)+tm_lines(col=\"mode\", lwd = 3,\n                                          popup.vars = c(\"mode\", \"from_id\", \"to_id\",\n                                                         \"segment_duration\", \"distance\",\n                                                         \"total_duration\", \"total_distance\"))+\n  tm_shape(Adresse1)+tm_dots(col=\"red\", size = .15)+\n  tm_shape(StationMorency)+tm_dots(col=\"green\", size = .15)\n\nMap2.Retour &lt;- à compléter\nMap3.Retour &lt;- à compléter\nMap4.Retour &lt;- à compléter\n\ntmap_arrange(Map1.Retour, Map2.Retour, Map3.Retour, Map4.Retour, ncol = 2, nrow = 2)\n\n# 5.  Arrêt de java\nr5r::stop_r5(r5r_core)\nrJava::.jgc(R.gc = TRUE)\n\nCorrection à la section 9.5.1.\n\n\n\n\n\n\n\nExercice 2. Calcul d’isochrones\n\n\nComplétez le code ci-dessous pour réaliser les étapes suivantes :\n\nCalculez des isochrones à pied de 5, 10 et 15 minutes.\nCalculez des isochrones à vélo de 5, 10 et 15 minutes.\nCartographiez les résultats.\n\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(r5r)\n\n## Construction du réseau\nsetwd(\"data/chap05/Laval\")\nrJava::.jinit()\noptions(java.parameters = \"-Xmx2G\")\ndossierdata &lt;- paste0(getwd(),\"/_DataReseau\")\nlist.files(dossierdata)\nr5r_core &lt;- setup_r5(data_path = dossierdata,\n                     elevation = \"TOBLER\",\n                     verbose = FALSE, overwrite = FALSE)\n\n## Point pour la Station Morency\nStationMorency &lt;- data.frame(id = \"Station Morency\",\n                             lon = -73.7199,\n                             lat = 45.5585,  45.5861)\nStationMorency &lt;- st_as_sf(StationMorency, \n                           coords = c(\"lon\",\"lat\"), crs = 4326)\n\n# 1. Calcul d'isochrones à pied de 5, 10 et 15 minutes\nIso.Marche &lt;- isochrone(à compléter)\n# 1.2. Isochrone à vélo de 5, 10 et 15 minutes\nIso.Velo &lt;- isochrone(à compléter)\n\n# 3. Cartographie les résultats\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\nCarte.Marche &lt;- tm_shape(à compléter)+\n                    tm_fill(à compléter)+\n                tm_shape(StationMorency)+tm_dots(col=\"darkred\", size = .25)\n\nCarte.Velo &lt;- tm_shape(à compléter)+\n                    tm_fill(à compléter)+\n                tm_shape(StationMorency)+tm_dots(col=\"darkred\", size = .25)\n\ntmap_arrange(Carte.Marche, Carte.Velo, ncol = 2)\n\n# 4. Arrêt de java\nr5r::stop_r5(r5r_core)\nrJava::.jgc(R.gc = TRUE)\n\nCorrection à la section 9.5.2.\n\n\n\n\n\n\nApparicio, Philippe, Marie-Soleil Cloutier et Richard Shearmur. 2007. « The case of Montreal’s missing food deserts: evaluation of accessibility to food supermarkets. » International journal of health geographics 6 (1): 1‑13. https://doi.org/10.1186/1476-072X-6-4.\n\n\nApparicio, Philippe, Jérémy Gelb, Anne-Sophie Dubé, Simon Kingham, Lise Gauvin et Éric Robitaille. 2017. « The approaches to measuring the potential spatial access to urban health services revisited: distance types and aggregation-error issues. » International journal of health geographics 16 (1): 32. https://doi.org/10.1186/s12942-017-0105-9.\n\n\nApparicio, Philippe et Anne-Marie Séguin. 2006. « Measuring the accessibility of services and facilities for residents of public housing in Montreal. » Urban studies 43 (1): 187‑211. http://dx.doi.org/10.1080/00420980500409334.\n\n\nDijkstra, Edsger Wybe. 1959. « A note on two problems in connexion with graphs:(Numerische Mathematik, 1 (1959), p 269-271). » Stichting Mathematisch Centrum.\n\n\nGilardi, Andrea et Robin Lovelace. 2022. osmextract: Download and Import Open Street Map Data Extracts. s.n. https://CRAN.R-project.org/package=osmextract.\n\n\nGuagliardo, Mark F. 2004. « Spatial accessibility of primary care: concepts, methods and challenges. » International journal of health geographics 3 (1): 1‑13. https://doi.org/10.1186/1476-072x-3-3.\n\n\nHewko, Jared, Karen E Smoyer-Tomic et M John Hodgson. 2002. « Measuring neighbourhood spatial accessibility to urban amenities: does aggregation error matter? » Environment and Planning A 34 (7): 1185‑1206. https://doi.org/10.1016/0038-0121(92)90004-O.\n\n\nHollister, Jeffrey, Tarak Shah, Alec L Robitaille, Marcus W Beck et Mike Johnson. 2023. elevatr: Access Elevation Data from Various APIs. s.n. https://github.com/jhollist/elevatr/.\n\n\nJepson, Victoria, Philippe Apparicio et Thi-Thanh-Hien Pham. 2022. « Environmental equity and access to parks in Greater Montreal: an analysis of spatial proximity and potential congestion issues. » Journal of Urbanism: International Research on Placemaking and Urban Sustainability: 1‑19. http://dx.doi.org/10.1080/17549175.2022.2150271.\n\n\nKhan, Abdullah A. 1992. « An integrated approach to measuring potential spatial access to health care services. » Socio-economic planning sciences 26 (4): 275‑287. https://doi.org/10.1016/0038-0121(92)90004-O.\n\n\nLuo, Wei et Yi Qi. 2009. « An enhanced two-step floating catchment area (E2SFCA) method for measuring spatial accessibility to primary care physicians. » Health & Place 15 (4). Elsevier: 1100‑1107. https://doi.org/10.1016/j.healthplace.2009.06.002.\n\n\nLuo, Wei et Fahui Wang. 2003. « Measures of spatial accessibility to health care in a GIS environment: synthesis and a case study in the Chicago region. » Environment and planning B: planning and design 30 (6): 865‑884. https://doi.org/10.1068/b29120.\n\n\nMcGrail, Matthew R. 2012. « Spatial accessibility of primary health care utilising the two step floating catchment area method: an assessment of recent improvements. » International Journal of Health Geographics 11. Springer: 1‑12. https://doi.org/10.1186/1476-072X-11-50.\n\n\nMcGrail, Matthew R et John S Humphreys. 2009. « Measuring spatial accessibility to primary care in rural areas: Improving the effectiveness of the two-step floating catchment area method. » Applied geography 29 (4): 533‑541. https://doi.org/10.1016/j.apgeog.2008.12.003.\n\n\nMorgan, Malcolm, Young Marcus, Robin Lovelace et Layik Hama. 2019. « OpenTripPlanner for R. » Journal of Open Source Software 4 (44): 1926. https://10.21105/joss.01926.\n\n\nNgui, André Ngamini et Philippe Apparicio. 2011. « Optimizing the two-step floating catchment area method for measuring spatial accessibility to medical clinics in Montreal. » BMC health services research 11 (1): 1‑12. https://doi.org/10.1186/1472-6963-11-166.\n\n\nPenchansky, Roy et J William Thomas. 1981. « The concept of access: definition and relationship to consumer satisfaction. » Medical care: 127‑140. https://doi.org/10.1097/00005650-198102000-00001.\n\n\nPereira, Rafael H. M., Marcus Saraiva, DanielHerszenhut, Carlos Kaue Vieira Braga et Matthew Wigginton Conway. 2021. « r5r: Rapid Realistic Routing on Multimodal Transport Networks with R5 in R. » Findings. https://doi.org/10.32866/001c.21262."
  },
  {
    "objectID": "06-AnalyseReseauEvenements.html#sec-061",
    "href": "06-AnalyseReseauEvenements.html#sec-061",
    "title": "6  Analyses d’évènements localisés sur un réseau",
    "section": "\n6.1 Pourquoi recourir à un réseau pour des méthodes d’analyse de répartition ponctuelle?",
    "text": "6.1 Pourquoi recourir à un réseau pour des méthodes d’analyse de répartition ponctuelle?\nLes méthodes classiques d’analyse de répartition ponctuelle postulent que l’espace analysé est le plus souvent en deux dimensions, homogène et sans limite dans toutes les directions. Toutefois, des phénomènes se produisent dans des espaces pour lesquels ces hypothèses sont totalement invalidées, menant à l’obtention de résultats biaisés, voire incohérents. C’est notamment le cas d’évènements localisés le long des segments d’un réseau qui sont par exemple :\n\nDes accidents de la route se produisent nécessairement le long des axes routiers.\nDes fuites d’eau se produisent le long des canalisations d’une ville.\nDes interruptions de service de bus se produisent le long de lignes de transport collectif.\nCertaines espèces d’oiseaux nichent systématiquement le long de cours d’eau.\n\nContrairement à un espace géographique classique en deux dimensions, il est possible de considérer qu’un réseau géographique (c’est-à-dire un réseau dont les nœuds et les lignes ont des coordonnées spatiales) est un espace en 1,5 dimension puisqu’il n’est possible de se déplacer que le long des lignes et de ne changer de direction qu’au niveau d’un nœud (Steenberghen, Aerts et Thomas 2010). Cette distinction pose trois problèmes principaux si nous utilisons les méthodes de répartition ponctuelle classiques.\nPremièrement, la distance euclidienne (à vol d’oiseau) tend systématiquement à sous-estimer la distance réelle entre deux points. En effet, la longueur d’un trajet sur un réseau est toujours plus grande ou égale à la distance euclidienne. La figure 6.1 illustre ce premier problème avec un cas simple comprenant trois points sur un réseau.\n\n\n\n\nFigure 6.1: Problèmes générés par la non-prise en compte du réseau : sous-estimation des distances\n\n\n\nDeuxièmement, la non-prise en compte des lignes du réseau peut mener à analyser des secteurs dans lesquels les évènements ne peuvent pas se produire. La figure 6.2 illustre un cas où nous tenterions de produire un nouveau jeu de points distribués aléatoirement (points bleus), mais en respectant la densité initiale des points réels (points rouges). Ce type de méthode est notamment utilisé pour déterminer si un ensemble de points est plus ou moins concentré comparativement à ce que le hasard produirait. Comparer des points sur le réseau à des points hors du réseau conduit à surévaluer la concentration des points sur le réseau, car il y a plus d’espace en dehors du réseau que sur le réseau.\n\n\n\n\nFigure 6.2: Problèmes générés par la non-prise en compte du réseau : surestimation de la concentration des points\n\n\n\nTroisièmement, la masse des évènements ne se propage pas dans un espace en 2D comme dans un espace en 1,5D (figure 6.3). Dans un réseau, la masse des évènements doit être divisée aux intersections. Si cette division n’est pas prise en compte, alors la masse des évènements est dupliquée aux intersections. Cette problématique se pose particulièrement aux méthodes d’estimation de densité par noyau que nous décrirons plus tard dans ce chapitre.\n\n\n\n\nFigure 6.3: Problèmes générés par la non-prise en compte du réseau : la masse des évènements\n\n\n\nDe nombreuses méthodes d’analyse de répartition ponctuelle ont été adaptées pour pouvoir être appliquées avec des réseaux géographiques (Okabe et Sugihara 2012). Dans ce chapitre, nous abordons les méthodes de densité par noyau sur un réseau (network kernel density estimation, NKDE) et la création de matrices de pondération spatiale avec des distances sur un réseau pour calculer des mesures d’autocorrélation spatiale."
  },
  {
    "objectID": "06-AnalyseReseauEvenements.html#sec-062",
    "href": "06-AnalyseReseauEvenements.html#sec-062",
    "title": "6  Analyses d’évènements localisés sur un réseau",
    "section": "\n6.2 Cartographie de la densité d’évènements sur un réseau",
    "text": "6.2 Cartographie de la densité d’évènements sur un réseau\nÀ la section 3.4.2, nous avons décrit les méthodes d’analyse de densité dans une maille régulière : carte de chaleur ou estimation de la densité par noyau (kernel density estimation – KDE). Pour un rappel, une KDE peut être utilisée pour tenter de reconstruire un processus spatial produisant des évènements. Le processus spatial en lui-même est impossible à mesurer, mais nous tentons de le reconstruire et de l’approximer en nous basant sur les évènements observés qui sont des réalisations du processus sous-jacent. Sur un réseau, la logique est exactement la même : un processus spatial qui est invisible conduit à la réalisation d’évènements le long des lignes d’un réseau géographique.\n\n6.2.1 Estimation de la densité des points sur un réseau\nL’estimation de la densité sur un réseau (Network Kernel Density Estimation – NKDE) utilise une approche similaire à la KDE (section 3.4.2). L’idée générale est de répartir la masse des évènements le long des lignes du réseau autour de chaque évènement et d’additionner ensuite ces densités pour obtenir une estimation locale de l’intensité du processus spatial générant ces évènements. Comparativement à la KDE, les spécificités de la NKDE sont les suivantes :\n\nLes intensités sont calculées non pas sur des pixels, mais sur leurs équivalents appelés lixels. Un lixel correspond à simplement une portion de segment de ligne du réseau d’une longueur déterminée (50 mètres par exemple).\nLes distances sont calculées sur le réseau et non à vol d’oiseau.\n\nComme pour la KDE, il faut déterminer la valeur du rayon d’influence (bandwidth) et choisir une fonction kernel.\n\n6.2.1.1 Trois formes de NKDE\nLa NKDE peut prendre trois formes différentes traitant différemment la répartition de la masse aux intersections dans un réseau.\nNKDE géographique (Geo-NKDE). Il s’agit du cas le plus simple, car aucun traitement particulier n’est réalisé aux intersections du réseau et la densité d’un lixel est simplement basée sur la distance entre le centre de ce lixel et un évènement. Cette méthode est intuitive et peu coûteuse en temps de calcul, mais elle produit des résultats biaisés. En effet, si la masse d’un évènement se propage de façon continue dans toutes les directions à une intersection, alors la masse est multipliée par le nombre de directions possibles. Par exemple, à la figure 6.4, qui comprend une intersection avec trois segments connectés, la masse totale est égale à 150 % et non à 100 %. Ainsi, dans un réseau avec de nombreuses intersections, l’intensité est systématiquement surestimée avec cette méthode.\n\n\nFigure 6.4: Répartition de la masse avec une NKDE géographique\n\nUn aperçu en 3D de la répartition de la masse est disponible à la figure ci-dessous avec un seul évènement.\n\n3D plot\n\n\n\nLa formule pour calculer la Geo-NKDE est :\n\\[\n\\hat{\\lambda}_h(u)=\\frac{1}{h} \\sum_{i=1}^N k\\left(\\frac{\\operatorname{dist}_{\\text {net }}\\left(u, e_i\\right)}{h}\\right) \\text{ avec :}\n\\tag{6.1}\\]\n\n\n\\(\\hat{\\lambda}_h(u)\\), l’estimation de l’intensité au point \\(u\\) avec la bandwidth \\(h\\).\n\n\\(N\\), le nombre d’évènements.\n\n\\(k\\), une fonction kernel.\n\n\\(\\operatorname{dist}_{\\text{net}}\\left(u, e_i\\right)\\), la distance réseau entre la localisation \\(u\\) et l’évènement \\(e_i\\).\n\nNKDE discontinue (ESD-NKDE). Cette seconde méthode impose que la masse des évènements soit divisée aux intersections par le nombre de directions possible. En procédant ainsi, il est possible d’éviter le biais de la Geo-NKDE, mais cela conduit à une estimation discontinue de la NKDE. En effet, l’intensité d’un évènement chute fortement au détour d’une intersection ce qui est contre-intuitif en géographie bien que l’intensité produite soit non biaisée (voir la figure ci-dessous). Comme pour la précédente, le second avantage de cette méthode est qu’elle est peu coûteuse en temps de calcul.\n\n3D plot\n\n\n\nLa formule pour calculer la ESD-NKDE est la suivante :\n\\[\n\\hat{\\lambda}_h\\left(u, e_i\\right)=k\\left(d_{i s t_{n e t}}\\left(u, e_i\\right)\\right) \\prod_{j=1}^J\\left(\\frac{1}{\\left(n_{i j}-1\\right)}\\right) \\text{ avec :}\n\\tag{6.2}\\]\n\n\n\\(\\prod_{j=1}^J\\left(\\frac{1}{\\left(n_{i j}-1\\right)}\\right)\\) est le terme qui permet de contrôler la réduction de la masse due aux \\(J\\) intersections rencontrées entre \\(u\\) et \\(e_i\\) et ayant un nombre d’embranchements \\(n_{ij}\\).\n\nNKDE continue (ESC-NKDE). La troisième méthode implique également de diviser la masse des évènements aux intersections, mais aussi de corriger rétroactivement la masse précédant l’intersection pour forcer l’estimation à être continue. Cette estimation est donc non biaisée et ne produit pas de discontinuité (voir la figure ci-dessous). Cependant, la correction rétroactive nécessite un temps de calcul nettement plus long que les deux précédentes méthodes.\n\n3D plot\n\n\n\nDu fait de sa nature récursive, il est difficile de présenter la ESC-NKDE avec une équation. Cependant, l’algorithme complet est décrit par Atsuyuki Okabe et Sugihara Kokichi (2012).\nLa figure 6.5 permet de comparer la répartition de la masse des évènements aux intersections entre les NKDE, tandis que le tableau 6.1 résume les avantages et inconvénients des trois types de NKDE.\n\n\nFigure 6.5: Comparaison des trois types de NDKDE : géographique (Geo-NKDE), discontinue (ESD-NKDE) et continue (ESC-NKDE)\n\n\n\n\n\nTableau 6.1: Comparaison des trois NKDE\n\n\n\n\n\n\nNKDE\nAvantage\nDésavantage\n\n\n\nNKDE géographique (Geo-NKDE)\nIntuitive et facile à calculer\nLa somme de la masse totale est inexacte.\n\n\nNKDE discontinue (ESD-NKDE)\nRespect de la masse totale et facile à calculer\nL’espace discontinu est contre-intuitif\n\n\nNKDE continue (ESC-NKDE)\nRespect de la masse totale et intuitive\nCouteux en termes de temps de calcul\n\n\n\n\n\n\n\n\n\n\n\nPackage spNetwork\n\n\nPour une lecture plus détaillée sur les trois NKDE, nous vous recommandons de lire l’article décrivant le package spNetwork (Gelb 2021).\n\n\n\n6.2.1.2 Correction de Diggle\nTrès souvent, les données collectées pour un phénomène analysé sont limitées à une zone géographique (territoire d’étude) et les évènements se produisant en dehors de cette zone ne sont pas enregistrés. Ce biais de collecte entraîne une sous-estimation systématique de l’intensité estimée par les méthodes KDE et NKDE aux frontières de la zone d’étude. Pour limiter cette sous-estimation, il est préférable de collecter directement les données dans un périmètre plus large que la zone étudiée. Cependant, lorsque les données ont déjà été collectées, il est possible d’appliquer la correction de Diggle (1985) (équation 6.3).\n\\[\n\\begin{gathered}\n\\lambda^D(u)=\\frac{1}{bw} \\sum_{i=1}^n w_i \\cdot \\frac{1}{e\\left(e_i\\right)} K\\left(\\operatorname{dist}\\left(u, e_i\\right)\\right) \\\\\ne(u)=\\int_W^v K(\\operatorname{dist}(u, v)) \\text{ avec :}\n\\end{gathered}\n\\tag{6.3}\\]\n\n\n\\(e(u)\\) étant la masse de l’évènement \\(u\\) localisé dans la zone d’étude \\(W\\).\n\nConcrètement, cette correction propose d’augmenter la masse des évènements localisés à proximité de la frontière de la zone d’étude. Ces évènements voient leur pondération multipliée par l’inverse de la proportion de leur masse comprise à l’intérieur de la zone d’étude. Ainsi, un point dont 100 % de la masse se trouve dans la zone d’étude garde le même poids (1/1 = 1); un point avec 75 % de sa masse dans la zone d’étude a son poids multiplié par 4/3 (1/0,75 = 4/3) et un point avec 50 % de sa masse dans la zone d’étude a son poids multiplié par deux (1/0,5 = 2).\n\n6.2.1.3 Bandwidths adaptatives\nJusqu’ici, nous avons présenté les méthodes d’estimation de la densité par kernel avec des bandwidths globales. Il existe une catégorie de kernels appelés adaptatifs qui utilisent, comme leur nom l’indique, des bandwidths s’adaptant localement.\nL’idée générale est que chaque évènement peut avoir sa propre bandwidth locale. Cette modification se justifie du point de vue théorique. Pour un rappel, nous considérons que les évènements ont eu lieu à un certain endroit du fait d’un patron spatial d’arrière-plan. Nous formulons donc l’hypothèse qu’un évènement aurait pu se produire dans un certain rayon (bandwidth) autour de son emplacement réel selon une probabilité décroissante avec la distance (fonction kernel). Dans les secteurs où se situent de nombreux points, notre incertitude sur la localisation d’un point est moins grande, nous pouvons donc utiliser des bandwidths plus petites. Cependant, dans les secteurs avec très peu de points, le processus spatial est beaucoup plus diffus et l’évènement aurait pu se produire dans un rayon plus large, incitant à utiliser des bandwidths plus grandes.\nDeux approches sont le plus souvent utilisées pour créer des bandwidths locales : la méthode d’Abramson (1982) et la méthode des k plus proches voisins (Orava 2011).\n\n6.2.1.3.1 Bandwidths adaptatives par la méthode d’Abramson\nLa méthode d’Abramson (1982) propose d’utiliser des bandwidths locales qui sont inversement proportionnelles à la racine carrée de l’intensité locale du processus spatial étudié. Cependant, puisque nous ne disposons pas d’une mesure de ce processus, nous devons en fournir une approximation à priori. Cette approximation est obtenue en sélectionnant une première bandwidth globale (appelée pilote) et l’estimation de la densité est effectuée. Une fois que la densité à priori est calculée à la localisation exacte de chaque évènement, il est ensuite possible de calculer une bandwidth locale pour chaque évènement avec l’équation 6.4 :\n\\[\nh(e_{i}) = h_{0} \\times \\frac{1}{\\sqrt{\\tilde{f}h_{0}(e_{i})}} \\times \\frac{1}{\\gamma_{f}}\\\\\n\\gamma_{f} = \\exp(\\frac{\\sum_{i}log(\\frac{1}{\\sqrt{\\tilde{f}h_{0}(e_{i})}})}{n}) \\text{ avec :}\n\\tag{6.4}\\]\n\n\n\\(h(e_{i})\\) est la bandwidth locale pour l’évènement \\(e_{i}\\).\n\n\\(h_0\\) est la bandwidth pilote globale.\n\n\\(\\tilde{f}h_{0}(e_{i})\\) est l’estimation locale de la densité à priori pour l’évènement \\(e_{i}\\) avec \\(h_0\\).\n\nL’objectif est bien évidemment de sélectionner la bandwidth pilote \\(h_0\\).\n\n6.2.1.3.2 Bandwidths adaptatives par la méthode des k plus proches voisins\nCette méthode est certainement plus facile à expliquer. Elle consiste à calculer, pour chaque évènement, la distance qui le sépare de son plus proche voisin de rang k (k étant un entier plus grand que 0). Dans des secteurs avec peu d’évènements, la distance au plus proche voisin de rang k sera plus grande. L’enjeu pour cette méthode est donc de déterminer k. La figure 6.6 illustre l’impact de ces méthodes sur les bandwidths locales en prenant le jeu de données fourni dans le package spNetwork, portant sur les accidents à vélo dans les quartiers centraux de Montréal.\n\n\n\n\nFigure 6.6: Comparaison des deux principales méthodes de création de bandwidths locales\n\n\n\n\n6.2.1.4 Sélection d’une bandwidth\n\nComme signalé dans la section 3.4.2.1, le choix de la bandwidth est crucial dans l’application d’une estimation de densité par kernel. Dans le cas de la NKDE, le nombre de méthodes est plus limité que pour la KDE, mais il est toujours possible d’utiliser l’approche par validation croisée des probabilités (likelihood cross validation). Plus exactement, cette méthode choisit une bandwidth de façon à minimiser l’impact qu’aurait le fait de retirer un évènement du jeu de donnée (leave one out cross validation). En effet, puisque chaque point est une réalisation d’un processus spatial, le fait de retirer un point des données ne devrait affecter que marginalement l’estimation locale de l’intensité du processus spatial. Il est possible de minimiser ce score en sélectionnant la bonne bandwidth. Il est possible de calculer ce score pour une bandwidth donnée avec l’équation 6.5 :\n\\[\n\\operatorname{LCV}(bw)=\\sum_i \\log \\hat{\\lambda}_{-i}\\left(x_i\\right)\n\\text{ avec :}\n\\tag{6.5}\\]\n\n\n\\(bw\\) est la bandwidth à évaluer.\n\n\\(\\hat{\\lambda}_{-i}\\) est l’intensité estimée à la localisation de l’évènement i sans la présence de l’évènement i.\n\nNotez qu’il s’agit d’une simplification de l’équation qui comporte normalement un second terme qui tend à être une constante et peut donc être retiré pour alléger les calculs (Loader 2006). Cette méthode peut aussi être utilisée pour sélectionner la bandwidth pilote ou k lorsque nous utilisons une bandwidth adaptative.\n\n6.2.2 Mise en œuvre dans R\nNous analysons ici les collisions ayant eu lieu sur le réseau routier de la ville de Sherbrooke. Nous commençons par appliquer une NKDE avec bandwidth fixe et nous la comparons avec deux NKDE utilisant des bandwidths adaptatives. Nous utilisons principalement le package spNetwork (Gelb 2021).\nLa première étape consiste à charger les données des accidents et le réseau routier. La figure 6.7 permet de visualiser la répartition spatiale des accidents.\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(ggplot2)\nlibrary(spNetwork)\nlibrary(future) # package utilisé pour accélérer les calculs dans spNetwork\nfuture::plan(future::multisession(workers = 5))\n## Importation des couches géographiques\nroutes &lt;- st_read('data/chap01/shp/Segments_de_rue.shp', quiet = TRUE)\ncollisions &lt;- st_read('data/chap04/DataAccidentsSherb.shp', quiet = TRUE)\n## Application de la même projection\nroutes &lt;- st_transform(routes, 2949)\ncollisions &lt;- st_transform(collisions, 2949)\nroutes &lt;- sf::st_cast(routes, 'LINESTRING')\n## Cartographie des données des collisions et du réseau routier\ntm_shape(routes) + \n  tm_lines('grey20') + \n  tm_shape(collisions) + \n  tm_dots('red', size = 0.05)+\ntm_layout(frame = FALSE)  \n\n\n\nFigure 6.7: Accidents sur le réseau routier de la ville de Sherbrooke\n\n\n\nPour l’analyse, nous utilisons la fonction kernel quadratique et la NKDE continue (ESC-NKDE). Puis, nous choisissons une bandwidth avec l’approche par validation croisée des probabilités. Notez que pour réduire le temps de calcul, la NKDE discontinue (ESD-NKDE) est utilisée dans la phase de sélection de la bandwidth, car il est bien plus rapide à calculer.\n\neval_bandwidth &lt;- bw_cv_likelihood_calc.mc(\n  bw_range = c(100,1200),\n  bw_step = 50,\n  lines = routes, \n  events = collisions,\n  w = rep(1, nrow(collisions)), # le poids de chaque évènement est 1\n  kernel_name = 'quartic',\n  method = 'discontinuous',\n  adaptive = FALSE,\n  max_depth = 10,\n  digits = 1,\n  tol = 0.1,\n  agg = 5, # les accidents dans un rayon de 5 mètres seront agrégés\n  grid_shape = c(5,5),\n  verbose = TRUE)\n\nÀ la figure 6.8, nous constatons qu’au-delà de 900 mètres, le gain obtenu en augmentant la valeur de la bandwidth est marginal. Par conséquent, nous retenons cette valeur de bandwidth pour la première estimation de l’intensité des accidents.\n\n## Graphique pour les bandwidths \nggplot(eval_bandwidth) + \n  geom_path(aes(x = bw, y = cv_scores)) + \n  geom_point(aes(x = bw, y = cv_scores), color = 'red')+\n  labs(x = \"Valeur de la bandwidth\", y = \"Valeur du CV\")\n\n\n\nFigure 6.8: Scores des bandwidths globales\n\n\n\n\n## Création des lixels d'une longueur de 100 mètres\nlixels &lt;- lixelize_lines(routes, 100, mindist = 50)\n## Centroïdes des lixels\nlixels_centers &lt;- spNetwork::lines_center(lixels)\n\n\n## Calcul de la NKDE\nintensity &lt;- nkde.mc(lines = routes,\n                     events = collisions,\n                     w = rep(1, nrow(collisions)),\n                     samples = lixels_centers,\n                     kernel_name = 'quartic',\n                     bw = 900,\n                     adaptive = FALSE,\n                     method = 'continuous',\n                     max_depth = 8,\n                     digits = 1,\n                     tol = 0.1,\n                     agg = 5,\n                     verbose = FALSE,\n                     grid_shape = c(5,5))\n\nUne fois que les valeurs de densité sont obtenues, nous pouvons les cartographier à l’échelle des lixels (figure 6.9).\n\nlixels$density &lt;- intensity * 1000\ntm_shape(lixels) + \n  tm_lines(\"density\", lwd = 1.5, n = 7, style = \"fisher\",\n           legend.format = list(text.separator = \"à\"))+\n  tm_layout(frame=FALSE)\n\n\n\nFigure 6.9: Densité des accidents sur le réseau routier de Sherbrooke\n\n\n\nNous pouvons à présent utiliser une bandwidth adaptative. Pour cela, nous devons réévaluer les différentes bandwidths globales avec l’approche par validation croisée des probabilités.\n\neval_bandwidth_adapt &lt;- bw_cv_likelihood_calc.mc(\n  bw_range = c(100,1200),\n  bw_step = 50,\n  lines = routes, \n  events = collisions,\n  w = rep(1, nrow(collisions)), # le poids de chaque évènement sera 1\n  kernel_name = 'quartic',\n  method = 'discontinuous',\n  adaptive = TRUE,\n  trim_bws = seq(100,1200,50) * 2,\n  max_depth = 10,\n  digits = 1,\n  tol = 0.1,\n  agg = 5, # tous les accidents dans un rayon de 5m seront agrégés\n  grid_shape = c(5,5),\n  verbose = TRUE\n)\n\n\nggplot() + \n  geom_path(data = eval_bandwidth,\n            mapping =  aes(x = bw, y = cv_scores)) + \n  geom_point(data = eval_bandwidth,\n             mapping =  aes(x = bw, y = cv_scores), color = 'red') + \n  geom_path(data = eval_bandwidth_adapt,\n            mapping =  aes(x = bw, y = cv_scores)) + \n  geom_point(data = eval_bandwidth_adapt,\n             mapping =  aes(x = bw, y = cv_scores), color = 'blue')+\n  labs(x = \"Valeur de la bandwidth\", y = \"Valeur du CV\")\n\n\n\nFigure 6.10: Scores des bandwidths adaptatives\n\n\n\nLa figure 6.10 indique que les scores obtenus par les bandwidths adaptatives sont systématiquement supérieurs à ceux obtenus par les bandwidths fixes. Nous gardons une bandwidth pilote de 900 mètres pour recalculer notre ESC-NKDE avec une bandwidth adaptative.\n\nintensity_adpt &lt;- nkde.mc(lines = routes,\n                  events = collisions,\n                  w = rep(1, nrow(collisions)),\n                  samples = lixels_centers,\n                  kernel_name = 'quartic',\n                  bw = 900,\n                  adaptive = TRUE,\n                  trim_bw = 1800,\n                  method = 'continuous',\n                  max_depth = 8,\n                  digits = 1,\n                  tol = 0.1,\n                  agg = 5,\n                  verbose = TRUE,\n                  grid_shape = c(5,5))\n\n\nlixels$density_adpt &lt;- intensity_adpt$k * 1000\ntm_shape(lixels) + \n  tm_lines(\"density_adpt\", lwd = 1.5, n = 7, style = \"fisher\",\n           legend.format = list(text.separator = \"à\"))+\n  tm_layout(frame=FALSE)\n\n\n\nFigure 6.11: Densité des accidents sur le réseau routier de Sherbrooke avec une bandwidth adaptative\n\n\n\nComparativement aux résultats obtenus avec les bandwidths fixes, nous constatons que le lissage est beaucoup plus faible et que les points chauds sont plus faciles à identifier. Le package spNetwork permet aussi d’utiliser la méthode des k plus proches voisins comme bandwidth locale.\n\nknn_net &lt;- spNetwork::network_knn(collisions,\n                                      lines = routes,\n                                      k = 30,\n                                      maxdistance = 3000,\n                                      grid_shape = c(1,1),\n                                      verbose = FALSE)\ndist_mat &lt;- knn_net$distances\n# Nous limitons les bandwidths avec des bornes de 100 à 3000 m\ndist_mat &lt;- ifelse(dist_mat &gt; 3000, 3000, dist_mat)\ndist_mat &lt;- ifelse(dist_mat &lt; 100, 100, dist_mat)\n\n\neval_bandwidth_knearest &lt;- bw_cv_likelihood_calc(\n  bws = NULL,\n  mat_bws = dist_mat, \n  lines = routes, \n  events = collisions,\n  w = rep(1, nrow(collisions)), # le poids de chaque évènement sera 1\n  kernel_name = 'quartic',\n  method = 'discontinuous',\n  adaptive = TRUE,\n  trim_bws = NULL,\n  max_depth = 10,\n  digits = 1,\n  tol = 0.1,\n  agg = 5, # tous les accidents dans un rayon de 5 mètres seront agrégés\n  grid_shape = c(5,5),\n  verbose = TRUE\n)\n\n\neval_bandwidth_knearest$knn &lt;- 1:30\nggplot() + \n  geom_path(data = eval_bandwidth_knearest,\n            mapping =  aes(x = knn, y = cv_scores)) + \n  geom_point(data = eval_bandwidth_knearest,\n             mapping =  aes(x = knn, y = cv_scores), color = 'red')+\n  labs(x = \"Nombre de plus proches voisins (k)\", y = \"Valeur du CV\")\n\n\n\nFigure 6.12: Scores des bandwidths adaptatives par k plus proches voisins\n\n\n\nLa figure 6.12 indique que le meilleur score est obtenu pour une bandwidth allant jusqu’au 16e voisin.\n\nintensity_adpt_knn &lt;- nkde.mc(lines = routes,\n                  events = collisions,\n                  w = rep(1, nrow(collisions)),\n                  samples = lixels_centers,\n                  kernel_name = 'quartic',\n                  bw = dist_mat[,16],\n                  trim_bw = 1800,\n                  method = 'continuous',\n                  max_depth = 8,\n                  digits = 1,\n                  tol = 0.1,\n                  agg = 5,\n                  verbose = TRUE,\n                  grid_shape = c(5,5))\n\n\nlixels$density_adpt_knn &lt;- intensity_adpt_knn * 1000\ntm_shape(lixels) + \n  tm_lines(\"density_adpt_knn\", lwd = 1.5, n = 7, style = \"fisher\",\n           legend.format = list(text.separator = \"à\"))+\n  tm_layout(frame=FALSE)\n\n\n\nFigure 6.13: Densité des accidents sur le réseau routier de Sherbrooke avec une bandwidth adaptative pour 16 plus proches voisins\n\n\n\n\n6.2.3 Estimation de la densité spatio-temporelle sur un réseau\nComme nous avons pu le voir dans la section 3.4.3, il est courant d’analyser des données d’évènements disposant à la fois d’une localisation dans l’espace et dans le temps. Il est alors possible de calculer une densité spatio-temporelle, soit de lisser les évènements à la fois dans la dimension spatiale et dans la dimension temporelle. Plus exactement, la densité d’un évènement i en un point p et un instant t correspond au produit de la densité spatiale et de la densité temporelle de i.\nCette extension est aussi valide dans le cas de l’analyse d’évènement sur réseau.\n\\[\n\\hat{\\lambda}_{h_n h_t}\\left(u_{n t}\\right)=\\frac{1}{h_n h_t} \\sum_{i=1}^N k_{n e t}\\left(\\frac{\\operatorname{dist}_{n e t}\\left(u_{n t}, e_i\\right)}{h_n}\\right) \\sum_{i=1}^N k_{\\text {time }}\\left(\\frac{\\operatorname{dist}_{\\text {time }}\\left(u_{n t}, e_i\\right)}{h_t}\\right)\n\\text{ avec :}\n\\]\n\n\n\\(h_t\\) et \\(h_n\\) les bandwidths temporelle et réseau.\n\n\\(u_{nt}\\) un évènement localisé au point n du réseau et à l’instant t dans le temps.\n\nLa figure 6.14 illustre le calcul de la TNKDE, soit l’estimation de la densité spatio-temporelle sur un réseau (Temporal Network Kernel Density Estimate).\n\n\nFigure 6.14: La TNKDE comme produit des densités spatiale et temporelle\n\nComme pour le NKDE, il est possible de :\n\nUtiliser des bandwidths variant localement dans l’espace et le temps.\nComparer des bandwidths par des méthodes de validation croisée.\nAppliquer des correctifs aux frontières spatio-temporelles de la zone d’étude.\n\n\n6.2.3.1 Application dans R\nNous reprenons simplement l’exemple de la section sur la NKDE et voir comment l’étendre au contexte spatio-temporel. Pour cela, nous utiliserons principalement le package spNetwork.\n\nlibrary(sf)\nlibrary(spNetwork)\nlibrary(lubridate)\nlibrary(metR)\nlibrary(future) # pour accélérer les calculs de spNetwork\nfuture::plan(future::multisession(workers = 5))\n\nroutes &lt;- st_read('data/chap01/shp/Segments_de_rue.shp', quiet = TRUE)\ncollisions &lt;- st_read('data/chap04/DataAccidentsSherb.shp', quiet = TRUE)\n## Préparation de la colonne avec les dates\ncollisions$dt &lt;- as_date(collisions$DATEINCIDE)\ncollisions$dt_num &lt;- as.numeric(collisions$dt - min(collisions$dt))\n## Reprojection dans le même système\nroutes &lt;- st_transform(routes, 32187)\ncollisions &lt;- st_transform(collisions, 32187)\nroutes &lt;- sf::st_cast(routes, 'LINESTRING')\nroutes$length &lt;- st_length(routes)\n## Préparation des routes et des lixels\nroutes &lt;- sf::st_cast(routes, 'LINESTRING')\n\nNous commençons par compléter le réseau routier. En effet, certaines sections sont isolées et forment des enclaves inaccessibles. Nous avons ignoré cette problématique jusqu’ici, mais nous verrons comment retirer les petites enclaves déconnectées de la partie principale du réseau. Pour cela, nous commençons par créer un objet de type graph à partir des routes avec le package spNetwork.\n\nlibrary(igraph)\nlibrary(dbscan)\nlibrary(tmap)\nroutes &lt;- sf::st_cast(routes, 'LINESTRING')\nroutes$length &lt;- st_length(routes)\ngraph &lt;- spNetwork::build_graph(routes, digits = 2,line_weight = \"length\")\nparts &lt;- components(graph$graph)\ngraph$spvertices$part &lt;- as.character(parts$membership)\ntm_shape(graph$spvertices) + \n  tm_dots(\"part\", size = 0.1)\n\n\n\nFigure 6.15: Visualisation des composantes du réseau routier\n\n\n\nÀ la figure 6.15, nous pouvons identifier la partie principale du réseau et les segments déconnectés (éléments avec des valeurs supérieures à 1). Puis, nous soustrayons ces éléments du réseau pour alléger le graphe et éviter d’associer des collisions avec des parties inaccessibles du réseau routier.\n\nmain_component &lt;- subset(graph$spvertices, graph$spvertices$part == \"1\")\nmain_network &lt;- subset(graph$spedges, \n                   (graph$spedges$start_oid %in% main_component$id) | \n                     (graph$spedges$end_oid %in% main_component$id)\n                   )\nmain_network &lt;- subset(main_network, as.numeric(st_length(main_network)) &gt; 0)\n\nMaintenant que nous avons nettoyé notre réseau, nous calculons les scores pour les bandwidths.\n\nlixels_main &lt;- lixelize_lines(main_network, 100, mindist = 50)\nlixels_main_centers &lt;- spNetwork::lines_center(lixels_main)\n# Calcul des scores pour les bandwidths\ncv_scores_tnkde &lt;- bws_tnkde_cv_likelihood_calc(\n  bws_net = seq(700,1500,100),\n  bws_time =  seq(10,40,5),\n  lines = main_network,\n  events = collisions,\n  time_field = \"dt_num\",\n  w = rep(1, nrow(collisions)),\n  kernel_name = \"quartic\",\n  method = \"continuous\",\n  max_depth = 10,\n  digits = 2,\n  tol = 0.1,\n  agg = 10,\n  grid_shape = c(5,5),\n  verbose = TRUE)\n\n\n# Création d'un graphique pour visualiser les résultats\nlibrary(ggplot2)\ndf2 &lt;- reshape2::melt(cv_scores_tnkde)\nggplot(df2) + \n  geom_tile(aes(x = Var1, y = Var2, fill = value)) + \n  geom_contour(aes(x = Var1, y = Var2, z = value),\n               breaks = c(-400,-300, -250, -200, -180, -150),\n               color = 'white', linetype = 'dashed')+\n  scale_fill_viridis_c() + \n  labs(x = \"Bandwidth spatiale (mètres)\", \n       y = \"Bandwidth temporelle (jours)\", \n       fill = \"cv score\") +\n  coord_fixed(ratio=30)\n\n\n\nFigure 6.16: Scores obtenus pour différentes combinaisons de bandwidths spatiales et temporelles\n\n\n\nLa figure 6.16 indique clairement que des bandwidths plus larges produisent de meilleurs résultats. Pour éviter d’avoir des résultats trop lissées, nous choisisons dans un premier temps la paire de bandwidths 30 jours et 1500 m. Puisque le temps de calcul peut être assez long compte tenu de la longueur des bandwidths (supérieure à 1 km), nous continuons donc à utiliser une NKDE discontinue.\n\n# Choix de la résolution temporelle (dix jours ici)\nsample_time &lt;- seq(0, max(collisions$dt_num), 10)\n# Calcul des densités\ntnkde_densities &lt;- tnkde.mc(lines = main_network,\n                   events = collisions,\n                   time_field = \"dt_num\",\n                   w = rep(1, nrow(collisions)), \n                   samples_loc = lixels_main_centers,\n                   samples_time = sample_time, \n                   kernel_name = \"quartic\",\n                   bw_net = 1500, bw_time = 30,\n                   adaptive = TRUE,\n                   trim_bw_net = 1800,\n                   trim_bw_time = 60,\n                   method = \"continuous\",\n                   div = \"bw\", max_depth = 10,\n                   digits = 2, tol = 0.01,\n                   adaptive_separate = FALSE,\n                   agg = 10, grid_shape = c(5,5), \n                   verbose  = TRUE)\n\nOn peut à présent représenter notre TNKDE avec une carte animée!\n\nlibrary(classInt)\nlibrary(viridis)\nall_times &lt;- min(collisions$dt) + days(sample_time)\ntnkde_densities$k &lt;- tnkde_densities$k*10000\ntnkde_densities$k &lt;- ifelse(tnkde_densities$k &lt; 0, 0, tnkde_densities$k)\ncolor_breaks &lt;- classIntervals(c(tnkde_densities$k), n = 10, style = \"kmeans\")\nall_maps &lt;- lapply(1:length(all_times), function(i){\n  dens &lt;- tnkde_densities$k[,i]\n  dt &lt;- all_times[[i]]\n  lixels_main$dens &lt;- dens\n  lixels2 &lt;- lixels_main[order(-1*lixels_main$dens),]\n  map &lt;- tm_shape(lixels2) + \n    tm_lines(\"dens\", breaks = color_breaks$brks,\n             palette = mako(10,direction = -1), lwd = 2) + \n    tm_layout(frame = FALSE, legend.show=FALSE,\n              main.title = as.character(all_times[[i]]))\n  return(map)\n})\n# Création d'une animation pour produire la carte animée\ntmap_animation(all_maps, filename = \"images/Chap06/animated_TNKDE_sherbrooke.gif\", \n               width = 1000, height = 1000, dpi = 150, delay = 50)\n\n\n\nFigure 6.17: Densité spatio-temporelle des collisions routières à Sherbrooke"
  },
  {
    "objectID": "06-AnalyseReseauEvenements.html#sec-063",
    "href": "06-AnalyseReseauEvenements.html#sec-063",
    "title": "6  Analyses d’évènements localisés sur un réseau",
    "section": "\n6.3 Mesure d’autocorrélation spatiale sur un réseau",
    "text": "6.3 Mesure d’autocorrélation spatiale sur un réseau\nDans le chapitre 3, nous avons présenté plusieurs mesures d’autocorrélation spatiale globales et locales. À titre de rappel, ces mesures utilisent une matrice \\(W\\) indiquant les relations spatiales (voisinage, distance, interaction) entre les observations. Lorsque nous analysons des observations sur un réseau, l’utilisation de matrices spatiales basées sur les distances réseaux plutôt qu’euclidiennes permet de représenter plus fidèlement l’organisation spatiale des observations. Voici quelques exemples de matrices de pondération spatiale construites à partir de distances calculées sur un réseau :\n\nMatrice de connectivité selon la distance (section 2.2.2.2) : deux observations sont considérées comme voisines si la longueur du plus court chemin qui les sépare est inférieure au seuil de distance maximal fixé.\nMatrice des k plus proches voisins (section 2.2.2.4) : une observation a uniquement pour voisins les k autres observations les plus proches.\nMatrices basées sur la distance (section 2.2.2.3) : chaque observation est voisine de toutes les autres observations et le poids accordé à une paire d’observations dans la matrice est obtenu en appliquant une fonction décroissante (inverse de la distance, inverse de la distance au carrée, inverse de l’exponentielle, etc.) à la longueur du plus court chemin entre les deux observations. Si la fonction renvoie une pondération de 0, alors les deux observations sont trop éloignées pour être considérées comme voisines.\n\nCes matrices peuvent ensuite être standardisées en ligne tel que décrit dans la section 2.2.3.\nUn bon exemple d’utilisation de ce type de matrice est l’évaluation de l’autocorrélation spatiale de l’intensité estimée par la méthode NKDE vu dans la section 6.1. En effet, la NKDE est une méthode essentiellement descriptive. Par conséquent, appliquer une mesure classique d’autocorrélation spatiale locale, comme les statistiques locales de Getis et Ord (section 2.4.1) ou la typologie basée sur le diagramme de Moran dans un contexte univarié (section 2.4.3.1), permet d’identifier les points chauds et froids de densité d’événemements statistiquement significatifs. Cette approche a notamment été proposée par Ikuho Yamada et Jean-Claude Thill (2007) sous le nom de ILINCS (I Local Indicators of Network-Constrained Clusters), mais ces auteurs utilisaient à l’époque une GEO-NKDE. Nous reprenons donc l’exemple précédent avec l’ESC-NKDE estimée pour les collisions routières à Sherbrooke et calculons le I de Moran global et une mesure d’autocorrélation spatiale locale sur les lixels, soit la typologie basée sur le diagramme de Moran.\n\n6.3.1 Mise en œuvre dans R\nLa première étape consiste à créer une matrice de pondération spatiale entre les lixels sur le réseau routier de Sherbrooke. Nous testons plusieurs matrices pour trouver celle qui permet d’obtenir la plus haute valeur pour le I de Moran global.\nPour commencer, nous calculons simplement les distances réseau entre les lixels à l’aide de spNetwork. Notez que la fonction network_listw et sa version multicœur network_listw.mc renvoient un objet de type listw typique du package spdep. Il est aussi possible de l’utiliser pour simplement obtenir des distances (avec les paramètres dist_func = 'identity' et matrice_type = 'I') pour ensuite leur appliquer des fonctions spécifiques. Nous utilisons ici cette approche pour éviter d’avoir à calculer plusieurs fois les chemins plus courts entre les lixels. Pour réduire le temps de calcul de la matrice, nous fixons la distance maximale à 2500 mètres avec (paramètre maxdistance = 2500 de la fonction spNetwork::network_listw.mc).\n\nroutes &lt;- st_read('data/chap01/shp/Segments_de_rue.shp', quiet = TRUE)\nroutes &lt;- st_transform(routes, 2949)\nroutes &lt;- sf::st_cast(routes, 'LINESTRING')\nfuture::plan(future::multisession(workers = 5))\nnet_distances &lt;- spNetwork::network_listw.mc(\n  origins = lixels_centers, \n  lines = routes,\n  maxdistance = 2500,\n  mindist = 1,\n  dist_func = 'identity',\n  matrice_type = 'I',\n  grid_shape = c(5,5)\n)\n\nPremièrement, nous construisons plusieurs matrices de connectivité selon la distance standardisées en ligne, captant les voisins à moins de 250, 300, 500, 700 et 900 mètres (rappelons que les lixels ont une longueur de 100 m et que nous partons du centre des lixels pour le calcul des distances).\n\n# une liste qui permettra de stocker toutes les matrices\nall_matrices &lt;- list()\nbin_dists &lt;- c(250,300,500,700,900)\nfor(d in bin_dists){\n  new_weights &lt;- lapply(net_distances$weights, function(x){\n    return( (x &lt;= d) / sum(x &lt;= d))\n  })\n  net_distances_temp &lt;- net_distances\n  net_distances_temp$weights &lt;- new_weights\n  all_matrices[[paste0(\"dist_mat_\",d)]] &lt;- net_distances_temp\n}\n\nDeuxièmement, nous considérons un ensemble de matrices binaires standardisées en ligne captant les k plus proches voisins (de 3 à 10); il faut cependant que les distances entre les observations restent inférieures à 2500 mètres.\n\nfor(k in 3:10){\n  new_weights &lt;- lapply(net_distances$weights, function(x){\n    x_r &lt;- rank(x, ties.method = \"min\")\n    return( (x_r &lt;= k) / sum(x_r &lt;= k))\n  })\n  net_distances_temp &lt;- net_distances\n  net_distances_temp$weights &lt;- new_weights\n  all_matrices[[paste0(\"k_mat_\",k)]] &lt;- net_distances_temp\n}\n\nTroisièmement, nous construisons des matrices avec l’inverse de la distance, l’inverse de la distance au carrée et un kernel quadratique avec des seuils maximaux de la distance fixés à 250, 300, 500, 700 et 900 mètres.\n\n# Inverse de la distance\ninv_weights &lt;- lapply(net_distances$weights, function(x){\n  inv &lt;- (1/x) \n  return( inv / sum(inv) )\n})\nnet_distances_temp &lt;- net_distances\nnet_distances_temp$weights &lt;- inv_weights\nall_matrices[[\"inv_mat\"]] &lt;- net_distances_temp\n\ninv2_weights &lt;- lapply(net_distances$weights, function(x){\n  inv &lt;- (1/x**2) \n  return( inv / sum(inv) )\n})\n\n# Inverse de la distance au carré\nnet_distances_temp &lt;- net_distances\nnet_distances_temp$weights &lt;- inv2_weights\nall_matrices[[\"inv2_mat\"]] &lt;- net_distances_temp\n\nbin_dists &lt;- c(250,300,500,700,900)\n\nfor(d in bin_dists){\n  new_weights &lt;- lapply(net_distances$weights, function(x){\n    if(is.null(x) == FALSE){\n      w &lt;- spNetwork::quartic_kernel(x,d)\n      return( (w) / sum(w))\n    }\n    else{\n      return(NULL)\n    }\n  })\nnet_distances_temp &lt;- net_distances\nnet_distances_temp$weights &lt;- new_weights\nall_matrices[[paste0(\"dist_quartic_\",d)]] &lt;- net_distance\n}\n\nMaintenant que nous disposons de toutes ces matrices, nous pouvons calculer le I de Moran global pour chaque matrice.\n\nlibrary(spdep)\n## Calcul du I de Moran pour les différentes matrices\nmoran_vals &lt;- sapply(all_matrices, function(W){\n  # Petite conversion vers le type de spdep\n  attr(W,'class') &lt;- c(\"listw\", \"nb\")\n  W$style &lt;- \"W\"\n  W$neighbours &lt;- W$nb_list\n  W$nb_list &lt;- NULL\n  val &lt;- moran(lixels$density_adpt_knn, listw = W,\n        n = nrow(lixels),\n        S0 = nrow(lixels),\n        zero.policy = TRUE)\n  return(val$I)\n})\n## Enregistrement dans un dataframe\ndf_moran &lt;- data.frame(\n  Matrices = names(all_matrices),\n  MoranIs = moran_vals\n)\n## Réalisation d'un graphique\nggplot(data=df_moran, aes(x=reorder(Matrices,MoranIs), y=MoranIs)) +\n  geom_segment( aes(x=reorder(Matrices,MoranIs), \n                    xend=reorder(Matrices,MoranIs), \n                    y=0, yend=MoranIs)) +\n  geom_point( size=4,fill=\"red\",shape=21)+\n  xlab(\"Matrice de pondération spatiale sur le réseau\") +\n  ylab(\"I de Moran\")+\n  coord_flip()\n\n\n\nFigure 6.18: I de Moran obtenu pour différentes matrices de pondération spatiale sur réseau\n\n\n\nÀ la figure 6.18, nous constatons que la matrice utilisant les trois plus proches voisins obtient la valeur du I de Moran la plus élevé. Nous la conservons pour calculer la mesure d’autocorrélation spatiale locale basée sur la typologie basée sur le diagramme de Moran (figure 6.19).\n\nW &lt;- all_matrices$k_mat_3\nattr(W,'class') &lt;- c(\"listw\", \"nb\")\nW$style &lt;- \"W\"\nW$neighbours &lt;- W$nb_list\nW$nb_list &lt;- NULL\nlocal_I &lt;- localmoran(lixels$density_adpt, listw = W, zero.policy = TRUE)\nlixels$loc_classes &lt;- attributes(local_I)$quadr$mean\nlixels$loc_p &lt;- local_I[,5]\nlixels$loc_classes2 &lt;- case_when(\n  lixels$loc_p &lt; 0.01 ~ lixels$loc_classes,\n  TRUE ~ 'non sign.'\n)\nCouleurs &lt;- c(\"High-High\" = \"#FF0000\",\n              \"Low-Low\" =\"#0000FF\",\n              \"High-Low\" = \"#f4ada8\",\n               \"Low-High\" =\"#a7adf9\",\n              'non sign.' = \"#eeeeee\")\ntm_shape(lixels) + \n  tm_lines('loc_classes2', palette = Couleurs,\n           lwd = 1.2, title.col = \"Typologie\")\n\n\n\nFigure 6.19: Typologie basée sur le diagramme de Moran\n\n\n\n\n\n\n\n\nMesures d’autocorrélation spatiale globale et locale sur un réseau\n\n\nAu chapitre 2, nous avons vu une panoplie de mesures d’autocorrélation spatiale globale (section 2.3) et locale (section 2.4) calculées avec différentes matrices de pondération spatiale (section 2.2).\nOr, n’importe quelle mesure d’autocorrélation spatiale peut être calculée à partir d’une matrice de pondération spatiale construite à partir des distances réseau. Retenez que le choix de la matrice reste un enjeu important puisqu’elle affecte significativement les résultats comme illustré à la figure 6.18."
  },
  {
    "objectID": "06-AnalyseReseauEvenements.html#sec-064",
    "href": "06-AnalyseReseauEvenements.html#sec-064",
    "title": "6  Analyses d’évènements localisés sur un réseau",
    "section": "\n6.4 DBSCAN sur un réseau",
    "text": "6.4 DBSCAN sur un réseau\nDans la section 4.1.1, Nous avons vu l’algorithme DBSCAN détectant des agrégats de points dans l’espace. Cet algorithme de classification non supervisée basée sur la densité des points identifie des agrégats de points à partir de deux paramètres : un rayon de recherche (\\(\\epsilon\\), epsilon) et un nombre minimum de points (\\(MinPts\\)). Il est assez facile d’adapter cette méthode afin que le rayon de recherche ne soit pas basée sur la distance euclidienne, mais plutôt sur la distance réseau. Cela est particulièrement pertinent lors les évènements sont localisés sur un réseau, comme des accidents routiers.\n\n6.4.1 Mise en œuvre dans R\nPour illustrer la version réseau du DBSCAN, nous reprenons l’exemple de la section 4.1.3 basé sur un jeu de données sur les incidents de sécurité publique survenus sur le territoire de la ville de Sherbrooke de juillet 2019 à juin 2022.\nTout d’abord, nous importons les couches géographiques des accidents et du réseau routier (figure 6.20).\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(spNetwork)\nlibrary(dbscan)\nlibrary(ggplot2)\nlibrary(spdep)\n## Importation des accidents\nAccidents.sf &lt;- st_read(dsn = \"data/chap04/DataAccidentsSherb.shp\", quiet=TRUE)\n## Importation des routes (réseau)\nroutes &lt;- st_read('data/chap01/shp/Segments_de_rue.shp', quiet=TRUE)\n# reprojection dans le même système\nroutes &lt;- st_transform(routes, 32187)\nAccidents.sf &lt;- st_transform(Accidents.sf, 32187)\nroutes &lt;- sf::st_cast(routes, 'LINESTRING')\ntm_shape(routes) + \n  tm_lines('black') + \n  tm_shape(Accidents.sf) + \n  tm_dots('red', size = 0.2)\n\n\n\nFigure 6.20: Accidents sur le réseau de la ville de Sherbrooke\n\n\n\nPuis, nous recherchons la valeur optimale d’epsilon (\\(\\epsilon\\)) avec un nombre minimal de quatre points (\\(MinPts\\)) pour former un agrégat. À la lecture de la figure 6.21, nous constatons que le coude se situe certainement entre 750 et 1250 mètres.\n\n\n\n\n\nMatrice de pondération spatiales des distances réseau avec spNetwork et spdep\n\n\nDans le code ci-dessous, les distances réseau sont obtenues avec le package spNetwork, sous forme d’un objet de type listw. Elles sont ensuite converties en matrice avec la fonction listw2mat du package spdep, et enfin en objet de type distance avec la fonction as.dist. Si le nombre de points à analyser est très grand, la matrice de distances pourrait excéder la mémoire vive disponible dans votre ordinateur.\n\n\n\n## Calcul des distances pour les quatre plus proches voisins\nknn_dists &lt;- network_knn(origins = Accidents.sf,\n                         lines = routes,\n                         k = 4,\n                         maxdistance = 5000,\n                         grid_shape = c(1,1),\n                         verbose = FALSE)\n## Graphique pour la distance au quatrième voisin le plus proche\ndists4 &lt;- knn_dists$distances[,4]\ndists4 &lt;- dists4[order(dists4)]\nDistKplusproche &lt;- data.frame(\n  distance = dists4,\n  id = 1:length(dists4)\n)\nggplot(data = DistKplusproche)+\n  geom_path(aes(x = id, y = distance), size=1)+\n  labs(x = \"Points triés par ordre croissant selon la distance\",\n       y = \"Distance au quatrième point le plus proche\")+\n  geom_hline(yintercept=250, color = \"#08306b\", linetype=\"dashed\", size=1)+\n  geom_hline(yintercept=500, color = \"#00441b\", linetype=\"dashed\", size=1)+\n  geom_hline(yintercept=1000, color = \"#67000d\", linetype=\"dashed\", size=1)+\n  geom_hline(yintercept=1500, color = \"#3f007d\", linetype=\"dashed\", size=1)\n\n\n\nFigure 6.21: Optimisation de la valeur d’epsilon pour les accidents sur réseau\n\n\n\nLa comparaison des figures 6.22, 6.23, 6.24 et 6.25 montre clairement qu’au-delà de 1000 mètres, les résultats sont assez peu intéressants, car presque tous les points sont agrégés dans le même groupe.\n\n## Calcul des distances réseau entre chaque évènement\nnetwork_dists &lt;- network_listw(origins = Accidents.sf,\n                               lines = routes,\n                               maxdistance = 5000,\n                               mindist = 1,\n                               dist_func = \"identity\",\n                               matrice_type = \"I\",\n                               grid_shape = c(1,1))\n\nnetwork_dists$neighbours &lt;- network_dists$nb_list\nnetwork_dists_matrix &lt;- listw2mat(network_dists)\n\n# 0 signifie que la distance entre les deux points \n# est plus grande que le paramètre maxdistance\nnetwork_dists_matrix &lt;- ifelse(network_dists_matrix == 0,\n                               5000, network_dists_matrix)\nnetwork_dists_matrix &lt;- as.dist(network_dists_matrix)\n\n# Algorithme dbscan avec les différentes valeurs d'epsilon\nresult250 &lt;- dbscan(network_dists_matrix, eps = 250, minPts = 4)\nresult500 &lt;- dbscan(network_dists_matrix, eps = 500, minPts = 4)\nresult1000 &lt;- dbscan(network_dists_matrix, eps = 1000, minPts = 4)\nresult1500 &lt;- dbscan(network_dists_matrix, eps = 1500, minPts = 4)\nAccidents.sf$gp_250 &lt;- as.character(result250$cluster)\nAccidents.sf$gp_500 &lt;- as.character(result500$cluster)\nAccidents.sf$gp_1000 &lt;- as.character(result1000$cluster)\nAccidents.sf$gp_1500 &lt;- as.character(result1500$cluster)\n\n# Cartographie\ntmap_options(max.categories = 50)\ntm_shape(Accidents.sf)+tm_dots(col=\"gp_250\", title = \"DBSCAN 250\", size = .5)\n\n\n\nFigure 6.22: Résultats obtenus pour le dbscan avec un valeur d’epsilon de 250 mètres\n\n\n\n\ntm_shape(Accidents.sf)+tm_dots(col=\"gp_500\", title = \"DBSCAN 500\", size = .5)\n\n\n\nFigure 6.23: Résultats obtenus pour le dbscan avec un valeur d’epsilon de 500 mètres\n\n\n\n\ntm_shape(Accidents.sf)+tm_dots(col=\"gp_1000\", title = \"DBSCAN 1000\", size = .5)\n\n\n\nFigure 6.24: Résultats obtenus pour le dbscan avec un valeur d’epsilon de 1000 mètres\n\n\n\n\ntm_shape(Accidents.sf)+tm_dots(col=\"gp_1500\", title = \"DBSCAN 1500\", size = .5)\n\n\n\nFigure 6.25: Résultats obtenus pour le dbscan avec un valeur d’epsilon de 1500 mètres"
  },
  {
    "objectID": "06-AnalyseReseauEvenements.html#sec-065",
    "href": "06-AnalyseReseauEvenements.html#sec-065",
    "title": "6  Analyses d’évènements localisés sur un réseau",
    "section": "\n6.5 Quiz de révision du chapitre",
    "text": "6.5 Quiz de révision du chapitre\n\n\n\n\n\nQuels problèmes posent l’utilisation des méthodes classiques d’analyse spatiale pour des données se produisant sur un réseau?\n\n\nRelisez au besoin la section 6.1.\n\n\n\n\n\n\nLa distance euclidienne tend à sous-estimer les distances réelles.\n\n\n\n\n\n\n\nLa distance euclidienne tend à surestimer les distances réelles.\n\n\n\n\n\n\n\nLe réseau est un espace comportant davantage de dimensions qu’un espace planaire.\n\n\n\n\n\n\n\nLe réseau est un espace ne respectant pas les hypothèses d’homogénéité et d’isotropie de l’espace planaire.\n\n\n\n\n\n\n\nLes méthodes classiques analysent des portions de l’espace en dehors du réseau.\n\n\n\n\n\n\n\n\n\n\nPour adapter la méthode classique du Kernel Density Estimate, il est nécessaire de :\n\n\nRelisez au besoin le début de la section 6.2.\n\n\n\n\n\n\nUtiliser une distance réseau plutôt qu’euclidienne.\n\n\n\n\n\n\n\nN’estimer les densités que le long des lixels représentant des fragments réguliers du réseau.\n\n\n\n\n\n\n\nAdapter la fonction de Kernel pour tenir compte de la division de la masse aux intersections.\n\n\n\n\n\n\n\nAppliquer une pénalité aux portions du réseau avec peu d’évènements.\n\n\n\n\n\n\n\n\n\n\nPour une NKDE, il est uniquement possible de calculer des densités selon une bandwidth fixe.\n\n\nRelisez au besoin la section 6.2.1.3.\n\n\n\n\n\n\nVrai\n\n\n\n\n\n\n\nFaux\n\n\n\n\n\n\n\n\n\n\nContrairement à la KDE classique, la NKDE est moins sensible au choix de la bandwidth.\n\n\nRelisez au besoin la section 6.2.1.4.\n\n\n\n\n\n\nVrai\n\n\n\n\n\n\n\nFaux\n\n\n\n\n\n\n\n\n\n\nL’extension spatio-temporelle du NKDE (TNKDE) consiste à :\n\n\nRelisez au besoin la section 6.2.1.\n\n\n\n\n\n\nCalculer une somme des densités spatiales et temporelles des évènements le long du réseau.\n\n\n\n\n\n\n\nCalculer le produit des densités spatiales et temporelles des évènements le long du réseau.\n\n\n\n\n\n\n\nCalculer l’intégrale des densités spatiales et temporelles des évènements le long du réseau.\n\n\n\n\n\n\n\n\n\n\nPour une TNKDE, il est uniquement possible de calculer des densités selon une bandwidth fixe.\n\n\nRelisez au besoin la section 6.2.1.\n\n\n\n\n\n\nVrai\n\n\n\n\n\n\n\nFaux\n\n\n\n\n\n\n\n\n\n\nLes méthodes KDE, NKDE, et TNKDE peuvent être vues comme :\n\n\nRelisez au besoin la section 6.2.\n\n\n\n\n\n\nDes méthodes descriptives visant à résumer un grand ensemble d’évènements en une carte de chaleur plus facilement interprétable.\n\n\n\n\n\n\n\nDes méthodes inférentielles permettant de déterminer statistiquement la présence de points chauds ou de points froids.\n\n\n\n\n\n\n\nDes algorithmes de type boite noire réduisant la dimensionnalité des données.\n\n\n\n\n\n\n\n\n\n\nL’utilisation d’une distance réseau plutôt qu’euclidienne pour un algorithme DBSCAN risque de :\n\n\nRelisez au besoin la section 6.4.\n\n\n\n\n\n\nAugmenter la taille des groupes détectés par l’algorithme, car la recherche de voisin sera plus facile sur le réseau\n\n\n\n\n\n\n\nRéduire la taille des groupes formés par l’algorithme, car la distance euclidienne tend à sous-estimer les distances réelles sur le réseau\n\n\n\n\n\n\n\n\n\nVérifier votre résultat"
  },
  {
    "objectID": "06-AnalyseReseauEvenements.html#sec-066",
    "href": "06-AnalyseReseauEvenements.html#sec-066",
    "title": "6  Analyses d’évènements localisés sur un réseau",
    "section": "\n6.6 Exercices de révision",
    "text": "6.6 Exercices de révision\n\n\n\n\n\nExercice 1. Réalisation d’un graphique pour trouver la valeur de la bandwidth optimale\n\n\nUtilisez la fonction kernel quadratique et la NKDE continue (ESC-NKDE), contruisez un graphique pour choisir une bandwidth avec l’approche par validation croisée des probabilités. Complétez le code ci-dessous.\n\nlibrary(sf)\nlibrary(spNetwork)\nlibrary(future)\n\nfuture::plan(future::multisession(workers = 5))\n# Importation des données sur les collisions cycles et le réseau de rues\nCollisions &lt;- st_read(dsn = \"data/chap06/Mtl/DonneesMTL.gpkg\", layer=\"CollisionsAvecCyclistes\", quiet=TRUE)\nReseauRues &lt;- st_read(dsn = \"data/chap06/Mtl/DonneesMTL.gpkg\", layer=\"Rues\", quiet=TRUE)\nReseauRues$LineID &lt;- 1:nrow(ReseauRues)\nLongueurKm &lt;- sum(as.numeric(st_length(ReseauRues)))/1000\nCollisions &lt;- st_transform(Collisions, st_crs(ReseauRues))\ncat(\"Informations sur les couches\",\n    \"\\n  Collisions avec cylistes :\", nrow(Collisions),\n    \"\\n  Réseau :\", round(LongueurKm,3), \"km\")\n# Cartographie\ntmap_mode(\"view\")\ntm_shape(ReseauRues) + tm_lines(\"black\") +\n  tm_shape(Collisions) + tm_dots(\"blue\", size = 0.025)+\ntm_scale_bar(c(0,1,2), position = 'left')+\n  tm_layout(frame = FALSE)\n## Évaluation des bandwidths de 100 à 1200 avec un saut de 50\neval_bandwidth &lt;- bw_cv_likelihood_calc.mc(à compléter)\n## Graphique pour les bandwidths \nà compléter\n\nCorrection à la section 9.6.1.\n\n\n\n\n\n\n\nExercice 2. Réalisation d’une NKDE continue.\n\n\nComplétez le code ci-dessous pour réaliser une NKDE continue avec un fonction kernel quadratique et une valeur de bandwidth de 500 mètres.\n\nlibrary(sf)\nlibrary(spNetwork)\nlibrary(future)\n## Création des lixels d'une longueur de 100 mètres\nlixels &lt;- lixelize_lines(ReseauRues, 100, mindist = 50)\nlixels_centers &lt;- spNetwork::lines_center(lixels)\n## Calcul de la NKDE continue\nintensity &lt;- nkde.mc(À compléter)\nlixels$density &lt;- intensity * 1000\n## Cartographie\nÀ compléter\n\nCorrection à la section 9.6.2.\n\n\n\n\n\n\nAbramson, Ian S. 1982. « On bandwidth variation in kernel estimates-a square root law. » The annals of Statistics: 1217‑1223. https://www.jstor.org/stable/2240724.\n\n\nDiggle, Peter. 1985. « A kernel method for smoothing point process data. » Journal of the Royal Statistical Society: Series C (Applied Statistics) 34 (2). Wiley Online Library: 138‑147.\n\n\nGelb, Jeremy. 2021. « spNetwork: A Package for Network Kernel Density Estimation. » The R Journal 13 (2): 561‑577. https://doi.org/10.32614/RJ-2021-102.\n\n\nLoader, Clive. 2006. Local regression and likelihood. Springer Science & Business Media.\n\n\nOkabe, Atsuyuki et Kokichi Sugihara. 2012. Spatial analysis along networks: statistical and computational methods. John Wiley & Sons.\n\n\nOrava, Jan. 2011. « K-nearest neighbour kernel density estimation, the choice of optimal k. » Tatra Mountains Mathematical Publications 50 (1): 39‑50.\n\n\nSteenberghen, Thérèse, Koen Aerts et Isabelle Thomas. 2010. « Spatial clustering of events on a network. » Journal of Transport Geography 18 (3): 411‑418. https://doi.org/10.1016/j.jtrangeo.2009.08.005.\n\n\nYamada, Ikuho et Jean-Claude Thill. 2007. « Local indicators of network-constrained clusters in spatial point patterns. » Geographical analysis 39 (3). Wiley Online Library: 268‑292. https://www.jstor.org/stable/40645354."
  },
  {
    "objectID": "07-RegressionSpatiales.html#sec-071",
    "href": "07-RegressionSpatiales.html#sec-071",
    "title": "7  Introduction aux modèles de régression spatiale",
    "section": "\n7.1 Modèles économétriques spatiaux",
    "text": "7.1 Modèles économétriques spatiaux\n\n\n\n\n\nRégression linéaire multiple et modèles économétriques spatiaux\n\n\nDans cette section, nous décrivons uniquement les modèles économétriques spatiaux dont la variable dépendante est continue. Sommairement, ces modèles sont des extensions de la régression linéaire multiple dans laquelle est intégrée l’autocorrélation spatiale. Avant de lire cette section, il faut donc bien maîtriser la régression linéaire multiple. Si ce n’est pas le cas, nous vous invitons vivement à lire le chapitre suivant (Apparicio et Gelb 2022).\nCes deux dernières décennies, plusieurs ouvrages traitant des modèles économétriques spatiaux ont été publiés, surtout en anglais (LeSage et Pace 2008; Anselin et Rey 2014; Bivand et al. 2008). Ils méritent grandement d’être consultés, tout comme l’excellent livre en français de Jean Dubé et Diègo Legros (2014).\n\n\nPourquoi recourir à des modèles économétriques spatiaux?\nDans un modèle, les résidus (\\(\\epsilon\\)) sont la différence entre les valeurs observées (\\(y_i\\)) et les valeurs prédites par le modèle (\\(\\widehat{y_i}\\)). Une des hypothèses de la régression linéaire multiple est que les observations doivent être indépendantes les unes des autres (indépendance du terme d’erreur). Le non-respect de cette hypothèse produit des résultats biaisés, notamment pour les coefficients de régression.\nLorsque les observations sont des entités spatiales (polygones, points par exemple), si les résidus du modèle sont autocorrélés spatialement, il y a un problème de dépendance spatiale du modèle. Autrement dit, les observations ne sont pas spatialement indépendantes les unes des autres. Pour vérifier la dépendance spatiale d’un modèle, il suffit de calculer le I de Moran sur les résidus du modèle, comme décrit au chapitre 2 (section 2.3).\nAutrement dit, un modèle de régression construit avec des données spatiales ne devrait pas avoir des résidus spatialement autocorrélés. Or, les modèles économétriques spatiaux permettent justement d’intégrer l’autocorrélation spatiale de différentes manières afin de s’assurer que l’hypothèse de l’indépendance du terme d’erreur est respectée.\n\n7.1.1 Bref retour sur la régression linéaire multiple\nÀ titre de rappel, la régression linéaire multiple permet de prédire et d’expliquer une variable dépendante (\\(Y\\)) en fonction de plusieurs variables indépendantes (\\(X\\)). L’équation de régression s’écrit alors :\n\\[\ny_i = \\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} +\\ldots+ \\beta_{k}x_{ki} + \\epsilon_{i}\n\\tag{7.1}\\]\navec :\n\n\n\\(y_i\\), la valeur de la variable dépendante Y pour l’observation i.\n\n\\(\\beta_{0}\\), la constante, soit la valeur prédite pour Y quand toutes les variables indépendantes sont égales à 0.\n\n\\(k\\) le nombre de variables indépendantes.\n\n\\(\\beta_{1}\\) à \\(\\beta_{k}\\), les coefficients de régression pour les variables indépendantes de 1 à k (\\(X_{1}\\) à \\(X_{k}\\)).\n\n\\(\\epsilon_{i}\\), le résidu pour l’observation de i, soit la partie de la valeur de \\(y_i\\) qui n’est pas expliquée par le modèle de régression.\n\nIl existe plusieurs écritures simplifiées de cette équation. Dans le cadre de ce chapitre, nous utilisons la forme matricielle suivante :\n\\[\ny = X\\beta + \\epsilon\n\\tag{7.2}\\]\navec :\n\n\n\\(y\\), un vecteur de dimension \\(n \\times 1\\) pour la variable dépendante, soit une colonne avec n observations.\n\n\\(X\\), une matrice de dimension \\(n \\times (k + 1)\\) pour les k variables indépendantes, incluant une autre colonne (avec la valeur de 1 pour les n observations) pour la constante, d’où \\(k + 1\\).\n\n\\(\\beta\\), un vecteur de dimension \\(k + 1\\), soit les coefficients de régression pour les k variables et la constante.\n\n\\(\\epsilon\\), un vecteur de dimension \\(n \\times 1\\) pour les résidus.\n\nConstruction du modèle MCO dans R\nAvec la fonction lm(), il est facile de construire un modèle de régression linéaire multiple basé sur la méthode des moindres carrés ordinaires (MCO). Dans le code ci-dessous, la formule de l’équation du modèle est donc NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed. Notez que la variable dépendante et les variables indépendantes sont séparées avec un tilde (~). Quant à la fonction summary(NomDuModèle), elle affiche les résultats du modèle.\n\n# Appel des différents packages utilisés dans le chapitre\nlibrary(spdep)\n## Construction du modèle\nModele.MCO &lt;- lm(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n                 data = LyonIris)\n## Résultats du modèle\nsummary(Modele.MCO)\n\n\nCall:\nlm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + \n    NivVieMed, data = LyonIris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.733  -4.457  -0.499   3.507  29.160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.43296    2.99550  16.502  &lt; 2e-16 ***\nPct0_14     -0.53352    0.06305  -8.461 2.94e-16 ***\nPct_65      -0.15047    0.05627  -2.674  0.00774 ** \nPct_Img      0.28287    0.05113   5.532 5.12e-08 ***\nPct_brevet  -0.24004    0.03721  -6.451 2.63e-10 ***\nNivVieMed   -0.31625    0.10180  -3.107  0.00200 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.685 on 500 degrees of freedom\nMultiple R-squared:  0.2832,    Adjusted R-squared:  0.276 \nF-statistic:  39.5 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nDépendance spatiale du modèle MCO?\nPour vérifier si ce modèle linéaire multiple a un problème de dépendance spatiale, nous calculons le I de Moran sur ses résidus avec la fonction lm.morantest, puis nous les cartographions.\n\n## Matrice de contiguïté selon le partage d'un segment (Rook)\nRook &lt;- poly2nb(LyonIris, queen=FALSE)\nW.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = \"W\")\n# Autocorrélation spatiale globale des résidus\nlm.morantest(Modele.MCO, W.Rook, alternative=\"two.sided\")\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet +\nNivVieMed, data = LyonIris)\nweights: W.Rook\n\nMoran I statistic standard deviate = 21.266, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nObserved Moran I      Expectation         Variance \n     0.587312061     -0.005375800      0.000776745 \n\n\nAvec une valeur du I de Moran de 0,587 (p &lt; 0,001), les résidus sont fortement autocorrélés spatialement, traduisant ainsi un problème de dépendance spatiale du modèle MCO et la nécessité de recourir à des modèles économétriques spatiaux. La cartographie des résidus à la figure 7.2 corrobore ce résultat.\n\nlibrary(tmap)\n## Ajout de la colonne dans LyonIris avec les valeurs des résidus\nLyonIris$ModeleMCO.Residus &lt;- residuals(Modele.MCO)\n## Cartographie\ntmap_mode(\"plot\")\ntm_shape(LyonIris)+\n  tm_fill(col=\"ModeleMCO.Residus\", n = 5, style = \"quantile\", \n          legend.format = list(text.separator = \"à\"),\n          palette = \"-RdBu\", title = \"Résidus\") +\n  tm_layout(frame=FALSE) + \n  tm_scale_bar(breaks = c(0,5))\n\n\n\nFigure 7.2: Cartographie des résidus du modèle de régression multiple\n\n\n\n\n7.1.2 Les différents modèles spatiaux autorégressifs\nSelon Jean Dubé et Diègo Legros, « cinq raisons peuvent motiver le choix d’un modèle autorégressif : la présence d’externalités, les effets d’entraînement, l’omission de variables importantes, la présence d’hétérogénéité spatiale des comportements, les effets mixtes » (2014, 120). Les effets mixtes peuvent être la combinaison d’externalités avec des effets d’entraînement ou encore d’externalités avec l’omission d’une ou de plusieurs variables importantes spatialement structurées.\n\n7.1.2.1 Modèle SLX : autocorrélation spatiale sur les variables indépendantes\nDans un modèle SLX, l’autocorrélation spatiale est intégrée au niveau des variables indépendantes. Autrement dit, les variables indépendantes spatialement décalées (\\(WX\\)) sont introduites aussi dans le modèle. Par conséquent, la valeur de chaque unité spatiale du modèle est ainsi expliquée à la fois par ses propres caractéristiques et celles dans le voisinage ou à proximité en fonction de la matrice de pondération spatiale (\\(W\\)).\n\n\n\n\n\nRappel sur les variables spatialement décalées\n\n\nDans le chapitre 2 sur l’autocorrélation spatiale, nous avons vu comment calculer une variable spatialement décalée avec une matrice de pondération spatiale (figure 2.29). À titre de rappel, lorsque cette dernière est standardisée en ligne, elle correspond à la valeur moyenne dans le voisinage.\n\n\nL’idée est alors d’introduire des externalités puisque les caractéristiques des entités spatiales proches ou voisines peuvent avoir un effet sur la variable dépendante (Dubé et Legros 2014). L’équation du modèle SLX, qui est estimée selon la méthode des moindres carrés ordinaires (comme la régression linéaire multiple), s’écrit alors :\n\\[\ny = X\\beta + WX\\theta + \\epsilon\n\\tag{7.3}\\]\navec :\n\n\n\\(y\\), la variable dépendante.\n\n\\(X\\), les variables indépendantes.\n\n\\(\\beta\\), les coefficients des variables indépendantes.\n\n\\(W\\), la matrice de pondération spatiale.\n\n\\(WX\\), les variables indépendantes spatiales décalées.\n\n\\(\\theta\\), les coefficients des variables indépendantes spatiales décalées.\n\n\\(\\epsilon\\), les résidus.\n\nConstruction du modèle SLX dans R\nLe modèle SLX est construit avec la fonction lmSLX du package spatialreg (Bivand, Millo et Piras 2021). Remarquez, dans le code ci-dessous, le paramètre listw=W.Rook qui est utilisé pour spécifier la matrice de pondération spatiale.\n\nlibrary(spatialreg)\n## Construction du modèle\nModele.SLX &lt;- lmSLX(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n                   listw=W.Rook,     # matrice de pondération spatiale\n                   data = LyonIris)  # dataframe\n## Résultats du modèle\nsummary(Modele.SLX)\n\n\nCall:\nlm(formula = formula(paste(\"y ~ \", paste(colnames(x)[-1], collapse = \"+\"))), \n    data = as.data.frame(x), weights = weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.6087  -3.5084  -0.7178   2.9128  26.8724 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    50.67786    4.18791  12.101  &lt; 2e-16 ***\nPct0_14        -0.20404    0.06268  -3.255  0.00121 ** \nPct_65         -0.03771    0.05361  -0.703  0.48217    \nPct_Img         0.10406    0.04849   2.146  0.03235 *  \nPct_brevet     -0.07363    0.03550  -2.074  0.03857 *  \nNivVieMed      -0.18441    0.11063  -1.667  0.09617 .  \nlag.Pct0_14    -0.77591    0.10295  -7.537 2.32e-13 ***\nlag.Pct_65     -0.06454    0.09115  -0.708  0.47924    \nlag.Pct_Img     0.64654    0.08593   7.524 2.53e-13 ***\nlag.Pct_brevet -0.30128    0.06157  -4.893 1.34e-06 ***\nlag.NivVieMed  -0.01805    0.17499  -0.103  0.91790    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.77 on 495 degrees of freedom\nMultiple R-squared:  0.4713,    Adjusted R-squared:  0.4606 \nF-statistic: 44.13 on 10 and 495 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\nEffets directs, indirects et totaux\n\n\nLa formulation d’un modèle SLX implique deux types d’effets pour les variables indépendantes (\\(X\\)) :\n\nles effets directs, soit ceux des caractéristiques des entités spatiales. Ils correspondent aux coefficients \\(\\beta\\) des variables indépendantes (\\(X\\)). Autrement dit, pour une observation \\(i\\), à chaque augmentation d’une unité d’une caractéristique \\(X\\), la valeur de \\(y_i\\) va varier (augmenter ou diminuer) en fonction du coefficient \\(\\beta\\).\nles effets indirects, soit ceux des caractéristiques des entités spatiales voisines ou proches définies selon la matrice de pondération spatiale. Ils correspondent aux coefficients \\(\\theta\\) des variables indépendantes spatialement décalées (\\(WX\\)). Autrement dit, les valeurs de \\(WX\\) des entités spatiales proches ou voisines \\(j\\) de \\(i\\) vont aussi être amenées à varier, impactant alors les valeurs \\(y_j\\) selon les coefficients \\(\\theta\\).\n\nPrenons l’exemple d’un modèle visant à prédire le prix de vente des maisons dans une ville en fonction de leurs caractéristiques des maisons, dont la superficie du jardin. Il est probable que plus la superficie du jardin de la maison \\(i\\) augmente, plus le prix de vente augmente également (effet direct, coefficient \\(\\beta\\)). Cette augmentation de la taille du jardin aura aussi un impact sur le prix des maisons voisines puisque leur prix est dépendant de la taille des jardins des maisons voisines. Ainsi, chaque maison \\(j\\), voisine de \\(i\\) verra son prix augmenter à cause de l’augmentation de la taille du jardin de la maison \\(i\\) (effet indirect).\nPour capturer l’impact total sur le prix des maisons d’une augmentation de la superficie du jardin de la maison \\(i\\), il suffit de sommer son effet direct (augmentation du prix de la maison \\(i\\)) et son effet indirect (augmentation du prix des maisons \\(j\\)) pour obtenir son effet total.\nLe code suivant permet de calculer ces effets directs et indirects.\n\n## Effets directs, indirects et totaux (uniquement les coefficients)\nimpacts(Modele.SLX)\n\nImpact measures (SlX, glht):\n                Direct    Indirect      Total\nPct0_14    -0.20403803 -0.77590830 -0.9799463\nPct_65     -0.03770918 -0.06453809 -0.1022473\nPct_Img     0.10406359  0.64653923  0.7506028\nPct_brevet -0.07363272 -0.30128171 -0.3749144\nNivVieMed  -0.18440960 -0.01804718 -0.2024568\n\n## Effets directs, indirects et totaux (coefficients, valeurs de z et de p)\nsummary(impacts(Modele.SLX))\n\nImpact measures (SlX, glht, n-k):\n                Direct    Indirect      Total\nPct0_14    -0.20403803 -0.77590830 -0.9799463\nPct_65     -0.03770918 -0.06453809 -0.1022473\nPct_Img     0.10406359  0.64653923  0.7506028\nPct_brevet -0.07363272 -0.30128171 -0.3749144\nNivVieMed  -0.18440960 -0.01804718 -0.2024568\n========================================================\nStandard errors:\n               Direct   Indirect      Total\nPct0_14    0.06268202 0.10295210 0.10045332\nPct_65     0.05361420 0.09114695 0.08556272\nPct_Img    0.04849085 0.08593145 0.08060028\nPct_brevet 0.03549819 0.06157121 0.05975821\nNivVieMed  0.11063207 0.17499339 0.14911021\n========================================================\nZ-values:\n               Direct   Indirect     Total\nPct0_14    -3.2551283 -7.5365951 -9.755241\nPct_65     -0.7033432 -0.7080664 -1.194998\nPct_Img     2.1460460  7.5238953  9.312658\nPct_brevet -2.0742665 -4.8932234 -6.273857\nNivVieMed  -1.6668729 -0.1031306 -1.357766\n\np-values:\n           Direct    Indirect   Total     \nPct0_14    0.0011334 4.8184e-14 &lt; 2.22e-16\nPct_65     0.4818419 0.47890    0.23209   \nPct_Img    0.0318693 5.3069e-14 &lt; 2.22e-16\nPct_brevet 0.0380546 9.9198e-07 3.5221e-10\nNivVieMed  0.0955397 0.91786    0.17454   \n\n\nÀ la lecture des valeurs de p, nous constatons que seule la variable Pct0_14 a un impact direct et indirect significatif au seuil 0,01. L’augmentation d’un point de pourcentage de la population de moins de 15 ans est associé localement à une réduction de 0,20 de la concentration annuelle du dioxyde d’azote. Chez les entités voisines, cette réduction est de 0,78 et l’effet total est donc une réduction de 0,98.\n\n\nDépendance spatiale du modèle SLX?\nCe modèle a-t-il corrigé le problème de dépendance spatiale du modèle de régression linéaire classique? Avec une valeur du I de Moran de 0,605 (p &lt; 0,001), les résidus sont toujours fortement autocorrélés spatialement (figure 7.3).\n\nlm.morantest(Modele.SLX, W.Rook, alternative=\"two.sided\")\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = formula(paste(\"y ~ \", paste(colnames(x)[-1],\ncollapse = \"+\"))), data = as.data.frame(x), weights = weights)\nweights: W.Rook\n\nMoran I statistic standard deviate = 21.951, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nObserved Moran I      Expectation         Variance \n    0.6046602748    -0.0072844321     0.0007771643 \n\nLyonIris$SLX.Residus &lt;- residuals(Modele.SLX)\ntm_shape(LyonIris)+\n  tm_fill(col=\"SLX.Residus\", n = 5, style = \"quantile\", \n          legend.format = list(text.separator = \"à\"),\n          palette = \"-RdBu\", title = \"Résidus\") +\n  tm_layout(frame=FALSE) + \n  tm_scale_bar(breaks = c(0,5))\n\n\n\nFigure 7.3: Cartographie des résidus du modèle SLX\n\n\n\n\n7.1.2.2 Modèle SAR : autocorrélation spatiale sur la variable dépendante\nDans le modèle SAR (aussi appelé SAR-LAG), l’autocorrélation spatiale est intégrée au niveau de la variable indépendante (\\(Wy\\)), qui est ainsi spatialement décalée. L’idée générale est que la valeur de la variable dépendante pour une observation (\\(y_i\\)) peut être influencée par les valeurs de \\(y\\) des observations voisines et proches. L’exemple le plus classique est le prix de vente des maisons : il est influencé à la fois par les caractéristiques intrinsèques de la maison (\\(X\\), par exemple, la superficie habitable, le nombre de chambres à coucher, de salles de bains, etc.) et par le prix de vente des maisons voisines (\\(Wy\\)). Jean Dubé et Diègo Legros (2014) qualifient ce phénomène « d’effets d’entraînement ou d’effets de débordement (spillover effects) » (2014, 123). L’équation du modèle SAR s’écrit alors :\n\\[\ny = Wy\\rho + X\\beta + \\epsilon\n\\tag{7.4}\\]\navec :\n\n\n\\(y\\), la variable dépendante.\n\n\\(W\\), la matrice de pondération spatiale.\n\n\\(Wy\\), la variable dépendante spatialement décalée.\n\n\\(\\rho\\) (prononcez rho), le coefficient de la variable dépendante spatialement décalée. Il varie de -1 à 1.\n\n\\(X\\), les variables indépendantes.\n\n\\(\\beta\\), les coefficients des variables indépendantes.\n\n\\(\\epsilon\\), les résidus.\n\nConstruction du modèle SAR dans R\nLe modèle SAR est construit avec la fonction lagsarlm du package spatialreg.\n\n## Construction du modèle\nModele.SAR &lt;- lagsarlm(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n                              listw=W.Rook,    # matrice de pondération spatiale\n                              data = LyonIris, # dataframe\n                              type = 'lag')    # Modèle lag par défaut\n## Résultats du modèle\nsummary(Modele.SAR, Nagelkerke=TRUE)\n\n\nCall:lagsarlm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + \n    NivVieMed, data = LyonIris, listw = W.Rook, type = \"lag\")\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-12.86859  -1.88111  -0.49760   0.94464  18.21351 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n             Estimate Std. Error z value  Pr(&gt;|z|)\n(Intercept)  7.838906   1.646232  4.7617 1.919e-06\nPct0_14     -0.098708   0.030554 -3.2306  0.001235\nPct_65      -0.034543   0.026957 -1.2814  0.200044\nPct_Img      0.030241   0.024491  1.2348  0.216917\nPct_brevet  -0.019234   0.017855 -1.0772  0.281384\nNivVieMed   -0.098413   0.048985 -2.0090  0.044534\n\nRho: 0.87939, LR test value: 620.31, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.01942\n    z-value: 45.283, p-value: &lt; 2.22e-16\nWald statistic: 2050.5, p-value: &lt; 2.22e-16\n\nLog likelihood: -1366.157 for lag model\nML residual variance (sigma squared): 10.181, (sigma: 3.1908)\nNagelkerke pseudo-R-squared: 0.78962 \nNumber of observations: 506 \nNumber of parameters estimated: 8 \nAIC: 2748.3, (AIC for lm: 3366.6)\nLM test for residual autocorrelation\ntest value: 0.6198, p-value: 0.43112\n\n\nDans les résultats ci-dessus, la valeur de rho est de 0,88 (LR = 620, p &lt; 0,001), traduisant un très fort effet d’entraînement. Autrement dit, lorsqu’en moyenne la concentration de dioxyde d’azote augmente dans les IRIS voisines (\\(Wy\\)), elle augmente aussi fortement chaque IRIS (\\(y\\)).\n\n\n\n\n\nEffets directs, indirects et totaux\n\n\nTout comme le modèle SLX vu précédemment, la formulation du modèle SAR-LAG implique des effets particuliers. Reprenons l’exemple d’un modèle prédisant le prix de vente des maisons avec cette fois-ci un modèle de type SAR-LAG :\n\nL’augmentation de la superficie du jardin de la maison \\(i\\) va faire augmenter le prix de la maison \\(i\\) (\\(y_i\\)).\nCette augmentation de prix de la maison \\(i\\) aura un impact sur les voisins de \\(i\\), soit les maisons \\(j\\), car leur prix dépend du prix de la maison \\(i\\) au travers du terme \\(Wy\\rho\\) du modèle. Par exemple, si \\(\\rho\\) vaut 0,8, alors 80% de l’augmentation du prix de \\(i\\) va se répercuter sur le prix des maisons \\(j\\).\nDe même, les voisines des maisons \\(j\\), les maisons \\(k\\) vont aussi être impactées par le changement de prix des maisons \\(j\\) et ainsi de suite de voisins en voisins.\nAu final, la maison \\(i\\) verra son prix augmenter encore plus, car le prix de ses voisines aura augmenté par effet de rétroaction.\n\nCe processus de propagation est appelé l’effet d’entraînement ou de débordement (spillover) en économétrie.\nL’effet original de l’augmentation de la taille du jardin sur la maison \\(i\\), combiné à l’augmentation par rétroaction, est appelé l’effet direct. L’effet cumulé de l’augmentation de la taille du jardin sur toutes les autres maisons (\\(\\neq i\\)) est appelé l’effet indirect. La somme des effets indirects et des directs est appelée effets totaux.\nÀ nouveau, il est possible d’utiliser la fonction impacts pour calculer ces effets directs et indirects.\n\n## Effets directs, indirects et totaux (uniquement les coefficients)\nimpacts(Modele.SAR, listw = W.Rook, R = 999)\n\nImpact measures (lag, exact):\n                Direct   Indirect      Total\nPct0_14    -0.13878038 -0.6796248 -0.8184052\nPct_65     -0.04856624 -0.2378349 -0.2864012\nPct_Img     0.04251743  0.2082131  0.2507306\nPct_brevet -0.02704205 -0.1324283 -0.1594703\nNivVieMed  -0.13836534 -0.6775923 -0.8159576\n\n## Effets directs, indirects et totaux (coefficients, valeurs de z et de p)\nsummary(impacts(Modele.SAR, listw = W.Rook, R = 999), zstats = TRUE, short = TRUE)\n\nImpact measures (lag, exact):\n                Direct   Indirect      Total\nPct0_14    -0.13878038 -0.6796248 -0.8184052\nPct_65     -0.04856624 -0.2378349 -0.2864012\nPct_Img     0.04251743  0.2082131  0.2507306\nPct_brevet -0.02704205 -0.1324283 -0.1594703\nNivVieMed  -0.13836534 -0.6775923 -0.8159576\n========================================================\nSimulation results ( variance matrix):\n========================================================\nSimulated standard errors\n               Direct  Indirect     Total\nPct0_14    0.04167115 0.2329616 0.2704515\nPct_65     0.03815213 0.1968362 0.2340270\nPct_Img    0.03270720 0.1676945 0.1996264\nPct_brevet 0.02515045 0.1276189 0.1523094\nNivVieMed  0.06864701 0.3524508 0.4181687\n\nSimulated z-values:\n              Direct  Indirect     Total\nPct0_14    -3.374576 -2.986158 -3.092173\nPct_65     -1.263313 -1.209912 -1.223587\nPct_Img     1.270413  1.223923  1.236293\nPct_brevet -1.037053 -1.002997 -1.011649\nNivVieMed  -2.020723 -1.936823 -1.964163\n\nSimulated p-values:\n           Direct    Indirect  Total   \nPct0_14    0.0007393 0.0028251 0.001987\nPct_65     0.2064767 0.2263128 0.221108\nPct_Img    0.2039374 0.2209813 0.216350\nPct_brevet 0.2997113 0.3158621 0.311706\nNivVieMed  0.0433084 0.0527670 0.049511\n\n\nL’interprétation des effets directs se rapproche de celle des coefficients classiques. Ainsi, selon ce modèle, l’augmentation du niveau de vie médian de 1000 € dans un IRIS est associée avec une diminution moyenne de la concentration de dioxyde d’azote de 0,14 dans cet IRIS. L’effet total est de -0.82, indiquant qu’en moyenne, l’augmentation de 1000 € du niveau de vie médian dans un IRIS est associée avec une diminution moyenne de 0,82 de la concentration de dioxyde d’azote dans l’ensemble des IRIS. Au final, l’effet indirect est simplement la différence entre l’effet total et l’effet direct. Nous pouvons constater ici que les effets indirects sont plus importants que les effets directs.\n\n\nDépendance spatiale du modèle SAR?\nCe modèle a-t-il corrigé le problème de dépendance spatiale du modèle de régression linéaire classique? Avec une valeur du I de Moran de -0,014 (p = 0,654), les résidus ne sont plus spatialement autocorrélés (figure 7.4).\n\n## Autocorrélation spatiale des résidus\nmoran.mc(resid(Modele.SAR), W.Rook, nsim=999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  resid(Modele.SAR) \nweights: W.Rook  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.014281, observed rank = 368, p-value = 0.632\nalternative hypothesis: greater\n\n## Cartographie des résidus\nLyonIris$SAR.Residus &lt;- resid(Modele.SAR)\ntm_shape(LyonIris)+\n  tm_fill(col=\"SAR.Residus\", n = 5, style = \"quantile\", \n          legend.format = list(text.separator = \"à\"),\n          palette = \"-RdBu\", title = \"Résidus\") +\n  tm_layout(frame=FALSE) + \n  tm_scale_bar(breaks = c(0,5))\n\n\n\nFigure 7.4: Cartographie des résidus du modèle SAR\n\n\n\n\n7.1.2.3 Modèle SEM : autocorrélation spatiale sur le terme d’erreur\nDans le modèle SEM (Spatial Error Model, appelé aussi SAR-ERROR), l’intégration de l’autocorrélation spatiale est réalisée sur le terme d’erreur, ce qui pourrait se justifier par l’omission d’une variable dépendante spatialement structurée (Dubé et Legros 2014, 126). L’équation du modèle SEM s’écrit :\n\\[\ny = X\\beta + u \\textrm{, } u = \\lambda Wu + \\epsilon\n\\tag{7.5}\\]\navec :\n\n\n\\(y\\), la variable dépendante.\n\n\\(W\\), la matrice de pondération spatiale.\n\n\\(\\lambda\\) (prononcez lambda), le coefficient sur le terme d’erreur spatialement décalé. Il varie de -1 à 1.\n\n\\(X\\), les variables indépendantes.\n\n\\(\\beta\\), les coefficients des variables indépendantes.\n\n\\(\\epsilon\\), les résidus.\n\nConstruction du modèle SAR dans R\nLe modèle SEM est construit avec la fonction lmSLX du package spatialreg.\n\n## Construction du modèle\nModele.SEM &lt;- errorsarlm(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n                         listw=W.Rook,    # matrice de pondération spatiale\n                         data = LyonIris) # dataframe\n## Résultats du modèle\nsummary(Modele.SEM, Nagelkerke=TRUE)\n\n\nCall:errorsarlm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + \n    NivVieMed, data = LyonIris, listw = W.Rook)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-12.86150  -1.83161  -0.44106   0.91029  17.94924 \n\nType: error \nCoefficients: (asymptotic standard errors) \n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 30.544576   2.358173 12.9526  &lt; 2e-16\nPct0_14     -0.035019   0.033393 -1.0487  0.29431\nPct_65      -0.026039   0.028970 -0.8988  0.36874\nPct_Img     -0.016770   0.026176 -0.6407  0.52175\nPct_brevet   0.023708   0.019074  1.2430  0.21388\nNivVieMed   -0.146309   0.060273 -2.4274  0.01521\n\nLambda: 0.91138, LR test value: 613.15, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.01651\n    z-value: 55.201, p-value: &lt; 2.22e-16\nWald statistic: 3047.2, p-value: &lt; 2.22e-16\n\nLog likelihood: -1369.737 for error model\nML residual variance (sigma squared): 9.9971, (sigma: 3.1618)\nNagelkerke pseudo-R-squared: 0.78662 \nNumber of observations: 506 \nNumber of parameters estimated: 8 \nAIC: NA (not available for weighted model), (AIC for lm: 3366.6)\n\n\nDans les résultats ci-dessus, la valeur de lambda est de 0,91 (LR = 613, p &lt; 0,001), traduisant une très forte autocorrélation spatiale sur le terme d’erreur.\nDépendance spatiale du modèle SEM?\nCe modèle a-t-il corrigé le problème de dépendance spatiale du modèle de régression linéaire classique? Avec une valeur du I de Moran de -0,013 (p = 0,614), les résidus ne sont plus spatialement autocorrélés.\n\n## Autocorrélation spatiale des résidus\nmoran.mc(resid(Modele.SEM), W.Rook, nsim=999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  resid(Modele.SEM) \nweights: W.Rook  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.011827, observed rank = 357, p-value = 0.643\nalternative hypothesis: greater\n\n\n\n7.1.2.4 Modèle SDM : autocorrélation spatiale sur la variable dépendante et les variables indépendantes\nLe modèle SDM (Spatial Durbin Model) est un modèle mixte qui intègre à la fois l’autocorrélation spatiale sur la variable dépendante (\\(Wy\\), effets d’entraînement ou de débordement) et sur les variables indépendantes (\\(WX\\), externalités). Il s’écrit alors :\n\\[\ny = Wy\\rho + X\\beta + WX\\theta + \\epsilon\n\\tag{7.6}\\]\navec :\n\n\n\\(y\\), la variable dépendante.\n\n\\(W\\), la matrice de pondération spatiale.\n\n\\(Wy\\), la variable dépendante spatialement décalée.\n\n\\(\\rho\\), le coefficient de la variable dépendante spatialement décalée.\n\n\\(X\\), les variables indépendantes.\n\n\\(\\beta\\), les coefficients des variables indépendantes.\n\n\\(WX\\), les variables indépendantes spatiales décalées.\n\n\\(\\theta\\), les coefficients des variables indépendantes spatiales décalées.\n\n\\(\\epsilon\\), les résidus.\n\nConstruction du modèle SDM dans R\nLe modèle SDM est construit avec la fonction lagsarlm du package spatialreg. Notez que le paramètre type = \"mixed\" spécifie l’utilisation d’un modèle mixte.\n\n## Construction du modèle\nModele.DurbinSpatial &lt;- lagsarlm(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n                       listw = W.Rook,    # matrice de pondération spatiale\n                       data = LyonIris,   # dataframe\n                       type = \"mixed\")\n## Résultats du modèles\nsummary(Modele.DurbinSpatial, Nagelkerke=TRUE)\n\n\nCall:lagsarlm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + \n    NivVieMed, data = LyonIris, listw = W.Rook, type = \"mixed\")\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-12.60922  -1.77753  -0.43909   0.99252  18.15526 \n\nType: mixed \nCoefficients: (asymptotic standard errors) \n                 Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)     8.1130457  2.5671301  3.1604 0.001576\nPct0_14        -0.0574046  0.0344908 -1.6643 0.096043\nPct_65         -0.0238715  0.0293647 -0.8129 0.416256\nPct_Img         0.0048364  0.0266560  0.1814 0.856025\nPct_brevet      0.0112746  0.0195259  0.5774 0.563656\nNivVieMed      -0.1463876  0.0605853 -2.4162 0.015682\nlag.Pct0_14    -0.1242574  0.0581170 -2.1381 0.032512\nlag.Pct_65      0.0255480  0.0499646  0.5113 0.609125\nlag.Pct_Img     0.1559952  0.0482138  3.2355 0.001214\nlag.Pct_brevet -0.0883930  0.0342496 -2.5809 0.009856\nlag.NivVieMed   0.1032469  0.0960201  1.0753 0.282257\n\nRho: 0.84127, LR test value: 492.38, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.023363\n    z-value: 36.009, p-value: &lt; 2.22e-16\nWald statistic: 1296.7, p-value: &lt; 2.22e-16\n\nLog likelihood: -1353.106 for mixed model\nML residual variance (sigma squared): 9.9845, (sigma: 3.1598)\nNagelkerke pseudo-R-squared: 0.8002 \nNumber of observations: 506 \nNumber of parameters estimated: 13 \nAIC: 2732.2, (AIC for lm: 3222.6)\nLM test for residual autocorrelation\ntest value: 0.0748, p-value: 0.78447\n\n\nEffets directs, indirects et totaux\n\n# Effets directs, indirects et totaux (uniquement les coefficients)\nimpacts(Modele.DurbinSpatial, listw = W.Rook, R = 999)\n\nImpact measures (mixed, exact):\n                Direct    Indirect       Total\nPct0_14    -0.12369039 -1.02079497 -1.14448536\nPct_65     -0.02177191  0.03233406  0.01056215\nPct_Img     0.06632543  0.94692639  1.01325182\nPct_brevet -0.01903815 -0.46681402 -0.48585217\nNivVieMed  -0.15403413 -0.11775603 -0.27179016\n\n# Effets directs, indirects et totaux (coefficients, valeurs de z et de p)\nsummary(impacts(Modele.DurbinSpatial, listw = W.Rook, R = 999), zstats = TRUE, short = TRUE)\n\nImpact measures (mixed, exact):\n                Direct    Indirect       Total\nPct0_14    -0.12369039 -1.02079497 -1.14448536\nPct_65     -0.02177191  0.03233406  0.01056215\nPct_Img     0.06632543  0.94692639  1.01325182\nPct_brevet -0.01903815 -0.46681402 -0.48585217\nNivVieMed  -0.15403413 -0.11775603 -0.27179016\n========================================================\nSimulation results ( variance matrix):\n========================================================\nSimulated standard errors\n               Direct  Indirect     Total\nPct0_14    0.04244663 0.3220020 0.3483720\nPct_65     0.03463367 0.2845439 0.3056634\nPct_Img    0.03295238 0.2656307 0.2845980\nPct_brevet 0.02451501 0.2060428 0.2217848\nNivVieMed  0.07063139 0.5081611 0.5443442\n\nSimulated z-values:\n               Direct   Indirect      Total\nPct0_14    -2.9045581 -3.1711580 -3.2850167\nPct_65     -0.6188453  0.1856750  0.1027268\nPct_Img     2.0444172  3.6111403  3.6071864\nPct_brevet -0.7775804 -2.3294542 -2.2500628\nNivVieMed  -2.1648691 -0.2753142 -0.5379164\n\nSimulated p-values:\n           Direct    Indirect   Total     \nPct0_14    0.0036777 0.00151833 0.00101976\nPct_65     0.5360183 0.85269962 0.91817984\nPct_Img    0.0409123 0.00030485 0.00030954\nPct_brevet 0.4368164 0.01983502 0.02444496\nNivVieMed  0.0303977 0.78307487 0.59063474\n\n\nDépendance spatiale du modèle SDM?\n\nmoran.mc(resid(Modele.DurbinSpatial), W.Rook, nsim=999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  resid(Modele.DurbinSpatial) \nweights: W.Rook  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.0046127, observed rank = 451, p-value = 0.549\nalternative hypothesis: greater\n\n\n\n7.1.2.5 Modèle SDEM : autocorrélation spatiale sur les variables indépendantes et sur le terme d’erreur\nLe modèle SDEM (Spatial Durbin Error Model en anglais) est un autre modèle mixte qui intègre à la fois l’autocorrélation spatiale sur les valeurs indépendantes (\\(WX\\), externalités) et sur le terme d’erreur (\\(u = \\lambda Wu + \\epsilon\\)). Il s’écrit alors :\n\\[\ny = X\\beta + WX\\theta + u \\textrm{, } u = \\lambda Wu + \\epsilon\n\\tag{7.7}\\]\navec :\n\n\n\\(y\\), la variable dépendante.\n\n\\(W\\), la matrice de pondération spatiale.\n\n\\(X\\), les variables indépendantes.\n\n\\(\\beta\\), les coefficients des variables indépendantes.\n\n\\(WX\\), les variables dépendantes spatiales décalées.\n\n\\(\\theta\\), les coefficients des variables indépendantes spatiales décalées.\n\n\\(\\lambda\\) (prononcez lambda), le coefficient sur le terme d’erreur spatialement décalé.\n\n\\(\\epsilon\\), les résidus.\n\nConstruction du modèle SDEM dans R\nLe modèle SDEM est construit avec la fonction errorsarlm du package spatialreg. Notez que le paramètre etype = \"mixed\" spécifie l’utilisation d’un modèle mixte.\n\n## Construction du modèle\nModele.DurbinErreur &lt;- errorsarlm(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n                                  listw=W.Rook,    # matrice de pondération spatiale\n                                  data = LyonIris, # dataframe\n                                  etype = 'emixed')\n## Résultats du modèle\nsummary(Modele.DurbinErreur, Nagelkerke=TRUE)\n\n\nCall:errorsarlm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + \n    NivVieMed, data = LyonIris, listw = W.Rook, etype = \"emixed\")\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-12.99324  -1.82407  -0.45644   1.06084  18.21108 \n\nType: error \nCoefficients: (asymptotic standard errors) \n                 Estimate Std. Error z value  Pr(&gt;|z|)\n(Intercept)    37.0610133  6.5010177  5.7008 1.192e-08\nPct0_14        -0.0819977  0.0416988 -1.9664   0.04925\nPct_65         -0.0263294  0.0347137 -0.7585   0.44817\nPct_Img         0.0046560  0.0310281  0.1501   0.88072\nPct_brevet      0.0097849  0.0238839  0.4097   0.68203\nNivVieMed      -0.1678555  0.0680048 -2.4683   0.01358\nlag.Pct0_14    -0.1767469  0.1023451 -1.7270   0.08417\nlag.Pct_65      0.0105325  0.0891835  0.1181   0.90599\nlag.Pct_Img     0.0927851  0.0797036  1.1641   0.24437\nlag.Pct_brevet -0.0380482  0.0566883 -0.6712   0.50210\nlag.NivVieMed  -0.1025307  0.1724055 -0.5947   0.55204\n\nLambda: 0.8976, LR test value: 464.09, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.018242\n    z-value: 49.204, p-value: &lt; 2.22e-16\nWald statistic: 2421, p-value: &lt; 2.22e-16\n\nLog likelihood: -1367.25 for error model\nML residual variance (sigma squared): 10.046, (sigma: 3.1696)\nNagelkerke pseudo-R-squared: 0.78871 \nNumber of observations: 506 \nNumber of parameters estimated: 13 \nAIC: NA (not available for weighted model), (AIC for lm: 3222.6)\n\n\nEffets directs, indirects et totaux\n\n## Effets directs, indirects et totaux (uniquement les coefficients)\nimpacts(Modele.DurbinErreur, listw = W.Rook, R = 999)\n\nImpact measures (SDEM, glht):\n                 Direct    Indirect       Total\nPct0_14    -0.081997661 -0.17674688 -0.25874455\nPct_65     -0.026329377  0.01053246 -0.01579692\nPct_Img     0.004656049  0.09278514  0.09744119\nPct_brevet  0.009784949 -0.03804816 -0.02826321\nNivVieMed  -0.167855503 -0.10253070 -0.27038620\n\n## Effets directs, indirects et totaux (coefficients, valeurs de z et de p)\nsummary(impacts(Modele.DurbinErreur, listw = W.Rook, R = 999), zstats = TRUE, short = TRUE)\n\nImpact measures (SDEM, glht, n):\n                 Direct    Indirect       Total\nPct0_14    -0.081997661 -0.17674688 -0.25874455\nPct_65     -0.026329377  0.01053246 -0.01579692\nPct_Img     0.004656049  0.09278514  0.09744119\nPct_brevet  0.009784949 -0.03804816 -0.02826321\nNivVieMed  -0.167855503 -0.10253070 -0.27038620\n========================================================\nStandard errors:\n               Direct   Indirect      Total\nPct0_14    0.04169878 0.10234506 0.13146452\nPct_65     0.03471367 0.08918350 0.11192948\nPct_Img    0.03102807 0.07970364 0.09949722\nPct_brevet 0.02388387 0.05668833 0.07344175\nNivVieMed  0.06800483 0.17240548 0.21172909\n========================================================\nZ-values:\n               Direct   Indirect      Total\nPct0_14    -1.9664284 -1.7269703 -1.9681701\nPct_65     -0.7584729  0.1180987 -0.1411328\nPct_Img     0.1500592  1.1641268  0.9793358\nPct_brevet  0.4096885 -0.6711815 -0.3848384\nNivVieMed  -2.4682881 -0.5947067 -1.2770386\n\np-values:\n           Direct   Indirect Total   \nPct0_14    0.049249 0.084173 0.049048\nPct_65     0.448168 0.905989 0.887765\nPct_Img    0.880718 0.244373 0.327414\nPct_brevet 0.682034 0.502105 0.700357\nNivVieMed  0.013576 0.552040 0.201589\n\n\nDépendance spatiale du modèle SDEM?\nAvec une valeur du I de Moran de -0,010 (p = 0,619), les résidus du modèle SDEM ne sont pas spatialement autocorrélés.\n\nmoran.mc(resid(Modele.DurbinErreur), W.Rook, nsim=999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  resid(Modele.DurbinErreur) \nweights: W.Rook  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.010362, observed rank = 358, p-value = 0.642\nalternative hypothesis: greater\n\n\n\n7.1.3 Quel modèle choisir?\n\n7.1.3.1 Tests du multiplicateur de Lagrange sur le modèle MCO\nL’utilisation des tests du multiplicateur de Lagrange (simple et robuste) a été largement popularisée par Anselin et al. (1996) pour vérifier si le recours à un modèle autorégressif est nécessaire, comparativement à un modèle de régression classique (MCO). Les tests sont calculés sur le modèle MCO avec la fonction lm.LMtests et une matrice de pondération spatiale. Ces tests permettent aussi de choisir entre les modèles SAR et SEM. La démarche suivante peut être utilisée pour choisir un modèle :\n\nSi toutes les valeurs des tests (simples et robustes) sont non significatives (p &gt; 0,05), alors le recours à un modèle autorégressif n’est pas nécessaire. Nous pouvons conserver le modèle de régression classique (MCO).\nSi les valeurs de LMlag ou RLMlag sont non significatives (p &gt; 0,05), alors le recours au modèle SAR n’est pas nécessaire.\nSi les valeurs de LMerr ou RLMerr sont non significatives (p &gt; 0,05), alors le recours au modèle SEM n’est pas nécessaire.\nSi les valeurs de RLMlag et RLMerr sont significatives (p &lt; 0,001), nous choisissons le modèle ayant la plus forte statistique.\n\nDans les résultats ci-dessous, nous ne retenons pas le modèle SEM car la valeur de 0,740 pour le RLMerr n’est pas significative (p = 0,3898). Par contre, les valeurs de LMlag et de RLMlag (555 et 123) sont significatives, ce qui justifie la sélection du modèle SAR.\n\nsummary(lm.LMtests(model = Modele.MCO,\n                   listw = W.Rook,\n                   test = c(\"LMlag\",\"LMerr\",\"RLMlag\",\"RLMerr\")))\n\n    Lagrange multiplier diagnostics for spatial dependence\ndata:  \nmodel: lm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet +\nNivVieMed, data = LyonIris)\nweights: W.Rook\n \n       statistic parameter p.value    \nLMlag  554.65778         1  &lt;2e-16 ***\nLMerr  432.83282         1  &lt;2e-16 ***\nRLMlag 122.56452         1  &lt;2e-16 ***\nRLMerr   0.73955         1  0.3898    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n7.1.3.2 Comparaison des modèles mixtes et non mixtes\nNous avons vu qu’il existe deux modèles mixtes (SDM et SDEM). Il convient alors de vérifier si le recours d’un modèle mixte est justifié comparativement à un modèle non mixte. Dans le code ci-dessous, nous vérifions si le modèle SDM est statistiquement différent du modèle SAR avec les fonctions LR.Sarlm et anova. Les résultats signalent un écart significatif des valeurs du log-vraisemblance (26,101, p &lt; 0,001). Par conséquent, ce modèle mixte a un apport significatif.\n\n## SDM et SEM sont-ils significativement différents?\nLR.Sarlm(Modele.DurbinSpatial, Modele.SAR)\n\n\n    Likelihood ratio for spatial linear models\n\ndata:  \nLikelihood ratio = 26.101, df = 5, p-value = 8.528e-05\nsample estimates:\nLog likelihood of Modele.DurbinSpatial           Log likelihood of Modele.SAR \n                             -1353.106                              -1366.157 \n\nanova(Modele.DurbinSpatial, Modele.SAR)\n\n                     Model df    AIC  logLik Test L.Ratio    p-value\nModele.DurbinSpatial     1 13 2732.2 -1353.1    1                   \nModele.SAR               2  8 2748.3 -1366.2    2  26.101 8.5283e-05\n\n\nÀ l’inverse, la différence entre les valeurs du log-vraisemblance des modèles SDEM et SEM n’est pas significative (4,9728, p = 0,42), signalant que l’utilisation d’un modèle SDEM comparativement à un modèle SEM n’est pas nécessaire.\n\n## SDEM et SEM sont-ils significativement différents?\nLR.Sarlm(Modele.DurbinErreur, Modele.SEM)\n\n\n    Likelihood ratio for spatial linear models\n\ndata:  \nLikelihood ratio = 4.9728, df = 5, p-value = 0.4192\nsample estimates:\nLog likelihood of Modele.DurbinErreur          Log likelihood of Modele.SEM \n                            -1367.250                             -1369.737 \n\nanova(Modele.DurbinErreur, Modele.SEM)\n\n                    Model df    AIC  logLik Test L.Ratio p-value\nModele.DurbinErreur     1 13 2760.5 -1367.2    1                \nModele.SEM              2  8 2755.5 -1369.7    2  4.9728  0.4192\n\n\n\n7.1.3.3 Mesures AIC et BIC et dépendance spatiale\nLe critère d’information d’Akaike (AIC) et le critère d’information bayésien (BIC) sont largement utilisés pour évaluer la qualité d’ajustement du modèle. Plus leurs valeurs sont faibles, meilleur est le modèle. Il est donc possible de comparer leurs valeurs pour les différents modèles (MCO, SLX, SAR, SEM, SDM et SDEM). Nous pouvons aussi comparer l’autocorrélation spatiale des résidus des modèles avec le I de Moran.\n\n## Valeurs d'AIC et de BIC\nAICs &lt;- AIC(Modele.MCO, Modele.SLX, Modele.SAR, Modele.SEM, \n            Modele.DurbinSpatial, Modele.DurbinErreur)\nBICs &lt;- BIC(Modele.MCO, Modele.SLX, Modele.SAR, Modele.SEM, \n            Modele.DurbinSpatial, Modele.DurbinErreur)\n## Autocorrélation spatiale des résidus\nIMoran.MCO &lt;- moran.mc(resid(Modele.MCO), W.Rook, nsim=999)\nIMoran.SLX &lt;- moran.mc(resid(Modele.SLX), W.Rook, nsim=999)\nIMoran.SLM &lt;- moran.mc(resid(Modele.SAR), W.Rook, nsim=999)\nIMoran.SEM &lt;- moran.mc(resid(Modele.SEM), W.Rook, nsim=999)\nIMoran.DurbinS &lt;- moran.mc(resid(Modele.DurbinSpatial), W.Rook, nsim=999)\nIMoran.DurbinE &lt;- moran.mc(resid(Modele.DurbinErreur), W.Rook, nsim=999)\nMoranI.s &lt;- c(IMoran.MCO$statistic, IMoran.SLX$statistic,\n             IMoran.SLM$statistic, IMoran.SEM$statistic,\n             IMoran.DurbinS$statistic, IMoran.DurbinE$statistic)\nMoranI.p &lt;- c(IMoran.MCO$p.value, IMoran.SLX$p.value,\n             IMoran.SLM$p.value, IMoran.SEM$p.value,\n             IMoran.DurbinS$p.value, IMoran.DurbinE$p.value)\n## Tableau\nComparaison &lt;- data.frame(Modele = c(\"MCO\", \"SLX\", \"SAR\", \"SEM\", \"Durbin S\", \"Durbin E\"),\n                          AIC = AICs$AIC,\n                          BIC = BICs$BIC,\n                          dl = AICs$df,\n                          MoranI = MoranI.s,\n                          MoranIp = MoranI.p)\nComparaison\n\n    Modele      AIC      BIC dl       MoranI MoranIp\n1      MCO 3366.626 3396.212  7  0.587312061   0.001\n2      SLX 3222.594 3273.313 12  0.604660275   0.001\n3      SAR 2748.314 2782.126  8 -0.014281059   0.654\n4      SEM 2755.474 2789.286  8 -0.011826630   0.613\n5 Durbin S 2732.212 2787.157 13 -0.004612686   0.507\n6 Durbin E 2760.501 2815.446 13 -0.010361614   0.616\n\n\nQuelques lignes de code suffisent pour créer deux graphiques permettant de comparer visuellement les résultats des différents modèles (figure 7.5). Les résultats démontrent que :\n\nLes modèles MCO et SLX ont un problème de dépendance spatiale puisque leurs résidus sont significativement autocorrélés spatialement. Par conséquent, ils ne devraient pas être retenus.\nLes modèles SDM, SAR et SEM sont les plus performants avec les valeurs d’AIC les plus faibles.\n\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n## Graphique pour l'autocorrélation spatiale\ng1 &lt;- ggplot(data=Comparaison, aes(x=reorder(Modele,MoranI), y=MoranI)) +\n      geom_segment(aes(x=reorder(Modele, MoranI),\n                   xend=reorder(Modele, MoranI),\n                   y=0, yend=MoranI)) +\n  geom_point( size=4,fill=\"red\",shape=21)+\n  xlab(\"Modèle\") + ylab(\"I de Moran\")+\n  labs(title=\"Autocorrélation spatiale des résidus\",\n       caption=\"Plus la valeur du I de Moran est faible, \\nmoins il y a d'autocorrélation spatiale.\")\n## Graphique pour les valeurs d'AIC\ng2 &lt;- ggplot(data=Comparaison, aes(x=reorder(Modele,AIC), y=AIC)) +\n  geom_segment(aes(x=reorder(Modele, AIC),\n                   xend=reorder(Modele, AIC),\n                   y=0, yend=AIC)) +\n  geom_point( size=4,fill=\"red\",shape=21)+\n  xlab(\"Modèle\") + ylab(\"AIC\")+\n  labs(title=\"Qualité d'ajustement du modèle\",\n       caption=\"Plus la valeur d'AIC est faible, \\nplus le modèle est performant.\")\n## Figure avec les deux graphiques\nggarrange(g1, g2)\n\n\n\nFigure 7.5: Comparaison des différents modèles"
  },
  {
    "objectID": "07-RegressionSpatiales.html#sec-072",
    "href": "07-RegressionSpatiales.html#sec-072",
    "title": "7  Introduction aux modèles de régression spatiale",
    "section": "\n7.2 Modèles généralisés additifs (GAM) avec une spline bivariée sur les coordonnées géographiques",
    "text": "7.2 Modèles généralisés additifs (GAM) avec une spline bivariée sur les coordonnées géographiques\nLes modèles généralisés additifs (Generalized additive models en anglais) permettent d’intégrer à la fois des effets linéaires et des effets non linéaires avec des splines. Ils peuvent alors être utilisés en intégrant une spline bivariée sur les coordonnées géographiques des centroïdes des entités spatiales.\n\n\n\n\n\nModèles généralisés additifs\n\n\nPour une description détaillée des modèles généralisés additifs, nous vous invitons vivement à lire le chapitre suivant (Apparicio et Gelb 2022).\n\n\n\n7.2.1 Principe de base d’un GAM intégrant l’espace\nAvec une spline bivariée sur les coordonnées géographiques, l’équation d’un modèle généralisé additif s’écrit :\n\\[\ng(Y) \\ = \\beta_{0} \\ + X\\beta + s(CoordX,CoordY) \\ + \\epsilon\n\\tag{7.8}\\]\navec :\n\n\n\\(y\\), la variable dépendante.\n\n\\(\\beta_{0}\\), la constante.\n\n\\(X\\), les variables indépendantes.\n\n\\(\\beta\\), les coefficients des variables dépendantes.\n\n\\(s(CoordX,CoordY)\\), spline bivariée sur les coordonnées x et y.\n\n\\(\\epsilon\\), les résidus.\n\nL’intérêt de recourir à une spline bivariée sur les coordonnées géographiques est double :\n\nContrôler l’effet de la localisation sur la variable dépendante (\\(y\\)). Les coefficients des autres variables indépendantes sont ainsi obtenus une fois l’espace pris en compte.\nÉvaluer l’effet de la localisation (patron spatial), une fois les autres variables indépendantes contrôlées. Autrement dit, toutes choses étant égales par ailleurs, quel est l’effet de la localisation sur la variable dépendante?\n\n7.2.2 Construction d’un modèle GAM dans R\n\n7.2.2.1 Réalisation du modèle GAM\nPour construire des modèles GAM dans R, nous utilisons la fonction gam du package mgcv (Wood 2011).\n\nlibrary(mgcv)\n## Ajout des coordonnées X et Y dans LyonIris \nxy &lt;- st_coordinates(st_centroid(LyonIris))\nLyonIris$X &lt;- xy[,1]\nLyonIris$Y &lt;- xy[,2]\n## Construction du modèle GAM\nModele.GAM1 &lt;- gam(NO2 ~  Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed+\n                          s(X, Y),    # spline sur les coordonnées X, Y\n                    data = LyonIris)  # dataframe\n## Résultats du modèle\nsummary(Modele.GAM1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nNO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + NivVieMed + s(X, \n    Y)\n\nParametric coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.402313   2.156322  14.099   &lt;2e-16 ***\nPct0_14     -0.053641   0.046224  -1.160    0.246    \nPct_65      -0.056310   0.038476  -1.463    0.144    \nPct_Img      0.008033   0.035397   0.227    0.821    \nPct_brevet   0.043874   0.027518   1.594    0.112    \nNivVieMed   -0.043282   0.072461  -0.597    0.551    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n        edf Ref.df     F p-value    \ns(X,Y) 26.5  28.62 27.22  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.715   Deviance explained = 73.3%\nGCV = 18.814  Scale est. = 17.605    n = 506\n\n\nLes résultats ci-dessus signalent que la localisation a un effet très significatif puisque (s(X,Y) = 26,5 avec p &lt; 0,001). Notez que la valeur de p permet de déterminer si la spline bivariée (et donc l’espace) a ou non un effet significatif. Si la valeur de p est supérieure à 0,05, alors il n’est pas nécessaire de conserver la spline bivariée sur les coordonnées géographiques.\nDe plus, le code ci-dessous permet de constater que le modèle GAM est plus performant que le modèle linéaire multiple classique (MCO).\n\nanova(Modele.MCO, Modele.GAM1)\n\nAnalysis of Variance Table\n\nModel 1: NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + NivVieMed\nModel 2: NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + NivVieMed + s(X, \n    Y)\n  Res.Df     RSS   Df Sum of Sq      F    Pr(&gt;F)    \n1  500.0 22346.0                                    \n2  473.5  8336.1 26.5     14010 30.029 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNous pouvons aussi introduire une spline plus complexe en augmentant le nombre de nœuds à 40.\n\nModele.GAM2 &lt;- gam(NO2 ~  Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed+\n                          s(X, Y, k= 40), data = LyonIris)\nsummary(Modele.GAM2)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nNO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + NivVieMed + s(X, \n    Y, k = 40)\n\nParametric coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.372093   2.088160  14.545   &lt;2e-16 ***\nPct0_14     -0.055294   0.044285  -1.249   0.2124    \nPct_65      -0.049122   0.036870  -1.332   0.1834    \nPct_Img      0.002226   0.033722   0.066   0.9474    \nPct_brevet   0.052229   0.026315   1.985   0.0478 *  \nNivVieMed   -0.050991   0.070081  -0.728   0.4672    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n         edf Ref.df     F p-value    \ns(X,Y) 35.68  38.51 24.27  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.747   Deviance explained = 76.7%\nGCV = 17.014  Scale est. = 15.613    n = 506\n\n\nLa valeur plus faible d’AIC pour le second modèle GAM signale qu’il est plus performant que le premier.\n\nAIC(Modele.MCO, Modele.GAM1, Modele.GAM2)\n\n                  df      AIC\nModele.MCO   7.00000 3366.626\nModele.GAM1 33.50046 2920.682\nModele.GAM2 42.67520 2868.355\n\n\n\n7.2.2.2 Visualisation de l’effet de l’espace\nPour visualiser les prédictions du modèle dans l’espace, toutes choses étant égales par ailleurs, nous utilisons la fonction vis.gam (figure 7.6). Les contours signalent qu’au centre de Lyon, les valeurs de dioxyde d’azote sont les plus élevées et dépassent même 40 \\(\\mu\\)g/m3.\n\nvis.gam(Modele.GAM2, view=c(\"X\", \"Y\"), plot.type = \"contour\", color=\"terrain\")\n\n\n\nFigure 7.6: Visualisation des prédictions dans l’espace avec la fonction vis.gam\n\n\n\nToutefois, il est plus intéressant de la représenter dans un raster, une fois contrôlées les autres variables indépendantes. Pour ce faire, six étapes sont nécessaires :\n\nCréer une grid.\nFixer les autres paramètres à leur moyenne respective.\nCalculer la prédiction pour la localisation.\nCentrer la prédiction.\nConstruire le raster avec les prédictions.\nDécouper et cartographier le raster.\n\n\nlibrary(raster)\nlibrary(terra)\n## Étape 1 : création d'une grid pour la prédiction de 100 mètres de résolution spatiale\nXcoords &lt;- seq(min(LyonIris$X-100), max(LyonIris$X+100), by=100)\nYcoords &lt;- seq(min(LyonIris$Y-100), max(LyonIris$Y+100), by=100)\nPredDF &lt;- expand.grid(Xcoords,Ycoords)\nnames(PredDF) &lt;- c(\"X\",\"Y\")\n## Étape 2 : fixation de tous les autres paramètres à leur moyenne\nfor(Var in c(\"VegHautPrt\",\"Pct0_14\",\"Pct_65\",\"Pct_Img\",\"Pct_brevet\", \"NivVieMed\")){\n  PredDF[[Var]] &lt;- mean(LyonIris[[Var]])\n}\n## Étape 3 : calcul de la prédiction\nPredDF$PM25 &lt;- predict(Modele.GAM2,newdata=PredDF)\n## Étape 4 : centrage de la prédiction (sans la constante)\nPredDF$CenterPredPM25 &lt;- PredDF$PM25 - mean(PredDF$PM25)\n### Étape 5 : construction du raster\nrasterGAM &lt;- rasterFromXYZ(PredDF[, c(\"X\", \"Y\", \"CenterPredPM25\")])\ncrs(rasterGAM) &lt;- crs(as(LyonIris, \"Spatial\"))\nrasterGAM &lt;- rast(rasterGAM)\n### Étape 6 : découpage et cartographie du raster\nLyonIris.SpatVector &lt;- vect(LyonIris)\nrasterGAM &lt;- terra::mask(rasterGAM, LyonIris.SpatVector)\nterra::plot(rasterGAM)\n\n\n\nFigure 7.7: Visualisation de l’effet de la localisation centrée sur zéro\n\n\n\nLa figure 7.7 signale que dans le centre de Lyon, le dioxyde d’azote est plus élevé de 10 à 20 \\(\\mu\\)g/m3, toutes choses étant égales par ailleurs. À l’inverse, dans les zones périphériques, il est faible. Cela signale un net patron spatial décroissant du centre vers la périphérie.\n\n7.2.2.3 Dépendance spatiale du modèle GAM\nPar contre, bien que l’autocorrélation spatiale des résidus du modèle GAM soit plus faible que pour le modèle MCO (I de Moran de 0,337 contre 0,570 avec p &lt; 0,001), il reste que le problème de la dépendance spatiale n’est pas corrigé.\n\nmoran.mc(resid(Modele.GAM2), W.Rook, nsim=999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  resid(Modele.GAM2) \nweights: W.Rook  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.33664, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater"
  },
  {
    "objectID": "07-RegressionSpatiales.html#sec-073",
    "href": "07-RegressionSpatiales.html#sec-073",
    "title": "7  Introduction aux modèles de régression spatiale",
    "section": "\n7.3 Régression géographiquement pondérée",
    "text": "7.3 Régression géographiquement pondérée\nLa régression géographiquement pondérée (geographically weighted regression - GWR, en anglais) a été proposée par Fotheringham et al. (2003) pour modéliser une variable continue. Depuis, plusieurs extensions ont été proposées, notamment des GWR mixtes, des GWR logistiques ou Poisson. Dans le cadre de cette section, nous abordons uniquement sa forme classique (variable dépendante continue).\n\n7.3.1 Principe de base\nPourquoi recourir à la GWR?\nDans la section 7.1, nous avons vu que les modèles autorégressifs visent à contrôler la dépendance spatiale d’un modèle de régression classique (MCO), afin d’améliorer l’estimation des coefficients de régression. L’objectif des modèles de régression géographiquement pondérée est différent : ils visent à analyser les variations spatiales de la relation entre la variable dépendante et les variables indépendantes.\nAutrement dit, les modèles GWR visent à explorer l’instabilité spatiale du modèle MCO afin d’analyser localement la relation entre la variable dépendante et les variables indépendantes. Pour une description détaillée en français de la GWR, consultez Apparicio et al. (2007).\nFormulation de la GWR\nContrairement à la régression linéaire classique et aux modèles spatiaux autorégressifs qui produisent une équation pour l’ensemble du tableau de données, la GWR produit une équation pour chaque unité spatiale i et ainsi, des valeurs locales de R2, \\(\\beta_0\\), \\(\\beta_k\\), t de Student, etc. La résolution de cette équation de régression locale est aussi basée sur la méthode des moindres carrés et sur une matrice de pondération W(i) dont les valeurs décroissent en fonction de la distance séparant les unités i et j. Autrement dit, plus j est proche de i, plus sa pondération est élevée et donc plus son « rôle » dans la détermination de l’équation de régression locale de i est important.\nDe la sorte, la GWR est une extension de la régression linéaire multiple classique où \\((u_i, v_i)\\) représente les coordonnées géographiques du centroïde de l’unité spatiale et où les paramètres \\(\\beta_0\\) et \\(\\beta_k\\) peuvent varier dans l’espace (équation 7.9).\n\\[\ny_i = \\beta_0(u_i, v_i)+ \\sum_{j=1}^k \\beta_j(u_i, v_i)x_{ij}+ \\epsilon_i\n\\tag{7.9}\\]\navec :\n\n\n\\((u_i, v_i)\\), les coordonnées géographiques de l’unité spatiale i.\n\n\\(y_i\\), la variable dépendante pour l’unité spatiale i.\n\n\\(\\beta_0(u_i, v_i)\\), la constante pour l’unité spatiale i aux coordonnées géographiques \\((u_i, v_i)\\).\n\n\\(\\beta_j(u_i, v_i)\\), le coefficient de régression pour la variable \\(x_j\\) (avec k variables indépendantes) pour l’unité spatiale i aux coordonnées géographiques \\((u_i, v_i)\\).\n\n\\(x_{ij}\\), la valeur de la variable indépendante \\(x_j\\) pour l’unité spatiale i.\n\n\\(\\epsilon_i\\), le terme d’erreur pour l’unité spatiale i.\n\nFotheringham et al. (2003) proposent deux fonctions kernel pour définir la pondération W(i) dans le modèle GWR : une fonction gaussienne (équation 7.10) et une fonction bicarrée (équation 7.11) où \\(d_{ij}\\) représente la distance euclidienne entre les points i et j et b, le rayon de zone d’influence autour du point i (bandwidth). Il existe une différence fondamentale entre les deux : la fonction gaussienne accorde un poids non nul à tous les points de l’espace d’étude aussi loin soient-ils, tandis que la fonction bicarrée ne tient pas compte des points distants à plus de b mètres de i, tel qu’illustré à la figure 7.8 avec une valeur fixée à 5000 mètres en guise d’exemple.\n\\[\nw_{ij} = exp[-.5(d_{ij}/b)^2]\n\\tag{7.10}\\]\n\\[\nw_{ij} = [1-(d_{ij}/b)^2]^2 \\text{ si } d_{ij}&lt; b \\text{, sinon } w_{ij}=0\n\\tag{7.11}\\]\n\n\n\n\nFigure 7.8: Fonctions kernel pour définir la matrice de pondération W(i)\n\n\n\nDans le modèle GWR, la valeur de b est soit fixée par la personne utilisatrice, soit optimisée avec la valeur de CV (cross-validation) ou celle de l’AIC. Notez qu’il est possible d’optimiser la taille de la zone d’influence à partir de la distance euclidienne ou du nombre de plus proches voisins.\n\n7.3.2 Construction et analyse du modèle GWR dans R\nPour construire un modèle GWR dans R, nous utilisons le package spgwr (Bivand et Yu 2023). La construction d’un modèle GWR comprend les étapes suivantes :\n\nSélection de la taille de la zone d’influence (bandwidth) optimale.\nRéalisation de la GWR avec la taille de la zone d’influence optimale.\nComparaison des modèles MCO et GWR.\nCartographie des résultats du modèle GWR (R2, coefficients, valeurs de t, etc.).\n\n\n7.3.2.1 Définition de la taille de la zone d’influence\nLa sélection de la taille de la zone d’influence optimale est réalisée avec la fonction gwr.sel pour laquelle :\n\nle paramètre gweight permet de spécifier une fonction kernel gaussienne (gwr.gauss) ou bicarrée (gwr.gauss).\nle paramètre adapt permet de spécifier si vous optimisez le nombre de plus proches voisins (adapt=TRUE) ou la distance (adapt=FALSE).\n\n\nlibrary(spgwr)\n## Optimisation du nombre de voisins avec le CV\nbwaCV.voisins  &lt;- gwr.sel(NO2 ~  Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n                          data = LyonIris,\n                          method = \"cv\",          # Méthode cv ou AIC\n                          gweight=gwr.bisquare,   # gwr.gauss ou gwr.bisquare\n                          adapt=TRUE,\n                          verbose = FALSE,\n                          RMSE = TRUE,\n                          longlat = FALSE,\n                          coords=cbind(LyonIris$X,LyonIris$Y))\n## Optimisation du nombre de voisins avec l'AIC\nbwaAIC.voisins &lt;- gwr.sel(NO2 ~  Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n                          data = LyonIris,\n                          method = \"AIC\",          # Méthode cv ou AIC\n                          gweight=gwr.bisquare,    # gwr.gauss ou gwr.bisquare\n                          adapt=TRUE,              # adaptatif\n                          verbose = FALSE,\n                          RMSE = TRUE,\n                          longlat = FALSE,\n                          coords=cbind(LyonIris$X,LyonIris$Y))\n## Optimisation de la distance avec le CV\nbwnaCV.dist &lt;- gwr.sel(NO2 ~  Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n                           data = LyonIris,\n                           method = \"cv\",     # méthode cv ou AIC\n                           gweight=gwr.Gauss, # gwr.gauss ou gwr.bisquare\n                           adapt=FALSE,       # non adaptatif\n                           verbose = FALSE,\n                           RMSE = TRUE,\n                           longlat = FALSE,\n                           coords=cbind(LyonIris$X,LyonIris$Y))\n## Optimisation de la distance avec l'AIC\nbwnaAIC.dist &lt;- gwr.sel(NO2 ~  Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n                          data = LyonIris,\n                          method = \"AIC\",      # méthode cv ou AIC\n                          gweight=gwr.Gauss,   # gwr.gauss ou gwr.bisquare\n                          adapt=FALSE,         # non adaptatif\n                          RMSE = TRUE,\n                          verbose = FALSE,\n                          longlat = FALSE,\n                          coords=cbind(LyonIris$X,LyonIris$Y))\n## Affichage des résultats d'optimisation \ncat(\"Sélection de la taille de la zone optimale (bandwidth)\",\n    \"\\n avec le nombre de plus proches voisins :\",\n    \"\\n  CV =\", round(bwaCV.voisins,4), \"nombre de voisins =\", \n    round(bwaCV.voisins*nrow(LyonIris)),\n    \"\\n  AIC =\", round(bwaAIC.voisins,4), \"nombre de voisins =\", \n    round(bwaAIC.voisins*nrow(LyonIris)),\n    \"\\nSélection de la taille de la zone optimale (bandwidth) avec la distance :\",\n    \"\\n  CV =\", round(bwnaCV.dist, 0),  \"mètres\",\n    \"\\n  AIC =\", round(bwnaAIC.dist, 0), \"mètres\")\n\nSélection de la taille de la zone optimale (bandwidth) \n avec le nombre de plus proches voisins : \n  CV = 0.1818 nombre de voisins = 92 \n  AIC = 0.1067 nombre de voisins = 54 \nSélection de la taille de la zone optimale (bandwidth) avec la distance : \n  CV = 1315 mètres \n  AIC = 1662 mètres\n\n\nLes résultats ci-dessus montrent que le nombre de plus proches voisins pourrait être de 92 selon l’approche cross-validation et de 54 selon la méthode basée sur l’AIC. Si la valeur de b est basée sur la distance, elle serait alors optimale à 1315 et 1662 mètres selon les deux méthodes.\n\n7.3.2.2 Réalisation de la GWR\nAvec la fonction gwr, nous estimons un modèle GWR avec un kernel bicarré et un nombre optimisé de plus voisins selon la méthode CV, soit 92.\n\nModele.GWR &lt;- gwr(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed,\n              data = LyonIris,\n              adapt=bwaCV.voisins,\n              gweight=gwr.bisquare,\n              hatmatrix=TRUE,\n              se.fit=TRUE,\n              coords=cbind(LyonIris$X,LyonIris$Y),\n              longlat=F)\n\nLe code ci-dessous permet de renvoyer les statistiques univariées des coefficients des 506 régressions locales, réalisées pour chacune des 506 entités spatiales (IRIS), et les statistiques d’ajustement du modèle (AIC, R2 global, etc.)\n\nModele.GWR\n\nCall:\ngwr(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + \n    NivVieMed, data = LyonIris, coords = cbind(LyonIris$X, LyonIris$Y), \n    gweight = gwr.bisquare, adapt = bwaCV.voisins, hatmatrix = TRUE, \n    longlat = F, se.fit = TRUE)\nKernel function: gwr.bisquare \nAdaptive quantile: 0.1818192 (about 92 of 506 data points)\nSummary of GWR coefficient estimates at data points:\n                  Min.   1st Qu.    Median   3rd Qu.      Max.  Global\nX.Intercept. 12.429518 30.249511 38.619342 48.038863 60.098584 49.4330\nPct0_14      -1.094802 -0.360556 -0.215643 -0.047687  0.382801 -0.5335\nPct_65       -0.715331 -0.158253 -0.031353  0.076086  0.464992 -0.1505\nPct_Img      -0.331892 -0.049146  0.077177  0.240755  0.670433  0.2829\nPct_brevet   -0.655221 -0.221655 -0.084954  0.047835  0.598456 -0.2400\nNivVieMed    -1.140895 -0.560649 -0.214717  0.193768  1.311228 -0.3162\nNumber of data points: 506 \nEffective number of parameters (residual: 2traceS - traceS'S): 107.1278 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 398.8722 \nSigma (residual: 2traceS - traceS'S): 4.118116 \nEffective number of parameters (model: traceS): 81.53263 \nEffective degrees of freedom (model: traceS): 424.4674 \nSigma (model: traceS): 3.992025 \nSigma (ML): 3.656286 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 2945.674 \nAIC (GWR p. 96, eq. 4.22): 2829.504 \nResidual sum of squares: 6764.424 \nQuasi-global R2: 0.783008 \n\n\n\n7.3.2.3 Comparaison des modèles MCO et GWR\nLe R2 global du modèle GWR est bien supérieur au modèle classique MCO (0,783 contre 0,283). Fotheringham et al. (2003) proposent plusieurs tests pour comparer les modèles GWR et classique qui sont implémentés dans le package spgwr (fonctions anova(Modele.GWR), anova(Modele.GWR, approx=TRUE), LMZ.F1GWR.test(Modele.GWR), LMZ.F2GWR.test(Modele.GWR).\nSi les valeurs de p de ces tests sont inférieures à 0,05, alors le modèle GWR améliore de façon significative la capacité prédictive du modèle de régression globale, ce que confirment les résultats ci-dessous.\n\nanova(Modele.GWR)\n\nAnalysis of Variance Table \n                    Df  Sum Sq Mean Sq F value\nOLS Residuals     6.00 22346.0                \nGWR Improvement 101.13 15581.6 154.078        \nGWR Residuals   398.87  6764.4  16.959  9.0854\n\nanova(Modele.GWR, approx=TRUE)\n\nAnalysis of Variance Table \napproximate degrees of freedom (only tr(S))\n                     Df  Sum Sq Mean Sq F value\nOLS Residuals     6.000 22346.0                \nGWR Improvement  75.533 15581.6 206.290        \nGWR Residuals   424.467  6764.4  15.936  12.945\n\nLMZ.F1GWR.test(Modele.GWR)\n\n\n    Leung et al. (2000) F(1) test\n\ndata:  Modele.GWR\nF = 0.37946, df1 = 430.81, df2 = 500.00, p-value &lt; 2.2e-16\nalternative hypothesis: less\nsample estimates:\nSS OLS residuals SS GWR residuals \n       22346.021         6764.424 \n\nLMZ.F2GWR.test(Modele.GWR)\n\n\n    Leung et al. (2000) F(2) test\n\ndata:  Modele.GWR\nF = 3.4476, df1 = 142.92, df2 = 500.00, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\n  SS OLS residuals SS GWR improvement \n          22346.02           15581.60 \n\n\nUn autre test (LMZ.F3GWR.test) permet de répondre à la question suivante : est-ce que les coefficients de régression du modèle GWR varient spatialement de façon significative? Les résultats ci-dessous démontrent que c’est le cas pour toutes les variables indépendantes et la constante (p &lt; 0,001).\n\nLMZ.F3GWR.test(Modele.GWR)\n\n\nLeung et al. (2000) F(3) test\n\n            F statistic Numerator d.f. Denominator d.f.     Pr(&gt;)    \n(Intercept)      2.2771       134.3880           430.81 1.629e-10 ***\nPct0_14          2.7767       141.7244           430.81 5.636e-16 ***\nPct_65           2.0918       169.0472           430.81 8.399e-10 ***\nPct_Img          1.9486       106.4400           430.81 1.550e-06 ***\nPct_brevet       2.4445       121.2830           430.81 1.629e-11 ***\nNivVieMed        3.6926       138.8118           430.81 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n7.3.2.4 Cartographie des résultats du modèle GWR\nDans un premier temps, nous ajoutons les valeurs locales des R2, des coefficients de régression et des valeurs de t dans la couche sf. Notez que les résultats locaux de la GWR sont stockés dans l’objet Modele.GWR$SDF.\n\n## Récupération du R carré local\nLyonIris$GWR.R2 &lt;- Modele.GWR$SDF$localR2\n## Récupération des coefficients de régression et calcul des valeurs de t locales\nnames(Modele.GWR$SDF)\n\n [1] \"sum.w\"              \"(Intercept)\"        \"Pct0_14\"           \n [4] \"Pct_65\"             \"Pct_Img\"            \"Pct_brevet\"        \n [7] \"NivVieMed\"          \"(Intercept)_se\"     \"Pct0_14_se\"        \n[10] \"Pct_65_se\"          \"Pct_Img_se\"         \"Pct_brevet_se\"     \n[13] \"NivVieMed_se\"       \"gwr.e\"              \"pred\"              \n[16] \"pred.se\"            \"localR2\"            \"(Intercept)_se_EDF\"\n[19] \"Pct0_14_se_EDF\"     \"Pct_65_se_EDF\"      \"Pct_Img_se_EDF\"    \n[22] \"Pct_brevet_se_EDF\"  \"NivVieMed_se_EDF\"   \"pred.se\"           \n\nVarsIndep &lt;- c(\"Pct0_14\", \"Pct_65\", \"Pct_Img\", \"Pct_brevet\", \"NivVieMed\")\nfor(e in VarsIndep){\n  # Nom des nouvelles variables\n  var.coef &lt;- paste0(\"GWR.\", \"B_\", e)\n  var.t    &lt;- paste0(\"GWR.\", \"T_\", e)\n  # Récupération des coefficients pour les variables indépendantes\n  LyonIris[[var.coef]] &lt;- Modele.GWR$SDF[[e]]\n  # Calcul des valeurs de t pour les variables indépendantes\n  LyonIris[[var.t]]    &lt;- Modele.GWR$SDF[[e]] / Modele.GWR$SDF[[paste0(e, \"_se\")]]\n}\n\nCartographie des R2 locaux\nLe code ci-dessous permet ensuite de cartographier les R2 locaux de la GWR (figure 7.9).\n\nlibrary(tmap)\ntm_shape(LyonIris)+\n  tm_borders(col=\"gray25\", lwd=.5)+\n  tm_fill(col=\"GWR.R2\", \n          palette=\"YlOrBr\", \n          n=5, style=\"quantile\",\n          legend.format = list(text.separator = \"à\"),\n          title = \"R2 locaux\")+\n  tm_layout(frame=FALSE)+\n  tm_scale_bar(breaks=c(0,5))\n\n\n\nFigure 7.9: Cartographie des R carrés locaux de la GWR\n\n\n\nCartographie des coefficients de régression\nLe code ci-dessous permet ensuite de cartographier les coefficients locaux de la GWR (figure 7.10).\n\nCarte1 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"GWR.B_Pct0_14\", palette=\"YlOrBr\", n=4, style=\"pretty\",\n                  legend.format = list(text.separator = \"à\"),\n                  title = \"Moins de 15 ans (%)\")+\n          tm_layout(frame=FALSE, legend.outside = TRUE)+tm_scale_bar(breaks=c(0,5))\nCarte2 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"GWR.B_Pct_65\", palette=\"YlOrBr\", n=4, style=\"pretty\", \n                  legend.format = list(text.separator = \"à\"),\n                  title = \"65 ans et plus (%)\")+\n          tm_layout(frame=FALSE, legend.outside = TRUE)+tm_scale_bar(breaks=c(0,5))\nCarte3 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"GWR.B_Pct_Img\", palette=\"YlOrBr\", n=4, style=\"pretty\", \n                  legend.format = list(text.separator = \"à\"),\n                  title = \"Immigrants (%)\")+\n          tm_layout(frame=FALSE, legend.outside = TRUE)+tm_scale_bar(breaks=c(0,5))\nCarte4 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"GWR.B_Pct_brevet\", palette=\"YlOrBr\", n=4, style=\"pretty\", \n                  legend.format = list(text.separator = \"à\"),\n                  title = \"Faible scolarité (%)\")+\n          tm_layout(frame=FALSE, legend.outside = TRUE)+tm_scale_bar(breaks=c(0,5))\nCarte5 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"GWR.B_NivVieMed\", palette=\"YlOrBr\", n=4, style=\"pretty\", \n                  legend.format = list(text.separator = \"à\"),\n                  title = \"Niveau de vie (€1000)\")+\n          tm_layout(frame=FALSE, legend.outside = TRUE)+tm_scale_bar(breaks=c(0,5))\ntmap_arrange(Carte1, Carte2, Carte3, Carte4, Carte5, ncol = 2, nrow=3)\n\n\n\nFigure 7.10: Cartographie des coefficients de régression de la GWR\n\n\n\nCartographie des valeurs de t\nPour cartographier les valeurs de t, nous utilisons les seuils de ± 1,96, 2,58 et 3,29, indiquant des seuils de signification à 5 %, 1 % et 0,1 % (figure 7.11).\n\nclasses.intervalles = c(-Inf, -3.29, -2.58, -1.96, 1.96, 2.58, 3.29, Inf)\nCarte1 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"GWR.T_Pct0_14\", palette=\"-RdBu\", \n                  breaks = classes.intervalles,\n                  legend.format = list(text.separator = \"à\"),\n                  title = \"Moins de 15 ans (%)\")+\n          tm_layout(frame=FALSE, legend.outside = TRUE)\nCarte2 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"GWR.T_Pct_65\", palette=\"-RdBu\",\n                  breaks = classes.intervalles,\n                  legend.format = list(text.separator = \"à\"),\n                  title = \"65 ans et plus (%)\")+\n          tm_layout(frame=FALSE, legend.outside = TRUE)\nCarte3 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"GWR.T_Pct_Img\", palette=\"-RdBu\", \n                  breaks = classes.intervalles,\n                  legend.format = list(text.separator = \"à\"),\n                  title = \"Immigrants (%)\")+\n          tm_layout(frame=FALSE, legend.outside = TRUE)\nCarte4 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"GWR.B_Pct_brevet\", palette=\"-RdBu\", \n                  breaks = classes.intervalles,\n                  legend.format = list(text.separator = \"à\"),\n                  title = \"Faible scolarité (%)\")+\n          tm_layout(frame=FALSE, legend.outside = TRUE)\nCarte5 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"GWR.T_NivVieMed\", palette=\"-RdBu\", \n                 breaks = classes.intervalles,\n                  legend.format = list(text.separator = \"à\"),\n                 title = \"Niveau de vie (€1000)\")+\n          tm_layout(frame=FALSE, legend.outside = TRUE)+\n          tm_scale_bar(breaks=c(0,5))\ntmap_arrange(Carte1, Carte2, Carte3, Carte4, Carte5, ncol = 2, nrow=3)\n\n\n\nFigure 7.11: Cartographie des valeurs de t de la GWR\n\n\n\nCartographie du nombre de variables significatives\nNous pouvons aussi cartographier le nombre de variables localement significatives aux seuils de 5 % et 1 %.\n\n## Identifier la variable plus significative avec les valeurs de t\nVarsT &lt;- paste0(\"GWR.T_\", c(\"Pct0_14\", \"Pct_65\", \"Pct_Img\", \"Pct_brevet\", \"NivVieMed\"))\nLyon.df  &lt;- st_drop_geometry(LyonIris)\nLyon.df &lt;- abs(Lyon.df[,VarsT])\nPlusSign &lt;- VarsT[apply(Lyon.df[VarsT],1,which.max)]\nPlusSign &lt;- substr(PlusSign, 7, nchar(PlusSign))\nMaxAbsTvalue &lt;- apply(Lyon.df[VarsT], 1, max)\nPlusSign &lt;- ifelse(MaxAbsTvalue&lt;1.96, \"Aucune\", PlusSign)\n## Nombre de variables significatives au seuil de 5%, soit abs(t)= 1,96)\nLyonIris$NbSignif_1.96 &lt;- as.factor(rowSums(Lyon.df &gt; 1.96))\nLyonIris$NbSignif_2.58 &lt;- as.factor(rowSums(Lyon.df &gt; 2.58))\nLyonIris$PlusSign      &lt;- as.factor(PlusSign)\n## Cartographie\nCarte1 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"NbSignif_1.96\", palette=\"Reds\",\n                 title = \"Sign. au seuil de 5%\")+\n          tm_layout(frame=FALSE)+ tm_scale_bar(breaks=c(0,5))\nCarte2 &lt;- tm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"NbSignif_2.58\", palette=\"Reds\", \n                 title = \"Sign. au seuil de 1%\")+\n          tm_layout(frame=FALSE)\ntmap_arrange(Carte1, Carte2, ncol=2, nrow=1)\n\n\n\nFigure 7.12: Nombre de variables significatives aux seuils de 5% et 1%\n\n\n\nCartographie de la variable la plus significative avec la valeur de t\nFinalement, le code ci-dessous permet de repérer la variable la plus significative au seuil de 5 %, c’est-à-dire avec la plus forte valeur absolue pour la valeur de t.\n\ntm_shape(LyonIris)+ tm_borders(col=\"gray25\", lwd=.5)+\n          tm_fill(col=\"PlusSign\", palette=\"Set1\", \n                 title = \"Variable la plus significative\")+\n          tm_layout(frame=FALSE)+ tm_scale_bar(breaks=c(0,5))\n\n\n\nFigure 7.13: Variable indépendante la plus significative au seuil de 5 %\n\n\n\n\n\n\n\n\nExtensions de la GWR classique\n\n\nÀ titre de rappel, la GWR classique permet de modéliser une variable dépendante continue. Plusieurs extensions ont été proposées, notamment :\n\nLa GWR mixte qui permet de spécifier des variables indépendantes variant spatialement et d’autres étant fixes (Fotheringham, Brunsdon et Charlton 2003).\nLes GWR logistique (pour une variable dépendante binaire) et Poisson (pour une variable dépendante de comptage) (Fotheringham, Brunsdon et Charlton 2003).\nLa régression géographiquement et temporellement pondérée (Geographical and temporal weighted regression – GTWR) (Fotheringham, Crespo et Yao 2015).\nLa régression géographiquement pondérée multiéchelle (Multiscale geographically weighted regression – MGWR) (Fotheringham, Yang et Kang 2017).\nL’analyse en composantes principales géographiquement pondérée (Geographically weighted principal components analysis – GWR PCA) (Harris, Brunsdon et Charlton 2011)."
  },
  {
    "objectID": "07-RegressionSpatiales.html#sec-074",
    "href": "07-RegressionSpatiales.html#sec-074",
    "title": "7  Introduction aux modèles de régression spatiale",
    "section": "\n7.4 Quiz de révision du chapitre",
    "text": "7.4 Quiz de révision du chapitre\n\n\n\n\n\nQu’est-ce que la dépendance spatiale d’un modèle de régression?\n\n\nRelisez au besoin la section 7.1.1.\n\n\n\n\n\n\nLorsque les variables indépendantes sont fortement corrélées entre elles.\n\n\n\n\n\n\n\nLorsque les résidus du modèle sont fortement autocorrélés spatialement.\n\n\n\n\n\n\n\n\n\n\nDans un modèle SLX, l’autocorrélation est introduite au niveau de :\n\n\nRelisez au besoin la section 7.1.2.1.\n\n\n\n\n\n\nVariable dépendante\n\n\n\n\n\n\n\nVariables indépendantes\n\n\n\n\n\n\n\nTerme d’erreur\n\n\n\n\n\n\n\nVariable dépendante et variables indépendantes\n\n\n\n\n\n\n\nVariable dépendante et terme d’erreur\n\n\n\n\n\n\n\n\n\n\nDans un modèle SAR, l’autocorrélation est introduite au niveau de :\n\n\nRelisez au besoin la section 7.1.2.2.\n\n\n\n\n\n\nVariable dépendante\n\n\n\n\n\n\n\nVariables indépendantes\n\n\n\n\n\n\n\nTerme d’erreur\n\n\n\n\n\n\n\n\n\n\nDans un modèle SEM, l’autocorrélation est introduite au niveau de :\n\n\nRelisez au besoin la section 7.1.2.3.\n\n\n\n\n\n\nVariable dépendante\n\n\n\n\n\n\n\nVariables indépendantes\n\n\n\n\n\n\n\nTerme d’erreur\n\n\n\n\n\n\n\n\n\n\nDans un modèle mixte SDM, l’autocorrélation est introduite au niveau de :\n\n\nRelisez au besoin la section 7.1.2.4.\n\n\n\n\n\n\nVariable dépendante\n\n\n\n\n\n\n\nVariables indépendantes\n\n\n\n\n\n\n\nTerme d’erreur\n\n\n\n\n\n\n\n\n\n\nDans un modèle SDEM, l’autocorrélation est introduite au niveau de :\n\n\nRelisez au besoin la section 7.1.2.5.\n\n\n\n\n\n\nVariable dépendante\n\n\n\n\n\n\n\nVariables indépendantes\n\n\n\n\n\n\n\nTerme d’erreur\n\n\n\n\n\n\n\n\n\n\nComment est intégré l’espace dans un modèle généralisé additif?\n\n\nRelisez au besoin la section 7.2.1.\n\n\n\n\n\n\nAvec une variable spatialement décalée\n\n\n\n\n\n\n\nAvec une spline bivariée sur les coordonnées x et y\n\n\n\n\n\n\n\n\n\n\nUn modèle de régression géographiquement pondérée permet d’explorer\n\n\nRelisez le deuxième encadré à la section 7.3.1.\n\n\n\n\n\n\nL’instabilité spatiale du modèle\n\n\n\n\n\n\n\nLa dépendance du modèle\n\n\n\n\n\n\n\n\n\n\nUn modèle de régression géographiquement pondérée produit autant de régressions que d’entités spatiales dans le jeu de données à l’étude.\n\n\nRelisez le deuxième encadré à la section 7.3.1.\n\n\n\n\n\n\nVrai\n\n\n\n\n\n\n\nFaux\n\n\n\n\n\n\n\n\n\nVérifier votre résultat"
  },
  {
    "objectID": "07-RegressionSpatiales.html#sec-075",
    "href": "07-RegressionSpatiales.html#sec-075",
    "title": "7  Introduction aux modèles de régression spatiale",
    "section": "\n7.5 Exercices de révision",
    "text": "7.5 Exercices de révision\n\n\n\n\n\nExercice 1. Réalisation de modèles de régression autorégressifs spatiaux\n\n\n\nlibrary(sf)\nlibrary(spatialreg)\n# Matrice de contiguïté selon le partage d'un segment (Rook)\nload(\"data/chap07/DonneesLyon.Rdata\")\nRook &lt;- poly2nb(LyonIris, queen=FALSE)\nRook &lt;- poly2nb(LyonIris, queen=FALSE)\nW.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = \"W\")\n# Modèles\nformule &lt;- \"PM25 ~  Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed\"\nModele.SLX &lt;- à compléter\nModele.SAR &lt;- à compléter\nModele.SEM &lt;- à compléter\nModele.DurbinSpatial &lt;- à compléter\nModele.DurbinErreur &lt;- à compléter\n\nCorrection à la section 9.7.1.\n\n\n\n\n\n\n\nExercice 2. Réalisation d’un modèle GAM\n\n\n\nlibrary(sf)\nlibrary(mgcv)\nload(\"data/chap07/DonneesLyon.Rdata\")\n# Ajout des coordonnées x et y\nxy &lt;- à compléter\nLyonIris$X &lt;- à compléter\nLyonIris$Y &lt;- à compléter\n# Construction du modèle\nformule &lt;- \"PM25 ~  Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed\"\nModele.GAM2 &lt;- gam(NO2 ~  à compléter\n                          à compléter,\n                          data = LyonIris)\nsummary(Modele.GAM2)\n\nCorrection à la section 9.7.2.\n\n\n\n\n\n\n\nExercice 2. Réalisation d’un modèle GWR\n\n\n\nlibrary(sf)\nlibrary(spgwr)\nload(\"data/chap07/DonneesLyon.Rdata\")\n# Ajout des coordonnées x et y\nxy &lt;- à compléter\nLyonIris$X &lt;- à compléter\nLyonIris$Y &lt;- à compléter\n# Optimisation du nombre de voisins avec le CV\nformule &lt;- \"PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed\"\nbwaCV.voisins  &lt;- gwr.sel(à compléter)\n# Optimisation du nombre de voisins avec l'AIC\nbwaCV.voisins  &lt;- gwr.sel(à compléter)\n# Réalisation de la GWR\nModele.GWR &lt;- gwr(à compléter)\n# Affichage des résultats\nModele.GWR\n\nCorrection à la section 9.7.3.\n\n\n\n\n\n\nAnselin, Luc, Anil K Bera, Raymond Florax et Mann J Yoon. 1996. « Simple diagnostic tests for spatial dependence. » Regional science and urban economics 26 (1): 77‑104. https://doi.org/10.1016/0166-0462(95)02111-6.\n\n\nAnselin, Luc et Sergio J Rey. 2014. Modern spatial econometrics in practice: A guide to GeoDa, GeoDaSpace and PySAL. GeoDa Press LLC.\n\n\nApparicio, Philippe et Jérémy Gelb. 2022. Méthodes quantitatives en sciences sociales : un grand bol d’R. FabriqueREL, Licence CC BY-SA. https://laeq.github.io/LivreMethoQuantBolR/.\n\n\nApparicio, Philippe, Anne-Marie Séguin et Xavier Leloup. 2007. « Modélisation spatiale de la pauvreté à Montréal: apport méthodologique de la régression géographiquement pondérée. » The Canadian Geographer/Le Géographe canadien 51 (4): 412‑427. https://doi.org/10.1111/j.1541-0064.2007.00189.x.\n\n\nBivand, Roger, Giovanni Millo et Gianfranco Piras. 2021. « A review of software for spatial econometrics in R. » Mathematics 9 (11): 1276. https://doi.org/10.3390/math9111276.\n\n\nBivand, Roger, Edzer J Pebesma, Virgilio Gómez-Rubio et Edzer Jan Pebesma. 2008. Applied spatial data analysis with R. Vol. 747248717. Springer.\n\n\nBivand, Roger et Danlin Yu. 2023. spgwr: Geographically Weighted Regression. s.n. https://CRAN.R-project.org/package=spgwr.\n\n\nDubé, Jean et Diègo Legros. 2014. Econométrie spatiale appliquée des microdonnées. ISTE Group.\n\n\nFotheringham, A Stewart, Chris Brunsdon et Martin Charlton. 2003. Geographically weighted regression: the analysis of spatially varying relationships. John Wiley & Sons.\n\n\nFotheringham, A Stewart, Ricardo Crespo et Jing Yao. 2015. « Geographical and temporal weighted regression (GTWR). » Geographical Analysis 47 (4). Wiley Online Library: 431‑452. https://doi.org/10.1111/gean.12071.\n\n\nFotheringham, A Stewart, Wenbai Yang et Wei Kang. 2017. « Multiscale geographically weighted regression (MGWR). » Annals of the American Association of Geographers 107 (6). Taylor & Francis: 1247‑1265. https://doi.org/10.1080/24694452.2017.1352480.\n\n\nHarris, Paul, Chris Brunsdon et Martin Charlton. 2011. « Geographically weighted principal components analysis. » International Journal of Geographical Information Science 25 (10). Taylor & Francis: 1717‑1736. https://doi.org/10.1080/13658816.2011.554838.\n\n\nLeSage, James P et R. Kelly Pace. 2008. An introduction to spatial econometrics. 123. CRC Press.\n\n\nWood, Simon N. 2011. « Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models. » Journal of the Royal Statistical Society Series B: Statistical Methodology 73 (1): 3‑36. https://doi.org/10.1111/j.1467-9868.2010.00749.x."
  },
  {
    "objectID": "08-ClassificationsSpatiales.html#sec-081",
    "href": "08-ClassificationsSpatiales.html#sec-081",
    "title": "8  Méthodes de classification non supervisée spatiale",
    "section": "\n8.1 Méthodes de classification non supervisée avec contrainte spatiale",
    "text": "8.1 Méthodes de classification non supervisée avec contrainte spatiale\nNous avons vu que l’objectif d’une méthode non supervisée appliquée à des données spatiales est de regrouper en n classes les unités spatiales d’une couche géographique. Prenons l’exemple de quatre variables environnementales cartographiées à la figure 8.1 pour les IRIS de la ville de Lyon, dont trois considérées comme des nuisances (bruit, dioxyde d’azote et particules fines) et une considérée comme avantageuse (végétation).\n\n\n\n\nFigure 8.1: Cartographie des variables environnementales\n\n\n\nIl est possible de regrouper les unités spatiales avec ou sans contrainte spatiale :\n\nSans contrainte spatiale, nous cherchons à regrouper les IRIS (unités spatiales) avec des valeurs similaires pour les quatre variables retenues (Lden, NO2, PM2,5 et pourcentage de canopée). Cette approche est illustrée à la figure 8.2 (a) avec l’algorithme k-moyennes (k-means en anglais) avec cinq classes.\nAvec contrainte spatiale, nous cherchons à regrouper les IRIS (unités spatiales) avec des valeurs similaires pour les quatre variables retenues, tout en nous assurant que les regroupements forment des régions avec une absence de mitage. Cette approche est illustrée à la figure 8.2 (b) avec l’algorithme SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) avec cinq classes. Autrement dit, l’objectif des méthodes de classification non supervisée avec contrainte spatiale est d’agréger n unités spatiales en m régions non discontinues (avec n &lt; m) et cohérentes du point de vue de leurs attributs (Openshaw et Rao 1995, 428).\n\n\n\n\n\nFigure 8.2: Classification non supervisée avec et sans contrainte spatiale\n\n\n\n\n\n\n\n\nIntérêt et limites des méthodes de classification avec une contrainte spatiale\n\n\nSelon Gelb et Apparicio (2021, 7), le résultat d’une méthode de classification avec une contrainte spatiale est « la création de régions très cohérentes spatialement, c’est-à-dire avec une absence de mitage. Autrement dit, avec ces méthodes, il n’est pas possible d’identifier de groupes qui seraient spatialement discontinus, c’est-à-dire composés de plusieurs ensembles régionaux séparés. L’impossibilité d’obtenir du mitage au sein des différentes régions peut masquer la présence de valeurs fortement dissemblables localement, malgré la prise en compte de l’espace. Or, ces observations systématiquement différentes de leurs voisines doivent faire l’objet d’une attention particulière dans les exercices de classification intégrant l’espace, ce que ne permettent pas ces méthodes d’agrégation spatiale.\nLes limites de ces méthodes, particulièrement celles relatives au mitage, ont conduit plus récemment à la mise au point de nouvelles méthodes incluant l’espace dans le processus de classification, sans imposer une contrainte de contiguïté. Plus spécifiquement, ces nouvelles méthodes sont des modifications des algorithmes classiques, tels que la CAH ou le FCM, pour intégrer la dimension spatiale en parallèle à la dimension sémantique des données. En d’autres termes, l’espace n’est plus intégré comme une contrainte dans les algorithmes de classification, mais plutôt comme une donnée supplémentaire ».\n\n\nLes principaux algorithmes de classification non supervisée avec contrainte spatiale (Spatially Constrained Clustering Methods en anglais) sont :\n\nLa méthode de zonage automatique (Automatic Zoning Procedure en anglais) (AZP) proposée par Openshaw (1977), puis améliorée par Openshaw et Rao (1995).\nL’algorithme SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) (Assunção et al. 2006).\nL’algorithme REDCAP (Regionalization with dynamically constrained agglomerative clustering and partitioning) (Guo 2008).\nL’algorithme du max-p-regions problem (Duque, Anselin et Rey 2012).\n\nPour mettre en œuvre ces différents algorithmes, nous utilisons le package rgeoda (Li et Anselin 2023). Notez que l’algorithme SKATER est aussi implémenté dans le package spded (fonction skater).\n\n8.1.1 Algorithmes AZP\nL’algorithme AZP (Automatic Zoning Problem) est une approche itérative et heuristique visant à regrouper des polygones adjacents en m régions, tout en maximisant la variance interrégionale (variance interclasse) et en minimisant la variance intrarégionale (variance intraclasse) calculées sur les p variables. Autrement dit, il vise à créer des régions non discontinues les plus homogènes possibles et les plus dissemblables entre elles sur la base des p variables. Pour utiliser l’AZP, il faut spécifier le nombre de régions (m) désiré. Notez qu’il existe trois algorithmes pour l’AZP :\n\nAZP (Automatic Zoning Procedure), soit la première version par Stan Openshaw (1977).\nAZP-SA (A simulated annealing AZP method) (Openshaw et Rao 1995).\nAZP-TABU (A tabu search heuristic version of AZP) (Openshaw et Rao 1995).\n\nPour une description détaillée de ces trois algorithmes, vous pouvez consulter Openshaw et Rao (1995) ou encore le lien suivant.\nAppliquons ces algorithmes aux 506 IRIS de la ville de Lyon avec les quatre variables environnementales préalablement centrées réduites (bruit, dioxyde d’azote, particules fines et pourcentage de végétation) et une matrice de contiguïté selon le partage d’un nœud. Le package rgeoda comprend trois fonctions pour l’AZP : azp_greedy (AZP), azp_sa (AZP-SA), azp_tabu (AZP-TABU). Pour l’exercice, nous fixons le nombre de régions à 5. Notez que par défaut, les variables seront centrées réduites (moyenne = 0 et écart-type = 1) avec le paramètre scale_method=\"standardize\".\n\nlibrary(rgeoda)\nlibrary(sf)\nlibrary(tmap)\n## Variables\nVarsEnv &lt;- c(\"Lden\", \"NO2\", \"PM25\", \"VegHautPrt\")\n## Dataframe sans la géométrie et les quatre variables\nload(\"data/chap08/DonneesLyon.Rdata\")\nData &lt;- st_drop_geometry(LyonIris[VarsEnv])\n## Création d'une matrice de contiguïté avec rgeoda\nqueen_w &lt;- queen_weights(LyonIris)\n## Calcul des trois algorithmes\nazp &lt;- rgeoda::azp_greedy(p=5,       # Nombre de régions\n                          w=queen_w, # Matrice contiguïté\n                          df=Data,   # Tableau de données\n                          scale_method = \"standardize\") # cote z\nazp.sa &lt;- rgeoda::azp_sa(p=5, w=queen_w, df=Data, cooling_rate = 0.85)\nazp.tab &lt;- rgeoda::azp_tabu(p=5, w=queen_w, df=Data, tabu_length = 10, conv_tabu = 10)\n## Création des trois champs dans la couche de Lyon\nLyonIris$Azp &lt;- as.character(azp$Clusters)\nLyonIris$Azp_sa &lt;- as.character(azp.sa$Clusters)\nLyonIris$Azp_tab &lt;- as.character(azp.tab$Clusters)\n\nCartographions les résultats des trois algorithmes AZP (figure 8.3).\n\n## Cartographie des résultats\nCarte.AZP1 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n              tm_fill(col=\"Azp\", palette = \"Set1\", title =\"\")+\n              tm_layout(frame=FALSE, \n                main.title = \"a. AZP\", \n                main.title.position = \"center\", \n                main.title.size = 1)\nCarte.AZP2 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n              tm_fill(col=\"Azp_sa\", palette = \"Set1\", title =\"\")+\n              tm_layout(frame=FALSE, \n                main.title = \"b. AZP Simulated Annealing\", \n                main.title.position = \"center\", \n                main.title.size = 1)\nCarte.AZP3 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n              tm_fill(col=\"Azp_tab\", palette = \"Set1\", title =\"\")+\n              tm_layout(frame=FALSE, \n                main.title = \"c. AZP Tabu Search\", \n                main.title.position = \"center\", \n                main.title.size = 1)\ntmap_arrange(Carte.AZP1, Carte.AZP2, Carte.AZP3, ncol = 2, nrow = 2)\n\n\n\nFigure 8.3: Regroupements des 505 IRIS en cinq régions selon les trois algorithmes AZP\n\n\n\nPar la suite, nous comparons les résultats obtenus des trois algorithmes en reportant :\n\nLes variances totale, intrarégionale et interrégionale, et surtout le ratio entre les variances intergroupe et totale. Ce ratio varie de 0 à 1 et exprime la proportion de la variance des variables qui est expliquée par les différentes régions obtenues; plus il est élevé, meilleur est le résultat. Par conséquent, il peut être utilisé pour identifier la solution optimale entre les trois algorithmes.\nLe nombre d’observations par région.\nLes valeurs moyennes des variables centrées réduites par région.\n\n\n## Calcul du ratio entre les variances intergroupe et totale\ncat(\"Ratio des variances interrégionale et totale\",\n    \"\\nAZP : \", round(azp$`The ratio of between to total sum of squares`, 3),\n    \"\\nAZP-SA : \", round(azp.sa$`The ratio of between to total sum of squares`, 3),\n    \"\\nAZP-TABU : \", round(azp.tab$`The ratio of between to total sum of squares`, 3)\n)\n\nRatio des variances interrégionale et totale \nAZP :  0.436 \nAZP-SA :  0.518 \nAZP-TABU :  0.428\n\n\nÀ la lecture des valeurs du ratio entre la variance interrégionale et la variance totale ci-dessus, la plus élevée est obtenue pour l’AZP-SA (0,518), suivie de celles de l’AZP (0,436) et de l’AZP-TABU (0,428). Nous retenons alors l’AZP-SA.\n\n## Nombre d'observations par région\ntable(LyonIris$Azp)\n\n\n  1   2   3   4   5 \n186 176 104  27  13 \n\ntable(LyonIris$Azp_sa)\n\n\n  1   2   3   4   5 \n221 107  87  51  40 \n\ntable(LyonIris$Azp_tab)\n\n\n  1   2   3   4   5 \n191 186  91  27  11 \n\n## Valeurs moyennes des variables centrées réduites par région\nData$Azp &lt;- azp$Clusters\nData$Azp_sa &lt;- azp.sa$Clusters\nData$Azp_tab &lt;- azp.tab$Clusters\naggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ Azp, data = Data, FUN = mean)\n\n  Azp     Lden      NO2     PM25 VegHautPrt\n1   1 58.05464 35.21961 18.85212   15.45333\n2   2 55.42159 26.45169 16.32884   13.98563\n3   3 53.23122 23.63965 14.95293   30.42529\n4   4 52.26742 22.99701 14.58065   20.39444\n5   5 48.71652 18.24301 13.07043   33.07154\n\naggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ Azp_sa, data = Data, FUN = mean)\n\n  Azp_sa     Lden      NO2     PM25 VegHautPrt\n1      1 57.41952 35.02151 18.80132   14.41317\n2      2 55.11019 22.52804 15.55562   14.12178\n3      3 51.08297 20.48809 14.16951   28.66000\n4      4 54.23625 24.47389 14.96836   22.30059\n5      5 58.40430 33.55155 17.08480   28.83775\n\naggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ Azp_tab, data = Data, FUN = mean)\n\n  Azp_tab     Lden      NO2     PM25 VegHautPrt\n1       1 55.24285 26.04447 16.19300   14.67712\n2       2 58.05464 35.21961 18.85212   15.45333\n3       3 53.19298 23.97221 14.99671   31.90967\n4       4 52.26742 22.99701 14.58065   20.39444\n5       5 48.32861 17.74683 12.84853   31.68364\n\n\nLes résultats finaux de l’AZP-SA sont présentés au tableau 8.1 et à la figure 8.4. L’analyse conjointe du tableau et de la carte permet ainsi d’interpréter chacune des classes. En guise d’exemple, nous pouvons conclure que :\n\nLa région 1 comprend 221 IRIS localisés au centre de la ville de Lyon et caractérisés par des niveaux moyens élevés de bruit (57,4), de dioxyde d’azote (35) et de particules fines (18,8) élevés et un faible pourcentage de canopée (14,4 %).\nPar contre, la région 2 comprend 107 IRIS localisés à l’extrême ouest de la ville et caractérisés par les plus faibles niveaux de polluants (51,1, 20,5 et 14,2) et une forte moyenne pour la canopée (28,7 %).\n\n\n\n\n\nTableau 8.1: Valeurs moyennes des variables pour les cinq régions obtenues par l’AZP-SA\n\nRégion\nLden\nNO2\nPM25\nVégétation\nNombre d’IRIS\n\n\n\n1\n57,4\n35,0\n18,8\n14,4\n221\n\n\n2\n55,1\n22,5\n15,6\n14,1\n107\n\n\n3\n51,1\n20,5\n14,2\n28,7\n87\n\n\n4\n54,2\n24,5\n15,0\n22,3\n51\n\n\n5\n58,4\n33,6\n17,1\n28,8\n40\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: Regroupement des IRIS en cinq régions selon l’AZP-TABU\n\n\n\nNous avons vu que pour les algorithmes AZP, il faut spécifier le nombre de régions. Nous l’avons fixé arbitrairement à 5. Comme pour n’importe quelle méthode de classification non supervisée, déterminer le nombre de classes optimal est une étape cruciale qui peut s’appuyer sur différentes techniques, dont la méthode du coude basée sur l’inertie expliquée (ici le ratio entre les variances interrégionale et totale), l’indicateur de silhouette et la méthode GAP. Pour une description détaillée de ces méthodes, consultez la section suivante (Apparicio et Gelb 2022). Le code ci-dessous permet de réaliser un graphique avec les valeurs du ratio (inertie expliquée) obtenues avec l’algorithme AZP-TABU calculé pour 2 à 10 régions. À la lecture de la figure 8.5, nous observons deux ruptures (coudes) très nettes à 5 et 8.\n\nlibrary(ggplot2)\nnregions &lt;- 2:10\nData &lt;- data.frame(scale(st_drop_geometry(LyonIris)[VarsEnv]))\nqueen_w &lt;- queen_weights(LyonIris)\ninertie &lt;- sapply(nregions, function(k){\n  # calcul de l'AZP-TABU avec k\n  resultat &lt;- azp_tabu(p=k, w=queen_w, df=Data, tabu_length = 10, conv_tabu = 10)\n  # récupération du ratio\n  ratios &lt;- resultat$`The ratio of between to total sum of squares`\n  return(ratios)\n})\n\ndf &lt;- data.frame(k = nregions, ratio = inertie)\nggplot(df) + \n  geom_line(aes(x = k, y = ratio)) + \n  geom_point(aes(x = k, y = ratio), color = \"red\") + \n  labs(x = \"Nombre de régions\", y = \"Inertie expliquée (%)\")\n\n\n\nFigure 8.5: Méthode du coude reposant sur l’inertie expliquée pour l’AZP-TABU\n\n\n\n\n8.1.2 Algorithme SKATER\nL’algorithme SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) (Assunção et al. 2006) permet aussi de créer des régions sans discontinuité, en recourant à une technique de la théorie des graphes, soit celle de l’arbre couvrant de poids minimal (minimum spanning tree). Succinctement, la classification est obtenue avec les étapes suivantes :\n\nCréation d’un graphe de connectivité pour les polygones de la couche géographique. Dans ce graphe, les nœuds sont les centroïdes des polygones et les arêtes représentent les liaisons entre deux entités spatiales voisines.\nPour chaque arête, nous calculons la dissimilarité (appelée coût) des deux polygones voisins en fonction des p variables.\nPour chaque polygone, nous retenons l’arête avec le coût minimal. Autrement dit, pour chaque polygone, nous retenons son polygone voisin qui lui est le plus semblable selon les p variables. Nous obtenons ainsi l’arbre couvrant de poids minimal.\nCet arbre est ensuite élagué en supprimant les arêtes avec les plus forts coûts et en créant ainsi des sous-graphes en m régions sans discontinuité.\n\nPour une description plus détaillée de l’algorithme, consultez l’article d’Assunção et al. (2006).\nLe code ci-dessous permet de centrer et de réduire les quatre variables (fonction scale) et de construire la matrice de voisinage entre les polygones de la couche LyonIris (fonction poly2nb de spdep).\n\nlibrary(spdep)\nlibrary(tmap)\n## Variables\nVarsEnv &lt;- c(\"Lden\", \"NO2\", \"PM25\", \"VegHautPrt\")\n## Dataframe sans la géométrie et les quatre variables\nload(\"data/chap08/DonneesLyon.Rdata\")\nData &lt;- st_drop_geometry(LyonIris[VarsEnv])\n## Données centrées et réduites\nLyonIrisZscore &lt;- data.frame(scale(Data))\n## Matrice voisinage\nLyon.nb &lt;- poly2nb(LyonIris)\n\nCalculons les coûts pour les arêtes reliant les nœuds avec la fonction nbcosts. Nous constatons que le polygone 1 est voisin des polygones 27, 26, 44 et 74 avec des coûts de 1,34, 1,74, 1,15 et 16,3. Par conséquent, parmi ses quatre voisins, le polygone 1 est le plus semblable au polygone 44 (coût minimal).\n\n## Calcul des coûts pour les arêtes\nlcosts &lt;- nbcosts(Lyon.nb, LyonIrisZscore)\nhead(Lyon.nb, n=1)\n\n[[1]]\n[1] 27 36 44 73\n\nhead(lcosts, n=1)\n\n[[1]]\n[1] 1.343210 1.735894 1.153787 1.632583\n\n\nÀ partir de ces coûts, nous pouvons trouver l’arbre couvrant de poids minimal (minimum spanning tree), objet dénommé ici Lyon.mst qui comprend trois colonnes :\n\nLa première pour l’identifiant du polygone.\nLa seconde pour l’identifiant du polygone voisin.\nLa troisième pour la valeur du coût minimal (similarité selon les variables retenues).\n\n\n## Matrice de pondération spatiale avec les coûts\nLyon.w &lt;- nb2listw(Lyon.nb, lcosts, style=\"B\")\n### Trouver l'arbre couvrant de poids minimal\nLyon.mst &lt;- mstree(Lyon.w)\nhead(Lyon.mst, n=3)\n\n     [,1] [,2]      [,3]\n[1,]  499  478 0.5025378\n[2,]  478   46 0.4618205\n[3,]   46    7 1.3665949\n\n\nLe code ci-dessous permet de visualiser le graphe de connectivité et l’arbre couvrant de poids minimal (figure 8.6).\n\n## Visualisation du graphe de connectivité \ncoords &lt;- st_coordinates(st_centroid(LyonIris))\nplot(st_geometry(LyonIris), border=\"gray\", lwd=.5, col=\"wheat\")\nplot(Lyon.nb, coords, add=TRUE, col=\"red\", lwd=1)\n## Visualisation de l'arbre couvrant de poids minimal\nplot(st_geometry(LyonIris), border=\"gray\", lwd=.5, col=\"wheat\")\nplot(Lyon.mst, coords, col=\"blue\", cex.lab=0.7, add=TRUE)\n\n\n\nFigure 8.6: Graphe de connectivité et arbre couvrant de poids minimal\n\nLe code ci-dessous permet de réaliser une classification SKATER avec cinq régions avec le package spdep.\n\n## SKATER avec le package spdep\nset.seed(123456789)\nskater5.spdep &lt;- spdep::skater(edges = Lyon.mst[,1:2], # premières colonnes de l'arbre \n                               data = data.frame(LyonIrisZscore),\n                               method = \"euclidean\",\n                               ncuts = 4)  # k-1 régions\ntable(skater5.spdep$groups)\n\n\n  1   2   3   4   5 \n 48 215  89  57  97 \n\n\nToutefois, il est plus simple d’utiliser la fonction skater de rgeoda qui ne nécessite pas de créer au préalable l’arbre couvrant de poids minimal.\n\n## SKATER avec le package rgeoda\nlibrary(rgeoda)\nData &lt;- st_drop_geometry(LyonIris[VarsEnv])\nqueen_w &lt;- queen_weights(LyonIris)\nskater5.rgeoda &lt;- rgeoda::skater(k = 5,        # k-1 régions\n                                 w = queen_w,  # matrice de contiguïté\n                                 scale_method = \"standardize\",\n                                 df = Data)    # dataframe\ntable(skater5.rgeoda$Clusters)\n\n\n  1   2   3   4   5 \n248 125  51  48  34 \n\n\nLa figure 8.7 démontre que les résultats obtenus sont légèrement différents avec les deux packages.\n\nLyonIris$skater5spdep &lt;- as.character(skater5.spdep$groups)\nLyonIris$skater5rgeoda &lt;- as.character(skater5.rgeoda$Clusters)\n\nCarte.SkaterA &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n              tm_fill(col=\"skater5spdep\", palette = \"Set1\", title =\"\")+\n              tm_layout(frame=FALSE, \n                main.title = \"a. SKATER spdep\", \n                main.title.position = \"center\", \n                main.title.size = 1)\nCarte.SkaterB &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n              tm_fill(col=\"skater5rgeoda\", palette = \"Set1\", title =\"\")+\n              tm_layout(frame=FALSE, \n                main.title = \"b. SKATER rgeoda\", \n                main.title.position = \"center\", \n                main.title.size = 1)\ntmap_arrange(Carte.SkaterA, Carte.SkaterB)\n\n\n\nFigure 8.7: Résultats de l’algorithme SKATER avec cinq classes obtenus avec les packages spdep et rgeoda\n\n\n\n\n\n\n\n\nAlgorithme SKATER avec un seuil minimal pour les classes\n\n\nDans une classification non supervisée avec une contrainte spatiale, il est possible de fixer un seuil minimal pour chaque région à partir d’une variable. L’exemple le plus classique est l’obtention de p régions qui doivent au moins avoir un nombre d’habitants fixé par la personne utilisatrice. Pour ce faire, nous utilisons deux paramètres de la fonction spdep::skater, soit crit = 50000 pour fixer le seuil et vec.crit = df$Population pour indiquer le vecteur sur lequel est calculé le critère.\n\nclus10_min &lt;- spdep::skater(edges = ct_mst[,1:2], \n                     # dataframe avec les variables centrées réduites\n                     data = dfs,               \n                     # seuil fixé\n                     crit = 50000,             \n                     # variable population du dataframe\n                     vec.crit = df$Population, \n                     ncuts = 4)\n\nFonction skater : différences entre les packagesrgeoda et spdep\nLa fonction skater de rgeoda a deux principaux avantages :\n\nComme décrit précédemment, l’avantage de la fonction skater de rgeoda est qu’elle ne nécessite pas de calculer au préalable l’arbre couvrant de poids minimal.\nscale_method = c(“raw”, “standardize”, “demean”, “mad”, “range_standardize”, “range_adjust”) permet de transformer directement les variables. La méthode par défaut est la cote z (moyenne = 0 et écart-type = 1).\n\nAvec la fonction skater de spdep, vous devez préalablement transformer vos variables et construire l’arbre couvrant de poids minimal. Par contre, elle intègre de nombreux types de distance pour évaluer la dissimilarité entre les unités spatiales avec le paramètre method = c(\"euclidean\",  \"maximum\", \"manhattan\", \"canberra\", \"binary\", \"minkowski\",  \"mahalanobis\") tandis que le paramètre distance_method = c(\"euclidean\", \"manhattan\") de rgeoda ne comprend que deux types de distance.\n\n\n\n8.1.3 Algorithmes REDCAP\nLes différentes versions de l’algorithme REDCAP (Regionalization with dynamically constrained agglomerative clustering and partitioning) proposé par Diansheng Guo (2008) sont aussi basées sur la construction d’un arbre (spanning tree) dont l’élagage est obtenu de cinq différentes façons :\n\nPremier ordre et saut minimal (First-order and Single-linkage) qui fournit un résultat identique à l’algorithme SKATER.\nOrdre complet et saut maximal (Full-order and Complete-linkage).\nOrdre complet et saut moyen (Full-order and Average-linkage).\nOrdre complet et saut minimal (Full-order and Single-linkage).\nOrdre complet et critère de Ward (Full-order and Ward-linkage).\n\nLe code ci-dessous permet de calculer les cinq versions de l’algorithmes REDCAP avec cinq régions et de comparer leurs résultats à partir du ratio (entre les variances interrégionale et totale) et du nombre d’observations par région.\n\nlibrary(rgeoda)\nlibrary(sf)\n## Préparation des données \nData &lt;- st_drop_geometry(LyonIris[VarsEnv])\nqueen_w &lt;- queen_weights(LyonIris)\n## Algorithmes REDCAP\nredcap5.A &lt;- redcap(k = 5, w = queen_w, scale_method = \"standardize\", df = Data,\n                    method = \"firstorder-singlelinkage\")\nredcap5.B &lt;- redcap(k = 5, w = queen_w, scale_method = \"standardize\", df = Data,\n                    method = \"fullorder-completelinkage\")\nredcap5.C &lt;- redcap(k = 5, w = queen_w, scale_method = \"standardize\", df = Data,\n                    method = \"fullorder-averagelinkage\")\nredcap5.D &lt;- redcap(k = 5, w = queen_w, scale_method = \"standardize\", df = Data,\n                    method = \"fullorder-singlelinkage\")\nredcap5.E &lt;- redcap(k = 5, w = queen_w, scale_method = \"standardize\", df = Data,\n                    method = \"fullorder-wardlinkage\")\n## Comparaison des résultats\nRatios &lt;- data.frame(Methode = c(\"firstorder-singlelinkage\", \n                                  \"fullorder-completelinkage\", \n                                  \"fullorder-averagelinkage\",\n                                  \"fullorder-singlelinkage\", \n                                  \"fullorder-wardlinkage\"),\n                      ratio = c(redcap5.A$`The ratio of between to total sum of squares`,\n                                redcap5.B$`The ratio of between to total sum of squares`,\n                                redcap5.C$`The ratio of between to total sum of squares`,\n                                redcap5.D$`The ratio of between to total sum of squares`,\n                                redcap5.E$`The ratio of between to total sum of squares`)\n                      )\nNobs &lt;- data.frame(rbind(table(redcap5.A$Clusters), \n              table(redcap5.B$Clusters), \n              table(redcap5.C$Clusters), \n              table(redcap5.D$Clusters), \n              table(redcap5.E$Clusters))\n              )\nnames(Nobs) &lt;- c(\"C1\", \"C2\", \"C3\", \"C4\", \"C5\")\nRatios &lt;- cbind(Ratios, Nobs)\nRatios\n\n                    Methode     ratio  C1  C2  C3 C4 C5\n1  firstorder-singlelinkage 0.4081577 248 125  51 48 34\n2 fullorder-completelinkage 0.4728026 173 156 115 47 15\n3  fullorder-averagelinkage 0.4993184 148 141 106 63 48\n4   fullorder-singlelinkage 0.3976055 227 102  73 53 51\n5     fullorder-wardlinkage 0.5185455 166 134 120 51 35\n\n\nÀ la lecture des valeurs du ratio ci-dessous, la meilleure classification serait celle obtenue avec un ordre complet et le critère de Ward. Cartographions les résultats des quatre versions de l’algorithme RECAP avec un ordre complet (figure 8.8).\n\n## Ajout des champs dans la couche\nLyonIris$RC5.FOcompletelinkage  &lt;- as.character(redcap5.B$Clusters)\nLyonIris$RC5.FOaveragelinkage   &lt;- as.character(redcap5.C$Clusters)\nLyonIris$RC5.FOsinglelinkage    &lt;- as.character(redcap5.D$Clusters)\nLyonIris$RC5.FOwardlinkage      &lt;- as.character(redcap5.E$Clusters)\n## Cartographie des résultats\nCarte.RCb &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n              tm_fill(col=\"RC5.FOcompletelinkage\", palette = \"Set1\", title =\"\")+\n              tm_layout(frame=FALSE, \n                main.title = \"a. Saut maximal\", \n                main.title.position = \"center\", \n                main.title.size = 1)\nCarte.RCc &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n              tm_fill(col=\"RC5.FOaveragelinkage\", palette = \"Set1\", title =\"\")+\n              tm_layout(frame=FALSE, \n                main.title = \"b. Saut moyen\", \n                main.title.position = \"center\", \n                main.title.size = 1)\nCarte.RCd &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n              tm_fill(col=\"RC5.FOsinglelinkage\", palette = \"Set1\", title =\"\")+\n              tm_layout(frame=FALSE, \n                main.title = \"c. Saut minimal\", \n                main.title.position = \"center\", \n                main.title.size = 1)\nCarte.RCe &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n              tm_fill(col=\"RC5.FOwardlinkage\", palette = \"Set1\", title =\"\")+\n              tm_layout(frame=FALSE, \n                main.title = \"d. Critère de Ward\", \n                main.title.position = \"center\", \n                main.title.size = 1)\ntmap_arrange(Carte.RCb, Carte.RCc, Carte.RCd, Carte.RCe, ncol = 2, nrow = 2)\n\n\n\nFigure 8.8: Regroupements des 505 IRIS en cinq régions selon les quatre versions de l’algorithme REDCAP avec un lien complet\n\n\n\n\n8.1.4 Algorithme du max-p-regions problem\n\nCet algorithme, proposé par Duque et al. (2012), n’est pas décrit ici. Notez qu’il peut être calculé avec trois fonctions du package rgeoda, soit maxp_greedy, maxp_sa et maxp_tabu."
  },
  {
    "objectID": "08-ClassificationsSpatiales.html#sec-082",
    "href": "08-ClassificationsSpatiales.html#sec-082",
    "title": "8  Méthodes de classification non supervisée spatiale",
    "section": "\n8.2 Méthodes de classification non supervisée avec une dimension spatiale",
    "text": "8.2 Méthodes de classification non supervisée avec une dimension spatiale\nNous avons vu que les méthodes de classification avec une contrainte spatiale visent à obtenir des régions non discontinues, c’est-à-dire sans mitage spatial. L’objectif des méthodes de classification non supervisée avec une dimension spatiale est quelque peu différent : classifier les observations en tenant compte de l’espace (proximité, voisinage entre les unités spatiales) afin de limiter les effets de mitage, sans toutefois l’interdire.\nDans le cadre de cette section, nous décrivons deux de ces méthodes qui intègrent la dimension spatiale de manière différente :\n\nLa méthode ClustGeo, qui est une extension de la classification ascendante hiérarchique, est une méthode de classification non supervisée, spatiale et stricte. Cette méthode repose sur deux matrices de dissimilarité : une matrice des distances sémantiques (attributaires) calculée sur les valeurs de plusieurs variables caractérisant les observations et une matrice de distances (euclidienne le plus souvent) entre les entités géographiques. Nous cherchons ainsi à regrouper les observations qui se ressemblent à la fois selon leurs attributs et selon leur proximité spatiale.\nLa méthode k-moyennes spatiale et floue (Spatial fuzzy c-means), qui est une extension de la méthode k-moyennes, est une méthode de classification non supervisée, spatiale et floue. Cette méthode repose sur deux matrices de dissimilarité : une matrice sémantique calculée sur les valeurs de plusieurs variables caractérisant les entités géographiques et une matrice sémantique spatialement décalée. Nous cherchons ainsi à regrouper les observations qui se ressemblent à la fois selon leurs caractéristiques et celles de leurs unités spatiales adjacentes ou proches.\n\nAutrement dit, dans la méthode ClustGeo, l’espace est introduit sous la forme d’une matrice de distances entre les entités spatiales (agencement spatial) tandis que dans la méthode du Spatial fuzzy c-means, il est introduit sous la forme d’une matrice de données sémantiques spatialement décalées (information sémantique dans l’environnement immédiat).\n\n8.2.1 Classification ascendante hiérarchique spatiale (ClustGeo)\n\n8.2.1.1 Description de la méthode ClustGeo\n\nLa méthode ClustGeo, proposée par Marie Chavent et ses collègues (2018), est une extension de la classification ascendante hiérarchique (CAH) qui intègre la dimension spatiale des entités géographiques. Cette méthode repose sur une idée brillante, soit de classer (regrouper) les observations (unités spatiales) en combinant deux matrices de dissimilarité :\n\nUne matrice sémantique calculée sur p variables caractérisant les unités spatiales (\\(D_0\\)).\n\nUne matrice spatiale calculée à partir des distances spatiales (habituellement euclidienne) entre les unités spatiales (\\(D_1\\)). Ces deux matrices sont ensuite fusionnées en une seule matrice finale représentant la combinaison de la distance spatiale et de la dissimilarité sémantique (attributaire) entre les observations. Notez qu’un paramètre \\(\\alpha\\), variant de 0 à 1, permet de définir le poids de la matrice spatiale comparativement à celui de la sémantique :\n\nAvec \\(\\alpha=0\\), le poids accordé à la matrice spatiale est nul. Nous obtenons ainsi une CAH classique puisque seules les différences attributaires sont conservées.\nAvec \\(\\alpha=1\\), le poids accordé à la matrice spatiale est maximal; la classification est alors purement spatiale et ignore les différences attributaires.\n\n\n\nPar conséquent, « […] l’enjeu principal est de fixer la valeur du paramètre 𝛼, considérant qu’une augmentation de 𝛼 revient à améliorer l’inertie expliquée de la matrice spatiale, au détriment d’une perte de l’inertie expliquée sur le plan sémantique » (Gelb et Apparicio 2021, 16).\n\n8.2.1.2 Calcul de la CAH classique\n\n\n\n\n\nRetour sur la classification ascendante hiérarchique (CAH)\n\n\nPour une description détaillée de la CAH, consultez la section suivante (Apparicio et Gelb 2022).\n\n\nLe code ci-dessous permet de construire l’arbre de classification selon le critère de Ward à partir de la matrice sémantique (figure 8.9).\n\n## Variables pour la CAH\nVarsEnv &lt;- c(\"Lden\", \"NO2\", \"PM25\", \"VegHautPrt\")\n## Dataframe sans la géométrie et les quatre variables\nload(\"data/chap08/DonneesLyon.Rdata\")\nData &lt;- st_drop_geometry(LyonIris[VarsEnv])\n## Centrage (moyenne = 0) et réduction des données (variance = 1)\nDataZscore &lt;- data.frame(scale(Data))\n## Matrice sémantique : dissimilarité des observations selon les variables\nMatrice.Semantique &lt;- dist(DataZscore, method = \"euclidean\")\n# Calcul du dendrogramme avec le critère WARD\nArbre &lt;- hclust(Matrice.Semantique, method = \"ward.D\")\nplot(Arbre, hang = -1, label = FALSE,\n     main = \"Dendrogramme \\n(arbre de classification selon le critère de Ward)\",\n     sub = \"\", ylab = \"Hauteur\", xlab = \"\"\n     )\n\n\n\nFigure 8.9: Arbre de classification (dendrogramme)\n\n\n\nÀ la lecture de la figure 8.10, nous ne détectons pas de seuils marqués dans l’inertie expliquée en fonction du nombre de groupes. Par conséquent, nous fixons arbitrairement le nombre de groupes à 5.\n\nlibrary(ggplot2)\n# Fonction pour l'inertie expliquée par les classes\nprop_inert_cutree &lt;- function(K,tree,n){\n  P &lt;- cutree(tree,k=K)\n  W &lt;- sum(tree$height[1:(n-K)])\n  Tot &lt;- sum(tree$height)\n  return(1-W/Tot)\n}\n# Inertie expliquée par des CAH de 2 à 10 classes\ndf.inertie &lt;- data.frame(NGroupes = 2:10,\n                         Inertie = sapply(2:10,\n                                          prop_inert_cutree,\n                                          tree=Arbre,\n                                          n=nrow(DataZscore)))\nggplot(df.inertie)+\n  geom_line(aes(x=NGroupes,y=Inertie))+\n  geom_point(aes(x=NGroupes,y=Inertie), color = \"red\") +\n  labs(y = \"Inertie expliquée\", x = \"Nombre de groupes\")\n\n\n\nFigure 8.10: Méthode du coude reposant sur l’inertie expliquée pour CAH\n\n\n\nNous pouvons visualiser l’arbre avec une coupure à cinq classes.\n\nplot(Arbre, labels = FALSE, \n     main = \"Partition en 2, 5 ou 9 classes\", \n     xlab = \"\", ylab = \"\", sub = \"\", axes = FALSE, hang = -1)\nrect.hclust(Arbre, 5, border = \"red\")\n\n\n\n## Coupure de l'arbre à cinq classes\nLyonIris$CAH5 &lt;- as.character(cutree(Arbre, k=5))\n## Nombre d'observations par classe\ntable(LyonIris$CAH5)\n\n\n  1   2   3   4   5 \n 62 120  88  81 155 \n\n## Valeurs moyennes des classes\naggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ CAH5, data = st_drop_geometry(LyonIris), FUN = mean)\n\n  CAH5     Lden      NO2     PM25 VegHautPrt\n1    1 49.52246 19.11011 13.84968   34.64694\n2    2 54.57170 22.29326 15.17248   12.34458\n3    3 62.53950 37.18071 18.25763   18.25330\n4    4 55.21535 25.85652 15.83278   26.67333\n5    5 55.08405 34.17193 18.90686   13.44716\n\n\nLes résultats de la CAH sont cartographiés à la figure 8.11 tandis que le nombre d’observations et les valeurs moyennes des classes sont reportés au tableau @tbl-dataCAHtab.\n\nlibrary(tmap)\n## Cartographie des résultats\nCarte.CAH5 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n              tm_fill(col=\"CAH5\", palette = \"Set1\", title =\"\")+\n              tm_layout(frame=FALSE, \n                main.title = \"CAH (critère de Ward)\", \n                main.title.position = \"center\", \n                main.title.size = 1)\nCarte.CAH5\n\n\n\nFigure 8.11: CAH avec le critère de Ward avec cinq classes\n\n\n\n\n\n\n\nTableau 8.2: Valeurs moyennes des variables pour les cinq classes obtenues avec la CAH\n\nClasse\nLden\nNO2\nPM25\nVégétation\nNombre d’IRIS\n\n\n\n1\n49,5\n19,1\n13,8\n34,6\n62\n\n\n2\n54,6\n22,3\n15,2\n12,3\n120\n\n\n3\n62,5\n37,2\n18,3\n18,3\n88\n\n\n4\n55,2\n25,9\n15,8\n26,7\n81\n\n\n5\n55,1\n34,2\n18,9\n13,4\n155\n\n\n\n\n\n\n\n8.2.1.3 Calcul de la méthode ClustGeo\n\nCalcul des deux matrices (sémantique et spatiale)\nDans un premier temps, nous créons les matrices sémantique (\\(D_0\\)) et spatiale (\\(D_1\\)). Notez ici que les distances spatiales utilisées correspondent aux distances euclidiennes entre les centroïdes des IRIS.\n\nlibrary(sf)\nlibrary(ClustGeo)\n## Variables\nVarsEnv &lt;- c(\"Lden\", \"NO2\", \"PM25\", \"VegHautPrt\")\n## Dataframe sans la géométrie et les quatre variables\nload(\"data/chap08/DonneesLyon.Rdata\")\nData &lt;- st_drop_geometry(LyonIris[VarsEnv])\n## Centrage (moyenne = 0) et réduction des données (variance = 1)\nDataZscore &lt;- data.frame(scale(Data))\n## Matrice sémantique : dissimilarité des observations selon les variables\nMatrice.Semantique &lt;- dist(DataZscore, method = \"euclidean\")\n## Matrice spatiale entre les unités spatiales\nxy &lt;- st_coordinates(st_centroid(LyonIris))\nMatrice.Spatiale &lt;- dist(xy, method = \"euclidean\")\n\nOptimisation de la valeur de \\(\\alpha\\)\nPour la méthode ClustGeo (avec k = 5), nous évaluons l’impact du paramètre α pour des valeurs de 0 à 1, avec un saut de 0,05. Pour ce faire, nous utilisons la fonction choicealpha du package ClustGeo. À la lecture de la figure 8.12, nous constatons que :\n\nPlus la valeur de \\(\\alpha\\) augmente, plus l’inertie expliquée par la matrice sémantique diminue (trait noir) et inversement, plus l’inertie expliquée par la matrice spatiale est forte.\nAvec \\(\\alpha = \\text{0,30}\\), l’inertie expliquée par la matrice spatiale est de 50 % pour une perte d’inertie expliquée par la matrice sémantique de seulement 7 %. Avec \\(\\alpha = \\text{0,35}\\), nous constatons une chute importante de l’inertie sémantique expliquée. Par conséquent, nous retenons la valeur de 0,30 pour la méthode ClustGeo. Ce choix donne un excellent compromis entre la préservation de l’inertie expliquée des données attributaires et la cohérence spatiale de la classification finale.\n\n\nalphas &lt;- seq(0, 1, 0.05)\nresult &lt;- choicealpha(D0 = Matrice.Semantique, # matrice sémantique\n                      D1 = Matrice.Spatiale,   # matrice spatiale \n                      range.alpha = alphas,    # valeurs de alpha\n                      K = 5,                   # nombre de classes\n                      wt = NULL, scale = TRUE, graph = FALSE)\n# Graphique avec Alpha\ndf.alpha &lt;- data.frame(result$Q)\ndf.alpha$alpha &lt;- alphas\nggplot(df.alpha)+\n  geom_line(aes(x=alphas,y= Q0), color = \"black\")+\n  geom_point(aes(x=alphas,y= Q0), color = \"black\", size=3) +\n  geom_line(aes(x=alphas,y= Q1), color = \"red\")+\n  geom_point(aes(x=alphas,y= Q1), color = \"red\", size=3) +\n  labs(y = \"Pseudo-inertie\", \n       x = \"Paramètre alpha\",\n       subtitle = \"Matrice sémantique (noir) et matrice spatiale (rouge)\")\n\n\n\nFigure 8.12: Impact des deux matrices dans la classification\n\n\n\nRéalisation de la méthode ClustGeo\n\n## Dendrogramme avec ClustGeo\nArbre.ClustGeo &lt;- hclustgeo(D0 = Matrice.Semantique, D1 = Matrice.Spatiale, alpha = 0.30)\n## Coupure de l'arbre à cinq classes\nLyonIris$ClustGeo5 &lt;- as.character(cutree(Arbre.ClustGeo, k=5))\n## Nombre d'observations par classe\ntable(LyonIris$ClustGeo5)\n\n\n  1   2   3   4   5 \n101  73 102 145  85 \n\n## Valeurs moyennes des classes\naggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ ClustGeo5, \n          data = st_drop_geometry(LyonIris), FUN = mean)\n\n  ClustGeo5     Lden      NO2     PM25 VegHautPrt\n1         1 51.99446 22.63948 14.69971   32.70396\n2         2 55.46789 20.84670 15.00392   14.07849\n3         3 61.41468 36.20785 18.23707   18.77059\n4         4 54.98927 34.16558 18.93142   12.84883\n5         5 54.05419 24.32165 15.45733   16.14224\n\n\n\n## Cartographie des résultats\nCarte.ClusteGeo &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n                    tm_fill(col=\"ClustGeo5\", palette = \"Set1\", title =\"\")+\n                    tm_layout(frame=FALSE, \n                      main.title = \"ClustGeo avec alpha = 0,30\", \n                      main.title.position = \"center\", \n                      main.title.size = 1)\nCarte.ClusteGeo\n\n\n\nFigure 8.13: Classification ClustGeo avec alpha = 0,30\n\n\n\n\n\n\n\nTableau 8.3: Valeurs moyennes des variables pour cinq classes obtenues par la méthode ClustGeo (alpha = 0,30)\n\nClasse\nLden\nNO2\nPM25\nVégétation\nNombre d’IRIS\n\n\n\n1\n52,0\n22,6\n14,7\n32,7\n101\n\n\n2\n55,5\n20,8\n15,0\n14,1\n73\n\n\n3\n61,4\n36,2\n18,2\n18,8\n102\n\n\n4\n55,0\n34,2\n18,9\n12,8\n145\n\n\n5\n54,1\n24,3\n15,5\n16,1\n85\n\n\n\n\n\n\nLa comparaison des typologies obtenues avec la CAH et la ClusteGeo démontre clairement que l’introduction d’une matrice spatiale dans la classification ClustGeo génère des classes qui sont moins discontinues spatialement (figure 8.14).\n\ntmap_arrange(Carte.CAH5, Carte.ClusteGeo, nrow = 1, ncol = 2)\n\n\n\nFigure 8.14: Impact des deux matrices dans la classification\n\n\n\n\n8.2.2 Spatial fuzzy c-means\n\nLa méthode SFCM (Spatial fuzzy c-means), proposée par Weiling Cai et ses collègues (2007), est une extension de FCM, soit une version floue de l’algorithme des k-moyennes. Le principe de base est le suivant :\n« Comparativement au FCM classique, le SFCM introduit dans son calcul, en plus du jeu de données original (\\(D_0\\)), une version spatialement décalée (\\(D_s\\)) de ce dernier (Cai, Chen et Zhang 2007). En analyse d’image, cela revient à calculer \\(D_s\\) en appliquant un filtre moyen ou médian à \\(D_0\\) (la médiane étant moins sensible aux valeurs extrêmes locales). Ce processus peut facilement s’appliquer à des entités géographiques vectorielles, en créant une matrice de pondération spatiale \\(W_{kl}\\) (l étant les voisins de k et la diagonale de cette matrice valant 0) (Getis 2009) et en utilisant les poids de cette matrice dans le calcul d’une moyenne ou d’une médiane pondérée locale » (Gelb et Apparicio 2021, 8).\nComme pour la méthode ClustGeo, il est possible de fixer une pondération à la dimension spatiale (\\(D_s\\)) avec un paramètre \\(\\alpha\\), qui varie de 0 à \\(\\infty\\). Une valeur de 0 signale qu’aucun poids n’est accordé à la dimension spatiale, ce qui revient à calculer un FCM classique. Si \\(\\alpha = \\text{2}\\), alors la version spatialement décalée aura deux fois plus de poids dans la classification que le jeu de données original.\n\n\n\n\n\nRetour sur une variable spatialement décalée et la classification k-moyennes\n\n\nLa notion de variable spatialement décalée a été abordée au chapitre 2 (figure 2.29). Pour une description détaillée de la classification k-moyennes, consultez la section suivante (Apparicio et Gelb 2022).\n\n\n\n8.2.2.1 Calcul de la classification c-moyennes floue classique (fuzzy c-means)\nÀ des fins de comparaison avec la CAH, nous proposons de calculer une classification c-moyennes floue classique (fuzzy c-means) avec cinq classes (k = 5). Puis, pour déterminer la valeur optimale de m (soit le degré de logique floue), nous calculons l’inertie expliquée et l’indice de silhouette pour des valeurs de m variant de 1,1 à 3 (avec un incrément de 0,1). Les résultats de ces deux indicateurs, présentés à la figure 8.15, signalent que :\n\nPlus le paramètre m augmente, plus l’inertie expliquée diminue (figure 8.15, a).\nLa valeur de l’indice de silhouette est maximale avec m = 1,8 (figure 8.15, b).\n\n\nlibrary(geocmeans)\nlibrary(ggpubr)\n## Variables pour la CAH\nVarsEnv &lt;- c(\"Lden\", \"NO2\", \"PM25\", \"VegHautPrt\")\n## Dataframe sans la géométrie et les quatre variables\nload(\"data/chap08/DonneesLyon.Rdata\")\nData &lt;- st_drop_geometry(LyonIris[VarsEnv])\n## Centrage (moyenne = 0) et réduction des données (variance = 1)\nDataZscore &lt;- data.frame(scale(Data))\n## Dataframe pour les différents paramètres avec k = 5 et degré de flou\nFCM_selection &lt;- select_parameters(algo = \"FCM\",\n                  data = DataZscore,\n                  k = 5, # nous pourrions ici tester avec k=2:10\n                  m = seq(1.1,3,0.1),\n                  classidx = TRUE, spconsist = FALSE,\n                  tol = 0.001, seed = 456,\n                  verbose = FALSE)\n\n[1] \"number of combinaisons to estimate :  20\"\n\n# Graphique avec l'inertie expliquée\nG1 &lt;- ggplot(FCM_selection) +\n      geom_line(aes(x = m, y = Explained.inertia)) +\n      geom_point(aes(x = m,  y = Explained.inertia), color = \"red\")+\n      labs(title =\"a. Variation des données expliquées\",\n           y = \"Inertie expliquée\", x = \"Paramètre m\")\n# Graphique avec l'indice de silhouette\nG2 &lt;- ggplot(FCM_selection) +\n      geom_line(aes(x = m, y = Silhouette.index)) +\n      geom_point(aes(x = m,  y = Silhouette.index), color = \"red\")+\n      labs(title =\"b. Consistance des groupes\",\n           y = \"Critère de silhouette floue\", x = \"Paramètre m\")\n# Combinaison des deux graphiques dans la figure\nggarrange(G1, G2)\n\n\n\nFigure 8.15: Évaluation de la qualité de la classification FCM avec cinq classes selon le degré de flou (m)\n\n\n\nRéalisons la classification c-moyennes floue et cartographions les probabilités d’appartenance à chacune des classes (figure 8.16), puis l’appartenance à une classe (figure 8.17).\n\n## Classification du c-moyennes \nFCM &lt;- CMeans(DataZscore, k = 5, m = 1.8, tol = 0.0001, verbose = FALSE, seed = 456)\n## Calcul des indicateurs de qualité\ncalcqualityIndexes(DataZscore, FCM$Belongings, 1.5)\n\n$Silhouette.index\n[1] 0.5502472\n\n$Partition.entropy\n[1] 0.9663788\n\n$Partition.coeff\n[1] 0.5143427\n\n$XieBeni.index\n[1] 1.269992\n\n$FukuyamaSugeno.index\n[1] 158.1419\n\n$Explained.inertia\n[1] 0.3757482\n\n## Cartographie des probabilités d'appartenance à chaque classe\nCartes.FCM &lt;- mapClusters(LyonIris,FCM$Belongings)\nnames(Cartes.FCM)\n\n[1] \"ProbaMaps\"   \"ClusterPlot\"\n\ntmap_arrange(Cartes.FCM$ProbaMaps, ncol = 2)\n\n\n\nFigure 8.16: Cartographie des probabilités d’appartenance aux cinq classes avec la classification c-moyennes\n\n\n\n\nCartes.FCM$ClusterPlot\n\n\n\nFigure 8.17: Cartographie des classes issues de la classification c-moyennes\n\n\n\n\n8.2.2.2 Calcul de la classification c-moyennes floue et spatiale (spatial fuzzy c-means)\nDans l’exemple ci-dessous, nous calculons une classification SFCM (spatial fuzzy c-means). Nous avons déterminé avec l’analyse du simple FCM que la valeur de 1,8 pour m semblait satisfaisante avec cinq groupes (k = 5). Nous devons maintenant déterminer la valeur de alpha, soit le poids accordé aux variables spatialement décalées comparativement aux données originales.\nPour cela, nous calculons toutes les valeurs possibles d’alpha entre 0 et 2 avec un intervalle de 0,05. Nous comparons ensuite les valeurs des indices de silhouette (qualité sémantique de la classification) et d’incohérence spatiale des différentes solutions.\n\nlibrary(spdep)\nlibrary(ggplot2)\n# Création d'une matrice de contiguïté standardisée\nNeighbours &lt;- poly2nb(LyonIris, queen = TRUE)\nWMat &lt;- nb2listw(Neighbours, style=\"W\", zero.policy = TRUE)\nSFCM_selection &lt;- select_parameters(algo = \"SFCM\",\n                  data = DataZscore,\n                  k = 5,\n                  m = 1.8,\n                  nblistw = WMat,\n                  alpha = seq(0,2,0.05),\n                  classidx = TRUE,\n                  tol = 0.001, seed = 456,\n                  spconsist  = TRUE,\n                  verbose = FALSE)\n\n[1] \"number of combinaisons to estimate :  41\"\n\ngraph1 &lt;- ggplot(SFCM_selection) + \n  geom_line(aes(x = alpha, y = Silhouette.index), color = 'black') + \n  geom_point(aes(x = alpha, y = Silhouette.index), color = 'red')+\n  labs(x = \"Alpha\", y = \"Indice de silhouette\", )\n\ngraph2 &lt;- ggplot(SFCM_selection) + \n  geom_line(aes(x = alpha, y = spConsistency), color = 'black') + \n  geom_point(aes(x = alpha, y = spConsistency), color = 'red')+\n  labs(x = \"Alpha\", y = \"Indice d'incohérence spatiale\")\n\nggarrange(graph1, graph2, ncol = 2, nrow = 1)\n\n\n\nFigure 8.18: Comparaison de différentes valeurs d’alpha pour le SFCM\n\n\n\nLa figure 8.18 démontre que l’augmentation d’alpha a pour effet de réduire la qualité sémantique de la classification (indice de silhouette), mais aussi de limiter l’inconsistance spatiale des résultats produits. Nous constatons aussi que l’augmentation d’alpha a un effet quasiment linéaire sur la dégradation de l’indice de silhouette et un effet curvilinéaire sur l’inconsistance spatiale. Cette dernière est donc plus impactée par les premières augmentations d’alpha. Nous proposons ici de retenir une solution avec \\(\\alpha\\) = 0,55, car elle correspond à une baisse relativement faible de l’indice de silhouette (0,55 versus 0,50) pour une augmentation importante de la cohérence spatiale des résultats (0,39 versus 0,22).\n\n# Calcul du SFCM\nSFCM &lt;- SFCMeans(DataZscore, WMat, \n                 k = 5, \n                 m = 1.8, \n                 alpha = 0.55,\n                 tol = 0.0001, standardize = FALSE,\n                 verbose = FALSE, seed = 456)\n\nRésultats des indicateurs de qualité du SFCM\n\ncalcqualityIndexes(DataZscore, SFCM$Belongings, 1.5)\n\n$Silhouette.index\n[1] 0.4995905\n\n$Partition.entropy\n[1] 1.039552\n\n$Partition.coeff\n[1] 0.4726401\n\n$XieBeni.index\n[1] 2.011733\n\n$FukuyamaSugeno.index\n[1] 380.0203\n\n$Explained.inertia\n[1] 0.3371823\n\n\nCartographie des probabilités d’appartenance à chaque classe\nLa cartographie des probabilité d’appartenance pour chacune des cinq classes est présentée aux figures 8.19, 8.20, 8.21, 8.22 et 8.22.\n\nCartes.SFCM &lt;- mapClusters(LyonIris, SFCM$Belongings, undecided = 0.45)\nCartes.SFCM$ProbaMaps[[1]]\n\n\n\nFigure 8.19: Cartographie des probabilités d’appartenance issues de la classification SFCM (classe 1)\n\n\n\n\nCartes.SFCM$ProbaMaps[[2]]\n\n\n\nFigure 8.20: Cartographie des probabilités d’appartenance issues de la classification SFCM (classe 2)\n\n\n\n\nCartes.SFCM$ProbaMaps[[3]]\n\n\n\nFigure 8.21: Cartographie des probabilités d’appartenance issues de la classification SFCM (classe 3)\n\n\n\n\nCartes.SFCM$ProbaMaps[[4]]\n\n\n\nFigure 8.22: Cartographie des probabilités d’appartenance issues de la classification SFCM (classe 4)\n\n\n\n\nCartes.SFCM$ProbaMaps[[5]]\n\n\n\nFigure 8.23: Cartographie des probabilités d’appartenance issues de la classification SFCM (classe 5)\n\n\n\nCartographie des probabilités d’appartenance à chaque classe du SFCM (figure 8.24)\n\nCartes.SFCM$ClusterPlot\n\n\n\nFigure 8.24: Cartographie des classes issues de la classification SFCM\n\n\n\nIl est intéressant de noter que les groupes construits par les deux algorithmes (ClustGeo et SFCM) produisent des solutions assez différentes. Nous retrouvons dans les deux classifications un groupe caractérisant les quartiers centraux, puis une distinction assez nette entre les secteurs est et ouest de l’agglomération. Cependant, ClustGeo regroupe les Iris du sud dans un groupe spécifique alors que SFCM a identifié deux groupes plus atypiques spatialement. Le groupe 5 correspond aux Iris situés le long du boulevard périphérique encerclant Lyon.\nLe groupe 5 correspond donc à un ensemble d’Iris avec les plus hauts niveaux de concentration de pollutions atmosphérique et sonore (figure ci-dessous). Il est similaire en cela au groupe 1, qui est cependant marqué par des niveaux de bruit un peu moins élevés.\n\nspiderPlots(Data,SFCM$Belongings,\n            chartcolors = c(\"darkorange3\",\n                            \"grey4\",\n                            \"darkgreen\",\n                            \"royalblue\", \n                            \"blueviolet\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe multiples saveurs du SFCM\n\n\nPlusieurs variantes du SFCM sont proposées par le package geocmeans qui offrent une grande flexibilité :\n\nLa version généralisée du SFCM (appelée SFGCM) avec l’ajout d’un paramètre supplémentaire (beta) qui accélère la convergence du SFCM en limitant la probabilité attribuée au groupe le moins probable de chaque observation.\nLes versions robustes du SFCM et SFGCM, qui normalisent les distances calculées entre les observations pour réaliser des classifications non sphériques.\nL’ajout d’un groupe de résidus, permettant d’attribuer les observations très incertaines à un groupe « poubelle », ce qui évite d’influencer la cohérence des « vrais groupes » avec des observations très atypiques."
  },
  {
    "objectID": "08-ClassificationsSpatiales.html#sec-083",
    "href": "08-ClassificationsSpatiales.html#sec-083",
    "title": "8  Méthodes de classification non supervisée spatiale",
    "section": "\n8.3 Quiz de révision du chapitre",
    "text": "8.3 Quiz de révision du chapitre\nsource(\"code_complementaire/QuizzFunctions.R\")\nChap08Quiz &lt;- quizz(\"quiz/Chap08.yml\", \"Chap08\")\nrender_quizz(Chap08Quiz)\n\n\n\n\n\nQuelles sont les méthodes de classification non supervisée avec une contrainte spatiale?\n\n\nRelisez l’introduction du chapitre 8.\n\n\n\n\n\n\nAlgorithmes AZP (Automatic Zoning Problem).\n\n\n\n\n\n\n\nAlgorithmes SKATER.\n\n\n\n\n\n\n\nClassification floue c-moyennes spatiale.\n\n\n\n\n\n\n\nAlgorithmes REDCAP.\n\n\n\n\n\n\n\nMéthode ClustGeo.\n\n\n\n\n\n\n\n\n\n\nQuelles sont les méthodes de classification non supervisée avec une dimensions spatiale?\n\n\nRelisez l’introduction de la section 8.\n\n\n\n\n\n\nAlgorithmes AZP (Automatic Zoning Problem).\n\n\n\n\n\n\n\nAlgorithmes SKATER.\n\n\n\n\n\n\n\nClassification floue c-moyenne spatiale.\n\n\n\n\n\n\n\nAlgorithmes REDCAP.\n\n\n\n\n\n\n\nMéthode ClustGeo.\n\n\n\n\n\n\n\n\n\n\nLa classification ascendante hiérarchique et la classification le k-moyennes sont des méthodes de classification non supervisée qui ne prennent pas en compte l’espace :\n\n\nRelisez au besoin la section 8.\n\n\n\n\n\n\nVrai\n\n\n\n\n\n\n\nFaux\n\n\n\n\n\n\n\n\n\n\nLa méthode de classification avec contrainte spatiale produit des régions non discontinues (sans mitage spatial) :\n\n\nRelisez au besoin la section 8.1.\n\n\n\n\n\n\nVrai\n\n\n\n\n\n\n\nFaux\n\n\n\n\n\n\n\n\n\n\nL’algorithme SKATER est basé sur :\n\n\nRelisez au besoin la section 8.1.2.\n\n\n\n\n\n\nUne matrice de distances.\n\n\n\n\n\n\n\nUn arbre couvrant de poids minimal.\n\n\n\n\n\n\n\nLe I de Moran.\n\n\n\n\n\n\n\n\n\n\nLa méthode ClustGeo est basée sur deux matrices :\n\n\nRelisez au besoin la section 8.2.1.\n\n\n\n\n\n\nUne matrice sémantique calculée sur p variables caractérisant les unités spatiales (D0).\n\n\n\n\n\n\n\nUne matrice spatiale calculée à partir des distances euclidiennes entre les unités spatiales (D1).\n\n\n\n\n\n\n\nUne matrice spatialement décalée de la matrice sémantique original (Ds).\n\n\n\n\n\n\n\n\n\n\nLa méthode c-moyennes spatiale floue est :\n\n\nRelisez au besoin la section 8.1.2.\n\n\n\n\n\n\nUne matrice sémantique calculée sur p variables caractérisant les unités spatiales (D0).\n\n\n\n\n\n\n\nUne matrice spatiale calculée à partir des distances euclidiennes entre les unités spatiales (D1).\n\n\n\n\n\n\n\nUne matrice spatialement décalée de la matrice sémantique original (Ds).\n\n\n\n\n\n\n\n\n\n\nLe paramètre alpha de la méthode ClustGeo permet de définir le poids accordé à la matrice spatiale. Son intervalle de variation est de :\n\n\nRelisez le deuxième encadré à la section 8.2.1.1.\n\n\n\n\n\n\n0 à 1.\n\n\n\n\n\n\n\n0 à l’infini.\n\n\n\n\n\n\n\n\n\n\nLe paramètre alpha de la classification c-moyennes spatiale floue permet de définir le poids accordé à la matrice spatialement décalée. Son intervalle de variation est de :\n\n\nRelisez le deuxième encadré à la section 8.2.1.1.\n\n\n\n\n\n\n0 à 1.\n\n\n\n\n\n\n\n0 à l’infini.\n\n\n\n\n\n\n\n\n\nVérifier votre résultat"
  },
  {
    "objectID": "08-ClassificationsSpatiales.html#sec-084",
    "href": "08-ClassificationsSpatiales.html#sec-084",
    "title": "8  Méthodes de classification non supervisée spatiale",
    "section": "\n8.4 Exercices de révision",
    "text": "8.4 Exercices de révision\n\n\n\n\n\nExercice 1. Réalisation de classification avec contrainte spatiale avec des variables socioéconomiques\n\n\n\nlibrary(rgeoda)\nlibrary(sf)\nlibrary(tmap)\n## Préparation des données\nload(\"data/chap08/DonneesLyon.Rdata\")\nVarSocioEco &lt;- c(\"Pct0_14\", \"Pct_65\", \"Pct_Img\", \"Pct_brevet\", \"NivVieMed\")\nData2 &lt;- st_drop_geometry(LyonIris[VarSocioEco])\nqueen_w &lt;- queen_weights(LyonIris)\n## Classification avec k = 4\nazp5_sa  &lt;- à compléter\nazp5_tab &lt;- à compléter\nskater5  &lt;- rgeoda::skater(à compléter)\nredcap5  &lt;- à compléter\n## Cartographie des résultats\nLyonIris$SE.azp4_sa  &lt;- à compléter\nLyonIris$SE.azp4_tab &lt;- à compléter\nLyonIris$SE.skater4 &lt;- à compléter\nLyonIris$SE.recap4  &lt;- à compléter\nCarte1 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n  tm_fill(col=\"SE.azp4_sa\", palette = \"Set1\", title =\"\")+\n  tm_layout(frame=FALSE,  main.title = \"a. AZP-SA\",\n            main.title.position = \"center\", main.title.size = 1)\nCarte2 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n  tm_fill(col=\"SE.azp4_tab\", palette = \"Set1\", title =\"\")+\n  tm_layout(frame=FALSE,  main.title = \"b. AZP-TABU\",\n            main.title.position = \"center\", main.title.size = 1)\nCarte3 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n  tm_fill(col=\"SE.skater4\", palette = \"Set1\", title =\"\")+\n  tm_layout(frame=FALSE,  main.title = \"c. Skater\",\n            main.title.position = \"center\", main.title.size = 1)\nCarte4 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n  tm_fill(col=\"SE.recap4\", palette = \"Set1\", title =\"\")+\n  tm_layout(frame=FALSE,  main.title = \"d. RECAP\",\n            main.title.position = \"center\", main.title.size = 1)\n\ntmap_arrange(à compléter)\n\nCorrection à la section 9.8.1.\n\n\n\n\n\n\nApparicio, Philippe et Jérémy Gelb. 2022. Méthodes quantitatives en sciences sociales : un grand bol d’R. FabriqueREL, Licence CC BY-SA. https://laeq.github.io/LivreMethoQuantBolR/.\n\n\nAssunção, Renato M, Marcos Corrêa Neves, Gilberto Câmara et Corina da Costa Freitas. 2006. « Efficient regionalization techniques for socio-economic geographical units using minimum spanning trees. » International Journal of Geographical Information Science 20 (7): 797‑811. https://doi.org/10.1080/13658810600665111.\n\n\nCai, Weiling, Songcan Chen et Daoqiang Zhang. 2007. « Fast and robust fuzzy c-means clustering algorithms incorporating local information for image segmentation. » Pattern recognition 40 (3): 825‑838. https://doi.org/10.1016/j.patcog.2006.07.011.\n\n\nChavent, Marie, Vanessa Kuentz-Simonet, Amaury Labenne et Jérôme Saracco. 2018. « ClustGeo: an R package for hierarchical clustering with spatial constraints. » Computational Statistics 33 (4): 1799‑1822. https://doi.org/10.1007/s00180-018-0791-1.\n\n\nDuque, Juan C, Luc Anselin et Sergio J Rey. 2012. « The max-p-regions problem. » Journal of Regional Science 52 (3): 397‑419. https://doi.org/10.1111/j.1467-9787.2011.00743.x.\n\n\nGelb, Jérémy et Philippe Apparicio. 2021. « Apport de la classification floue c-means spatiale en géographie: essai de taxinomie socio-résidentielle et environnementale à Lyon. » Cybergeo: European Journal of Geography. https://doi.org/10.4000/cybergeo.36414.\n\n\nGetis, Arthur. 2009. « Spatial weights matrices. » Geographical Analysis 41 (4): 404‑410. https://doi.org/10.1111/j.1538-4632.2009.00768.x.\n\n\nGuo, Diansheng. 2008. « Regionalization with dynamically constrained agglomerative clustering and partitioning (REDCAP). » International Journal of Geographical Information Science 22 (7): 801‑823. https://doi.org/10.1080/13658810600665111.\n\n\nJain, Anil K et Richard C Dubes. 1988. Algorithms for clustering data. Prentice-Hall, Inc.\n\n\nKaufman, Leonard. 1990. « Partitioning around medoids (program pam). » Finding groups in data 344: 68‑125. https://doi.org/10.1002/9780470316801.ch2.\n\n\nLebart, Ludovic, Alain Morineau et Marie Piron. 1995. Statistique exploratoire multidimensionnelle. Dunod.\n\n\nLi, Xun et Luc Anselin. 2023. rgeoda: R Library for Spatial Data Analysis. s.n. https://CRAN.R-project.org/package=rgeoda.\n\n\nOpenshaw, Stan. 1977. « A geographical solution to scale and aggregation problems in region-building, partitioning and spatial modelling. » Transactions of the institute of british geographers: 459‑472. https://doi.org/10.2307/622300.\n\n\nOpenshaw, Stan et Liang Rao. 1995. « Algorithms for reengineering 1991 Census geography. » Environment and planning A 27 (3): 425‑446. https://doi.org/10.1068/a270425."
  },
  {
    "objectID": "10-Exercices.html#sec-1001",
    "href": "10-Exercices.html#sec-1001",
    "title": "9  Correction des exercices",
    "section": "\n9.1 Exercices du chapitre 1",
    "text": "9.1 Exercices du chapitre 1\n\n9.1.1 Exercice 1\n\nlibrary(sf)\n## Importation des deux couches\nArrond &lt;- st_read(\"data/chap01/shp/Arrondissements.shp\", quiet = TRUE)\nRues &lt;- st_read(\"data/chap01/shp/Segments_de_rue.shp\", quiet = TRUE)\n## Création d'un objet sf pour l'arrondissement des Nations : requête attributive\ntable(Arrondissements$NOM)\nArrond.DesNations &lt;- subset(Arrondissements, \n                            NOM == \"Arrondissement des Nations\")\n## Découper les rues avec le polygone de l'arrondissement des nations\nRues.DesNations &lt;- st_intersection(Rues, Arrond.DesNations)\n\n\n9.1.2 Exercice 2\n\nlibrary(sf)\nlibrary(tmap)\n## Importation des deux couches\nAD.RMRSherb &lt;- st_read(dsn = \"data/chap01/gpkg/Recen2021Sherbrooke.gpkg\", \n                       layer = \"SherbAD\", quiet = T)\nHotelVille &lt;- data.frame(ID = 1, Nom = \"Hotel de Ville\",\n                         lon = -71.89306, lat = 45.40417)\nHotelVille &lt;- st_as_sf(HotelVille, coords = c(\"lon\",\"lat\"), crs = 4326)\n## Changement de projection avant de s'assurer que les deux couches aient la même\nHotelVille &lt;- st_transform(HotelVille, st_crs(AD.RMRSherb))\n## Ajout d'un champ pour la distance en km à l'hôtel de Ville pour les secteurs de recensement\nAD.RMRSherb$DistHVKM &lt;- as.numeric(st_distance(AD.RMRSherb,HotelVille)) / 1000\n## Cartographie en quatre classes selon les quantiles\ntmap_mode(\"plot\")\ntm_shape(AD.RMRSherb)+\n  tm_fill(col= \"DistHVKM\", \n          palette = \"Reds\",\n          n=4,\n          style = \"quantile\",\n          title =\"Distance à l'hôtel de Ville (km)\")+\n  tm_borders(col=\"black\")\n\n\n9.1.3 Exercice 3\n\nlibrary(sf)\n## Importation de la couche des divisions de recensement du Québec\nDR.Qc &lt;- st_read(dsn = \"data/chap01/gpkg/Recen2021Sherbrooke.gpkg\", \n                 layer = \"DivisionsRecens2021\", quiet = T)\n## Importation du fichier csv des division de recensement\nDR.Data &lt;- read.csv(\"data/chap01/tables/DRQC2021.csv\")\n## Jointure attributive avec le champ IDUGD\nDR.Qc &lt;- merge(DR.Qc, DR.Data, by=\"IDUGD\")\n## Il y a déja deux champs dans la table pour calculer la densité de population :\n## SUPTERRE : superficie en km2\n## DRpop_2021 : population en 2021\nDR.Qc$HabKm2 &lt;- DR.Qc$DRpop_2021 / DR.Qc$SUPTERRE\nhead(DR.Qc, n=2)\nsummary(DR.Qc$HabKm2)\n\n\n9.1.4 Exercice 4\n\nlibrary(sf)\n## Importation du réseau de rues\nRues &lt;- st_read(\"data/chap01/shp/Segments_de_rue.shp\", quiet=T)\nunique(Rues$TYPESEGMEN)\n## Sélection des tronçons autoroutiers\nAutoroutes &lt;- subset(Rues, TYPESEGMEN == \"Autoroute\")\n## Création d'une couche sf pour le point avec les coordonnées\n## en degrés (WGS84, EPSG : 4326) : -71.91688, 45.37579\nPoint1_sf &lt;- data.frame(ID = 1,\n                         lon = -71.91688, lat = 45.37579)\nPoint1_sf &lt;- st_as_sf(Point1_sf, coords = c(\"lon\",\"lat\"), crs = 4326)\n## Changement de projection avant de s'assurer que les deux couches aient la même\nPoint1_sf &lt;- st_transform(Point1_sf, st_crs(Autoroutes))\n## Trouver le tronçon autoroutier le plus proche\nPlusProche &lt;- st_nearest_feature(Point1_sf, Autoroutes)\nprint(PlusProche)\nPoint1_sf$AutoroutePlusProche &lt;- as.numeric(st_distance(Point1_sf,\n                                                        Autoroutes[PlusProche,]))\ncat(\"Distance à l'autoroute la plus proche :\", Point1_sf$AutoroutePlusProche, \"m.\")\n## Zone tampon \nZoneTampon &lt;- st_buffer(Point1_sf, Point1_sf$AutoroutePlusProche)\n## Cartographie\ntmap_mode(\"view\")\ntm_shape(ZoneTampon)+\n  tm_borders(col= \"black\")+\ntm_shape(Autoroutes)+\n  tm_lines(col=\"red\")+\ntm_shape(Point1_sf)+\n  tm_dots(col= \"blue\", shape=21, size = .2)"
  },
  {
    "objectID": "10-Exercices.html#sec-1002",
    "href": "10-Exercices.html#sec-1002",
    "title": "9  Correction des exercices",
    "section": "\n9.2 Exercices du chapitre 2",
    "text": "9.2 Exercices du chapitre 2\n\n9.2.1 Exercice 1\n\n\nFigure 9.1: Exercice sur la contiguïté et les ordres d’adjacence\n\n\n9.2.2 Exercice 2\n\nlibrary(sf)\nlibrary(spdep)\nlibrary(tmap)\n## Importation de la couche des secteurs de recensement\nSRQc &lt;- st_read(dsn = \"data/chap02/exercice/RMRQuebecSR2021.shp\", quiet=TRUE)\n\n## Matrice selon le partage d'un segment (Rook)\nRook &lt;- poly2nb(SRQc, queen=FALSE)\nW.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = \"W\")\n\n## Coordonnées des centroïdes des entités spatiales\ncoords &lt;- st_coordinates(st_centroid(SRQc))\n\n## Matrices de l'inverse de la distance\n# Trouver le plus proche voisin\nk1 &lt;- knn2nb(knearneigh(coords))\nplusprochevoisin.max &lt;- max(unlist(nbdists(k1,coords)))\n# Voisins les plus proches avec le seuil de distance maximal\nVoisins.DistMax &lt;- dnearneigh(coords, 0, plusprochevoisin.max)\n# Distances avec le seuil maximum\ndistances &lt;- nbdists(Voisins.DistMax, coords)\n# Inverse de la distance au carré\nInvDistances2 &lt;- lapply(distances, function(x) (1/x^2))\n## Matrices de pondérations spatiales standardisées en ligne\nW_InvDistances2Reduite &lt;- nb2listw(Voisins.DistMax, glist = InvDistances2, style = \"W\")\n\n## Matrice des plus proches voisins avec k = 2\nk2 &lt;- knn2nb(knearneigh(coords, k = 2))\nW.k2 &lt;-  nb2listw(k2, zero.policy=FALSE, style = \"W\")\n\n\n9.2.3 Exercice 3\n\nlibrary(sf)\nlibrary(spdep)\nlibrary(tmap)\n## Cartographie de la variable\ntm_shape(SRQc)+\n  tm_polygons(col=\"D1pct\", title = \"Premier décile de revenu (%)\",\n              style=\"quantile\", n=5, palette=\"Greens\")+\n  tm_layout(frame = F)+tm_scale_bar(c(0,5,10))\n\n## I de Moran avec la méthode Monte-Carlo avec 999 permutations\n# utilisez la fonction moran.mc\n# avec la matrice W.Rook\nmoran.mc(SRQc$D1pct, listw=W.Rook, zero.policy=TRUE,  nsim=999)\n# avec la matrice W_InvDistances2\nmoran.mc(SRQc$D1pct, listw=W_InvDistances2Reduite, zero.policy=TRUE,  nsim=999)\n# avec la matrice W.k2\nmoran.mc(SRQc$D1pct, listw=W.k2, zero.policy=TRUE,  nsim=999)\n\nLes valeurs du I de Moran sont les suivantes : 0,69 pour la matrice Rook, 0,52 pour la matrice inverse de la distance au carré réduite et 0,75 pour la matrice selon le critère des deux plus proches voisins.\n\n9.2.4 Exercice 4\n\n####################\n## Calcul du Z(Gi)\n####################\nSRQc$D1pct_localGetis &lt;- localG(SRQc$D1pct, \n                                W.Rook, \n                                zero.policy=TRUE)\n# Définition des intervalles et des noms des classes\nclasses.intervalles = c(-Inf, -3.29, -2.58, -1.96, 1.96, 2.58, 3.29, Inf)\nclasses.noms = c(\"Point froid (p = 0,001)\", \n                \"Point froid (p = 0,01)\", \n                \"Point froid (p = 0,05)\", \n                \"Non significatif\",\n                \"Point chaud (p = 0,05)\", \n                \"Point chaud (p = 0,01)\", \n                \"Point chaud (p = 0,001)\")\n\n## Création d'un champ avec les noms des classes\nSRQc$D1pct_localGetisP &lt;- cut(SRQc$D1pct_localGetis,\n                              breaks = classes.intervalles,\n                              labels = classes.noms)\n## Cartographie\ntm_shape(SRQc)+\n  tm_polygons(col =\"D1pct_localGetisP\", \n              title=\"Z(Gi)\", palette=\"-RdBu\", lwd = 1)+\n  tm_layout(frame =F)\n\n####################\n## Typologie LISA\n####################\n## Cote Z (variable centrée réduite)\nzx &lt;- (SRQc$D1pct - mean(SRQc$D1pct))/sd(SRQc$D1pct)\n## variable X centrée réduite spatialement décalée avec une matrice Rook\nwzx &lt;- lag.listw(W.Rook, zx)\n## I de Moran local (notez que vous pouvez aussi utiliser la fonction localmoran_perm)\nlocalMoranI  &lt;- localmoran(SRQc$D1pct, W.Rook)\nplocalMoranI &lt;- localMoranI[, 5]\n## Choisir un seuil de signification\nsignif = 0.05\n## Construction de la typologie\nTypologie &lt;- ifelse(zx &gt; 0 & wzx &gt; 0, \"1. HH\", NA)\nTypologie &lt;- ifelse(zx &lt; 0 & wzx &lt; 0, \"2. LL\", Typologie)\nTypologie &lt;- ifelse(zx &gt; 0 & wzx &lt; 0, \"3. HL\", Typologie)\nTypologie &lt;- ifelse(zx &lt; 0 & wzx &gt; 0, \"4. LH\", Typologie)\nTypologie &lt;- ifelse(plocalMoranI &gt; signif, \"Non sign\", Typologie)  # Non significatif\n## Enregistrement de la typologie dans un champ\nSRQc$TypoIMoran.D1pct &lt;- Typologie\n## Couleurs\nCouleurs &lt;- c(\"red\", \"blue\", \"lightpink\", \"skyblue2\", \"lightgray\")\nnames(Couleurs) &lt;- c(\"1. HH\",\"2. LL\",\"3. HL\",\"4. LH\",\"Non sign\")\n## Cartographie\ntmap_mode(\"plot\")\ntm_shape(SRQc) + \n  tm_polygons(col = \"TypoIMoran.D1pct\", palette = Couleurs, \n              title =\"Autocorrélation spatiale locale\")+\n  tm_layout(frame = FALSE)"
  },
  {
    "objectID": "10-Exercices.html#sec-1003",
    "href": "10-Exercices.html#sec-1003",
    "title": "9  Correction des exercices",
    "section": "\n9.3 Exercices du chapitre 3",
    "text": "9.3 Exercices du chapitre 3\n\n9.3.1 Exercice 1\n\nlibrary(sf)\nlibrary(tmap)\n## Importation des données \nArrondissements &lt;-  st_read(dsn = \"data/chap03/Arrondissements.shp\", quiet=TRUE)\nIncidents &lt;- st_read(dsn = \"data/chap03/IncidentsSecuritePublique.shp\", quiet=TRUE)\n## Changement de projection\nArrondissements &lt;- st_transform(Arrondissements, crs = 3798) \nIncidents &lt;- st_transform(Incidents, crs = 3798)\n## Couche pour les accidents\nAccidents &lt;- subset(Incidents, Incidents$DESCRIPTIO %in% \n                      c(\"Accident avec blessés\", \"Accident mortel\"))\n## Coordonnées et projection cartographique\nxy &lt;- st_coordinates(Accidents)\nProjCarto &lt;- st_crs(Accidents)\n## Centre moyen\nCentreMoyen &lt;- data.frame(X = mean(xy[,1]),\n                          Y = mean(xy[,2]))\nCentreMoyen &lt;- st_as_sf(CentreMoyen, coords = c(\"X\", \"Y\"), crs = ProjCarto)\n# Distance standard combiné\nCentreMoyen$DS &lt;- c(sqrt(mean((xy[,1] - mean(xy[,1]))**2 + \n                              (xy[,2] - mean(xy[,2]))**2)))\nCercleDS &lt;- st_buffer(CentreMoyen, dist = CentreMoyen$DS)\nhead(CercleDS)\n\n\n9.3.2 Exercice 2\n\nlibrary(sf)\nlibrary(tmap)\n## Importation des données \nSR &lt;- st_read(dsn = \"data/chap03/Recen2021Sherbrooke.gpkg\",\n              layer = \"DR_SherbSRDonnees2021\", quiet=TRUE)\n## Couche pour les accidents pour l'année 2021\nAcc2021 &lt;- subset(Incidents, Incidents$DESCRIPTIO %in% \n                    c(\"Accident avec blessés\", \"Accident mortel\")\n                  & ANNEE==2021)\n## Nous nous assurons que les deux couches aient la même projection cartographique\nSR &lt;- st_transform(SR, st_crs(Acc2021))\n## Calcul du nombre d'incidents par SR \nSR$Acc2021 &lt;- lengths(st_intersects(SR, Acc2021))\n## Calcul du nombre de méfaits pour 1000 habitants\nSR$DensiteMAcc2021Hab &lt;- SR$Acc2021 / (SR$SRpop_2021 / 1000)\n## Cartographie\ntm_shape(SR)+\n  tm_polygons(col=\"Acc2021\", style=\"pretty\", \n              title=\"Nombre pour 1000 habitants\",\n              border.col = \"black\", lwd = 1)+\n  tm_bubbles(size = \"DensiteMAcc2021Hab\", border.col = \"black\", alpha = .5,\n             col = \"aquamarine3\", title.size = \"Nombre\", scale = 1.5)+ \n  tm_layout(frame = FALSE)+tm_scale_bar(text.size = .5, c(0, 5, 10))\n\n\n9.3.3 Exercice 3\n\nlibrary(sf)\nlibrary(spatstat)\nlibrary(tmap)\nlibrary(terra)\n\n## Importation des données \nArrondissements &lt;-  st_read(dsn = \"data/chap03/Arrondissements.shp\", quiet=TRUE)\nIncidents &lt;- st_read(dsn = \"data/chap03/IncidentsSecuritePublique.shp\", quiet=TRUE)\n## Changement de projection\nArrondissements &lt;- st_transform(Arrondissements, crs = 3798) \nIncidents &lt;- st_transform(Incidents, crs = 3798)\n## Couche pour les méfaits pour l'année 2021\nM2021 &lt;- subset(Incidents, DESCRIPTIO == \"Méfait\" & ANNEE==2021)\n## Pour accélérer les calculs, nous retenons uniquement l'arrondissement des Nations\n# Couche pour l'arrondissement des Nations\nArrDesNations &lt;- subset(Arrondissements, NOM == \"Arrondissement des Nations\")\n# Sélection des accidents localisés dans l'arrondissement Des Nations\nRequeteSpatiale &lt;- st_intersects(M2021, ArrDesNations, sparse = FALSE)\nM2021$Nations &lt;- RequeteSpatiale[, 1]\nM2021Nations &lt;- subset(M2021, M2021$Nations == TRUE)\n\n## Conversion des données sf dans le format de spatstat\n# la fonction as.owin est utilisée pour définir la fenêtre de travail\nfenetre &lt;- as.owin(ArrDesNations)\n## Conversion des points au format ppp pour les différentes années\nM2021.ppp &lt;- ppp(x = st_coordinates(M2021Nations)[,1],\n                 y = st_coordinates(M2021Nations)[,2],\n                 window = fenetre, check = T)\n\n## Kernel quadratique avec un rayon de 500 mètres et une taille de pixel de 50 mètres\nkdeQ &lt;- density.ppp(M2021.ppp, sigma=500, eps=50, kernel=\"quartic\")\n## Conversion en raster\nRkdeQ &lt;- terra::rast(kdeQ)*1000000\n## Projection cartographique\ncrs(RkdeQ) &lt;- \"epsg:3857\"\n## Visualisation des résultats\ntmap_mode(\"plot\")\n  tm_shape(RkdeQ) + tm_raster(style = \"cont\", palette=\"Reds\", title = \"Gaussien\")+\n  tm_shape(M2021Nations) + tm_dots(col = \"black\", size = 0.01)+\n  tm_shape(ArrDesNations) + tm_borders(col = \"black\", lwd = 3)+\n  tm_layout(frame = F)"
  },
  {
    "objectID": "10-Exercices.html#sec-1004",
    "href": "10-Exercices.html#sec-1004",
    "title": "9  Correction des exercices",
    "section": "\n9.4 Exercices du chapitre 4",
    "text": "9.4 Exercices du chapitre 4\n\n9.4.1 Exercice 1\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(dbscan)\nlibrary(ggplot2)\n## Importation des données\nCollissions &lt;- st_read(dsn = \"data/chap04/collisions.gpkg\", \n                       layer = \"CollisionsRoutieres\", \n                       quiet = T)\n## Collisions impliquant au moins une personne à vélo en 2020 et 2021\nColl.Velo &lt;- subset(Collissions,\n                    Collissions$NB_VICTIMES_VELO &gt; 0 &\n                      Collissions$AN %in% c(2020, 2021))\n## Coordonnées géographiques\nxy &lt;- st_coordinates(Coll.Velo)\n## Graphique pour la distance au quatrième voisin le plus proche\nDistKplusproche &lt;- kNNdist(xy, k = 4)\nDistKplusproche &lt;- as.data.frame(sort(DistKplusproche, decreasing = FALSE))\nnames(DistKplusproche) &lt;- \"distance\"\nggplot(data = DistKplusproche)+\n  geom_path(aes(x = 1:nrow(DistKplusproche), y = distance), size=1)+\n  labs(x = \"Points triés par ordre croissant selon la distance\",\n       y = \"Distance au quatrième point le plus proche\")+\n  geom_hline(yintercept=250, color = \"#08306b\", linetype=\"dashed\", size=1)+\n  geom_hline(yintercept=500, color = \"#00441b\", linetype=\"dashed\", size=1)+\n  geom_hline(yintercept=1000, color = \"#67000d\", linetype=\"dashed\", size=1)\n## DBSCAN avec les quatre distances\nset.seed(123456789)\ndbscan250  &lt;- dbscan(xy, eps = 250, minPts = 4)\ndbscan500  &lt;- dbscan(xy, eps = 500, minPts = 4)\ndbscan1000 &lt;- dbscan(xy, eps = 1000, minPts = 4)\n## Affichage des résultats\ndbscan250\ndbscan500\ndbscan1000\n## Enregistrement dans la couche de points sf Coll.Velo\nColl.Velo$dbscan250 &lt;- as.character(dbscan250$cluster)\nColl.Velo$dbscan500 &lt;- as.character(dbscan500$cluster)\nColl.Velo$dbscan1000 &lt;- as.character(dbscan1000$cluster)\n\nColl.Velo$dbscan250 &lt;- ifelse(nchar(Coll.Velo$dbscan250) == 1,\n                              paste0(\"0\", Coll.Velo$dbscan250),\n                              Coll.Velo$dbscan250)\nColl.Velo$dbscan500 &lt;- ifelse(nchar(Coll.Velo$dbscan500) == 1,\n                               paste0(\"0\", Coll.Velo$dbscan500),\n                               Coll.Velo$dbscan500)\nColl.Velo$dbscan1000 &lt;- ifelse(nchar(Coll.Velo$dbscan1000) == 1,\n                               paste0(\"0\", Coll.Velo$dbscan1000),\n                               Coll.Velo$dbscan1000)\n## Extraction des agrégats\nAgregats.dbscan250  &lt;- subset(Coll.Velo, dbscan250 != \"00\")\nAgregats.dbscan500  &lt;- subset(Coll.Velo, dbscan500 != \"00\")\nAgregats.dbscan1000 &lt;- subset(Coll.Velo, dbscan1000 != \"00\")\n## Cartographie des résultats\ntmap_mode(\"view\")\ntm_shape(Agregats.dbscan250)+tm_dots(col=\"dbscan250\", size = .05)\ntm_shape(Agregats.dbscan500)+tm_dots(col=\"dbscan500\", size = .05)\ntm_shape(Agregats.dbscan1000)+tm_dots(col=\"dbscan1000\", size = .05)\n\n\n9.4.2 Exercice 2\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(dbscan)\nlibrary(ggplot2)\n## Importation des données\nCollissions &lt;- st_read(dsn = \"data/chap04/collisions.gpkg\", layer = \"CollisionsRoutieres\")\n## Collisions impliquant au moins une personne à vélo en 2020 et 2021\nColl.Velo &lt;- subset(Collissions,\n                    Collissions$NB_VICTIMES_VELO &gt; 0 &\n                      Collissions$AN %in% c(2020, 2021))\n## Coordonnées géographiques\nxy &lt;- st_coordinates(Coll.Velo)\nColl.Velo$x &lt;- xy[,1]\nColl.Velo$y &lt;- xy[,2]\n## Conversion du champ DT_ACCDN au format Date\nColl.Velo$DT_ACCDN &lt;- as.Date(Coll.Velo$DT_ACCDN)\n## ST-DBSCAN avec eps1 = 500, esp2 = 30 et minpts = 4\nResultats.stdbscan &lt;- stdbscan(x = Coll.Velo$x,\n                               y = Coll.Velo$y,\n                               time = Coll.Velo$DT_ACCDN,\n                               eps1 = 500,\n                               eps2 = 30,\n                               minpts = 4)\n## Enregistrement des résultats ST-DBSCAN dans la couche de points sf\nColl.Velo$stdbscan &lt;- as.character(Resultats.stdbscan$cluster)\nColl.Velo$stdbscan &lt;- ifelse(nchar(Coll.Velo$stdbscan) == 1,\n                             paste0(\"0\", Coll.Velo$stdbscan),\n                             Coll.Velo$stdbscan)\n## Nombre de points par agrégat avec la fonction table\ntable(Coll.Velo$stdbscan)\n## Sélection des points appartenant à un agrégat avec la fonction subset\nAgregats &lt;- subset(Coll.Velo, stdbscan != \"00\")\n## Conversion de la date au format POSIXct\nAgregats$dtPOSIXct &lt;- as.POSIXct(Agregats$DT_ACCDN, format = \"%Y/%m/%d\")\n## Tableau récapitulatif\nlibrary(\"dplyr\")  \nTableau.stdbscan &lt;-\n  st_drop_geometry(Agregats) %&gt;%\n  group_by(stdbscan) %&gt;% \n  summarize(points = n(),\n            date.min = min(DT_ACCDN),\n            date.max = max(DT_ACCDN),\n            intervalle.jours = as.numeric(max(DT_ACCDN)-min(DT_ACCDN)))\n## Affichage du tableau\nprint(Tableau.stdbscan, n = nrow(Tableau.stdbscan))\n## Construction du graphique\nggplot(Agregats) + \n  geom_point(aes(x = dtPOSIXct, \n                 y = stdbscan, \n                 color = stdbscan),\n             show.legend = FALSE) +\n  scale_x_datetime(date_labels = \"%Y/%m\")+\n  labs(x= \"Temps\",\n       y= \"Identifiant de l'agrégat\",\n       title = \"ST-DBSCAN avec Esp1 = 1000, Esp2 = 21 et MinPts = 4\")\n## Création d'une couche pour les agrégats\nstdbcan.Agregats &lt;- subset(Coll.Velo, stdbscan != \"00\")\n## Cartographie\ntmap_mode(\"view\")\n  tm_shape(stdbcan.Agregats)+\n    tm_dots(shape = 21, col=\"stdbscan\", size=.025, title = \"Agrégat\")"
  },
  {
    "objectID": "10-Exercices.html#sec-1005",
    "href": "10-Exercices.html#sec-1005",
    "title": "9  Correction des exercices",
    "section": "\n9.5 Exercices du chapitre 5",
    "text": "9.5 Exercices du chapitre 5\n\n9.5.1 Exercice 1\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(r5r)\n\nsetwd(\"data/chap05/Laval\")\nrJava::.jinit()\noptions(java.parameters = \"-Xmx2G\")\n\n# 1. Construction du réseau\ndossierdata &lt;- paste0(getwd(),\"/_DataReseau\")\nlist.files(dossierdata)\nr5r_core &lt;- setup_r5(data_path = dossierdata,\n                     elevation = \"TOBLER\",\n                     verbose = FALSE, overwrite = FALSE)\n\n# 2. Création de deux points\nPts &lt;- data.frame(id = c(\"Station Morency\", \"Adresse 1\"),\n                  lon = c(-73.7199, -73.7183),\n                  lat = c(45.5585,  45.5861))\nPts &lt;- st_as_sf(Pts, coords = c(\"lon\",\"lat\"), crs = 4326)\nStationMorency &lt;- Pts[1,]\nAdresse1 &lt;- Pts[2,]\n\n## 2.1. Trajets en automobile\nAuto.1 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                               origins = Adresse1,\n                               destinations = StationMorency,\n                               mode = \"CAR\",\n                               shortest_path = FALSE,\n                               drop_geometry = FALSE)\nAuto.2 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                               origins = StationMorency,\n                               destinations = Adresse1,\n                               mode = \"CAR\",\n                               shortest_path = FALSE,\n                               drop_geometry = FALSE)\n## 2.2. Trajets en vélo\nvelo.1 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                               origins = StationMorency,\n                               destinations = Adresse1,\n                               mode = \"BICYCLE\",\n                               bike_speed = 12,  # par défaut 12\n                               shortest_path = FALSE,\n                               drop_geometry = FALSE)\nvelo.2 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                               origins = Adresse1,\n                               destinations = StationMorency,\n                               mode = \"BICYCLE\",\n                               bike_speed = 12,  # par défaut 12\n                               shortest_path = FALSE,\n                               drop_geometry = FALSE)\n## 2.3. Trajets à pied\nmarche.1 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                                 origins = StationMorency,\n                                 destinations = Adresse1,\n                                 mode = \"WALK\",\n                                 walk_speed = 4.5,  # par défaut 3.6\n                                 shortest_path = FALSE,\n                                 drop_geometry = FALSE)\nmarche.2 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                                 origins = Adresse1,\n                                 destinations = StationMorency,\n                                 mode = \"WALK\",\n                                 walk_speed = 4.5,  # par défaut 12\n                                 shortest_path = FALSE,\n                                 drop_geometry = FALSE)\n\n## 2.4. Trajets en transport en commun\ndateheure.matin &lt;- as.POSIXct(\"12-02-2024 08:00:00\",\n                              format = \"%d-%m-%Y %H:%M:%S\")\ndateheure.soir  &lt;- as.POSIXct(\"12-02-2024 18:00:00\",\n                              format = \"%d-%m-%Y %H:%M:%S\")\n### Définir le temps de marche maximal\nminutes_marches_max &lt;- 20\nTC.1 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                                 origins = Adresse1,\n                                 destinations = StationMorency,\n                                 mode = c(\"WALK\", \"TRANSIT\"),\n                                 max_walk_time = minutes_marches_max,\n                                 walk_speed = 4.5,\n                                 departure_datetime = dateheure.matin,\n                                 shortest_path = FALSE,\n                                 drop_geometry = FALSE)\nTC.2 &lt;- detailed_itineraries(r5r_core = r5r_core,\n                                  origins = StationMorency,\n                                  destinations = Adresse1,\n                                  mode = c(\"WALK\", \"TRANSIT\"),\n                                  max_walk_time = minutes_marches_max,\n                                  walk_speed = 4.5,\n                                  departure_datetime = dateheure.soir,\n                                  shortest_path = FALSE,\n                                  drop_geometry = FALSE)\n\n\n# 4. Cartographie\n  # - Map1.Aller : Marche (de la résidence à la station de métro)\n  # - Map2.Aller : Vélo (de la résidence à la station de métro)\n  # - Map3.Aller : Auto (de la résidence à la station de métro)\n  # - Map4.Aller : Transport en commun (de la résidence à la station de métro)\ntmap_mode(view)\nMap1.Aller &lt;- tm_shape(marche.1)+tm_lines(col=\"mode\", lwd = 3,\n                                      popup.vars = c(\"mode\", \"from_id\", \"to_id\",\n                                                     \"segment_duration\", \"distance\",\n                                                     \"total_duration\", \"total_distance\"))+\n              tm_shape(Adresse1)+tm_dots(col=\"green\", size = .15)+\n              tm_shape(StationMorency)+tm_dots(col=\"red\", size = .15)\n\nMap2.Aller &lt;- tm_shape(velo.1)+tm_lines(col=\"mode\", lwd = 3,\n                                          popup.vars = c(\"mode\", \"from_id\", \"to_id\",\n                                                         \"segment_duration\", \"distance\",\n                                                         \"total_duration\", \"total_distance\"))+\n              tm_shape(Adresse1)+tm_dots(col=\"green\", size = .15)+\n              tm_shape(StationMorency)+tm_dots(col=\"red\", size = .15)\n\nMap3.Aller &lt;- tm_shape(Auto.1)+tm_lines(col=\"mode\", lwd = 3,\n                                        popup.vars = c(\"mode\", \"from_id\", \"to_id\",\n                                                       \"segment_duration\", \"distance\",\n                                                       \"total_duration\", \"total_distance\"))+\n              tm_shape(Adresse1)+tm_dots(col=\"green\", size = .15)+\n              tm_shape(StationMorency)+tm_dots(col=\"red\", size = .15)\n\nMap4.Aller &lt;- tm_shape(TC.1)+tm_lines(col=\"mode\", lwd = 3,\n                                        popup.vars = c(\"mode\", \"from_id\", \"to_id\",\n                                                       \"segment_duration\", \"distance\",\n                                                       \"total_duration\", \"total_distance\"))+\n              tm_shape(Adresse1)+tm_dots(col=\"green\", size = .15)+\n              tm_shape(StationMorency)+tm_dots(col=\"red\", size = .15)\n\ntmap_arrange(Map1.Aller, Map2.Aller, Map3.Aller, Map4.Aller, ncol = 2, nrow = 2)\n\n## Réaliser une figure avec quatre figures pour les trajets retour :\n  # - Map1.Retour : Marche (de la station de métro à la résidence)\n  # - Map2.Retour : Vélo (de la station de métro à la résidence)\n  # - Map3.Retour : Auto (de la station de métro à la résidence)\n  # - Map4.Retour : Transport en commun (de la station de métro à la résidence)\n\nMap1.Retour &lt;- tm_shape(marche.2)+tm_lines(col=\"mode\", lwd = 3,\n                                          popup.vars = c(\"mode\", \"from_id\", \"to_id\",\n                                                         \"segment_duration\", \"distance\",\n                                                         \"total_duration\", \"total_distance\"))+\n  tm_shape(Adresse1)+tm_dots(col=\"red\", size = .15)+\n  tm_shape(StationMorency)+tm_dots(col=\"green\", size = .15)\n\nMap2.Retour &lt;- tm_shape(velo.2)+tm_lines(col=\"mode\", lwd = 3,\n                                        popup.vars = c(\"mode\", \"from_id\", \"to_id\",\n                                                       \"segment_duration\", \"distance\",\n                                                       \"total_duration\", \"total_distance\"))+\n  tm_shape(Adresse1)+tm_dots(col=\"red\", size = .15)+\n  tm_shape(StationMorency)+tm_dots(col=\"green\", size = .15)\n\nMap3.Retour &lt;- tm_shape(Auto.2)+tm_lines(col=\"mode\", lwd = 3,\n                                        popup.vars = c(\"mode\", \"from_id\", \"to_id\",\n                                                       \"segment_duration\", \"distance\",\n                                                       \"total_duration\", \"total_distance\"))+\n  tm_shape(Adresse1)+tm_dots(col=\"red\", size = .15)+\n  tm_shape(StationMorency)+tm_dots(col=\"green\", size = .15)\n\nMap4.Retour &lt;- tm_shape(TC.2)+tm_lines(col=\"mode\", lwd = 3,\n                                      popup.vars = c(\"mode\", \"from_id\", \"to_id\",\n                                                     \"segment_duration\", \"distance\",\n                                                     \"total_duration\", \"total_distance\"))+\n  tm_shape(Adresse1)+tm_dots(col=\"red\", size = .15)+\n  tm_shape(StationMorency)+tm_dots(col=\"green\", size = .15)\n\ntmap_arrange(Map1.Retour, Map2.Retour, Map3.Retour, Map4.Retour, ncol = 2, nrow = 2)\n\n# 5. Arrêt de java\nr5r::stop_r5(r5r_core)\nrJava::.jgc(R.gc = TRUE)\n\n\n9.5.2 Exercice 2\n\n## Construction du réseau\nsetwd(\"data/chap05/Laval\")\nrJava::.jinit()\noptions(java.parameters = \"-Xmx2G\")\ndossierdata &lt;- paste0(getwd(),\"/_DataReseau\")\nlist.files(dossierdata)\nr5r_core &lt;- setup_r5(data_path = dossierdata,\n                     elevation = \"TOBLER\",\n                     verbose = FALSE, overwrite = FALSE)\n\n## Point pour la Station Morency\nStationMorency &lt;- data.frame(id = \"Station Morency\",\n                             lon = -73.7199,\n                             lat = 45.5585,  45.5861)\nStationMorency &lt;- st_as_sf(StationMorency, \n                           coords = c(\"lon\",\"lat\"), crs = 4326)\n\n# 1. Calcul d'isochrones à pied de 5, 10 et 15 minutes\nIso.Marche &lt;- isochrone(r5r_core = r5r_core,\n                        origins = StationMorency,\n                        mode = \"WALK\",\n                        cutoffs = c(5, 10, 15),\n                        sample_size = .8,\n                        time_window = 120,\n                        progress = FALSE)\n# 1.2. Isochrone à vélo de 5, 10 et 15 minutes\nIso.Velo &lt;- isochrone(r5r_core = r5r_core,\n                      origins = StationMorency,\n                      mode = \"BICYCLE\",\n                      cutoffs = c(10, 20, 30),\n                      sample_size = .8,\n                      time_window = 120,\n                      progress = FALSE)\n\n# 3. Cartographie les résultats\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\nCarte.Marche &lt;- tm_shape(Iso.Marche)+\n                    tm_fill(col=\"isochrone\", \n                            alpha = .4, \n                            breaks = c(0, 5, 10, 15),\n                            title =\"Marche\",\n                            legend.format = list(text.separator = \"à\"))+\n                    tm_shape(StationMorency)+tm_dots(col=\"darkred\", size = .25)\n\nCarte.Velo &lt;- tm_shape(Iso.Velo)+\n                    tm_fill(col=\"isochrone\", \n                            alpha = .4, \n                            breaks = c(0, 5, 10, 15),\n                            title =\"Vélo\",\n                            legend.format = list(text.separator = \"à\"))+\n                    tm_shape(StationMorency)+tm_dots(col=\"darkred\", size = .25)\n\n\ntmap_arrange(Carte.Marche, Carte.Velo, ncol = 2)\n\n# 4. Arrêt de java\nr5r::stop_r5(r5r_core)\nrJava::.jgc(R.gc = TRUE)"
  },
  {
    "objectID": "10-Exercices.html#sec-1006",
    "href": "10-Exercices.html#sec-1006",
    "title": "9  Correction des exercices",
    "section": "\n9.6 Exercices du chapitre 6",
    "text": "9.6 Exercices du chapitre 6\n\n9.6.1 Exercice 1\n\nlibrary(sf)\nlibrary(spNetwork)\nlibrary(future)\n\nfuture::plan(future::multisession(workers = 5))\n# Importation des données sur les collisions cycles et le réseau de rues\nCollisions &lt;- st_read(dsn = \"data/chap06/Mtl/DonneesMTL.gpkg\", layer=\"CollisionsAvecCyclistes\", quiet=TRUE)\nReseauRues &lt;- st_read(dsn = \"data/chap06/Mtl/DonneesMTL.gpkg\", layer=\"Rues\", quiet=TRUE)\nReseauRues$LineID &lt;- 1:nrow(ReseauRues)\nLongueurKm &lt;- sum(as.numeric(st_length(ReseauRues)))/1000\nCollisions &lt;- st_transform(Collisions, st_crs(ReseauRues))\ncat(\"Informations sur les couches\",\n    \"\\n  Collisions avec cylistes :\", nrow(Collisions),\n    \"\\n  Réseau :\", round(LongueurKm,3), \"km\")\n# Cartographie\ntmap_mode(\"view\")\ntm_shape(ReseauRues) + tm_lines(\"black\") +\n  tm_shape(Collisions) + tm_dots(\"blue\", size = 0.025)+\ntm_scale_bar(c(0,1,2), position = 'left')+\n  tm_layout(frame = FALSE)\n\n\n## Évaluation des bandwidths de 100 à 1200 avec un saut de 50\neval_bandwidth &lt;- bw_cv_likelihood_calc.mc(\n  bw_range = c(100,1200),\n  bw_step = 50,\n  lines = ReseauRues, \n  events = Collisions,\n  w = rep(1, nrow(Collisions)),\n  kernel_name = 'quartic',\n  method = 'discontinuous',\n  adaptive = FALSE,\n  max_depth = 10,\n  digits = 1,\n  tol = 0.1,\n  agg = 5,\n  grid_shape = c(5,5),\n  verbose = TRUE)\n\n## Graphique pour les bandwidths \nggplot(eval_bandwidth) + \n  geom_path(aes(x = bw, y = cv_scores)) + \n  geom_point(aes(x = bw, y = cv_scores), color = 'red')+\n  labs(x = \"Valeur de la bandwidth\", y = \"Valeur du CV\")\n\n\n9.6.2 Exercice 2\n\nlibrary(sf)\nlibrary(spNetwork)\nlibrary(future)\n## Création des lixels d'une longueur de 100 mètres\nlixels &lt;- lixelize_lines(ReseauRues, 100, mindist = 50)\nlixels_centers &lt;- spNetwork::lines_center(lixels)\n## Calcul de la NKDE continue\nintensity &lt;- nkde.mc(lines = ReseauRues,\n                     events = Collisions,\n                     w = rep(1, nrow(Collisions)),\n                     samples = lixels_centers,\n                     kernel_name = 'quartic',\n                     bw = 500,\n                     adaptive = FALSE,\n                     method = 'continuous',\n                     max_depth = 8,\n                     digits = 1,\n                     tol = 0.1,\n                     agg = 5,\n                     verbose = FALSE,\n                     grid_shape = c(5,5))\nlixels$density &lt;- intensity * 1000\n## Cartographie\ntm_shape(lixels) + \n  tm_lines(\"density\", lwd = 1.5, n = 7, style = \"fisher\",\n           legend.format = list(text.separator = \"à\"))+\n  tm_layout(frame=FALSE)"
  },
  {
    "objectID": "10-Exercices.html#sec-1007",
    "href": "10-Exercices.html#sec-1007",
    "title": "9  Correction des exercices",
    "section": "\n9.7 Exercices du chapitre 7",
    "text": "9.7 Exercices du chapitre 7\n\n9.7.1 Exercice 1\n\nlibrary(sf)\nlibrary(spatialreg)\n# Matrice de contiguïté selon le partage d'un segment (Rook)\nload(\"data/chap06/DonneesLyon.Rdata\")\nRook &lt;- poly2nb(LyonIris, queen=FALSE)\nRook &lt;- poly2nb(LyonIris, queen=FALSE)\nW.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = \"W\")\n# Modèles\nformule &lt;- \"PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed\"\nModele.SLX &lt;- lmSLX(formule, listw=W.Rook,  data = LyonIris)  # dataframe\nModele.SAR &lt;- lagsarlm(formule, listw=W.Rook,  data = LyonIris,  type = 'lag')\nModele.SEM &lt;- errorsarlm(formule, listw=W.Rook, data = LyonIris)\nModele.DurbinSpatial &lt;- lagsarlm(formule, listw = W.Rook, data = LyonIris, type = \"mixed\")\nModele.DurbinErreur &lt;- errorsarlm(formule, listw=W.Rook, data = LyonIris, etype = 'emixed')\n# Résultats des modèles\nsummary(Modele.SLX)\nsummary(Modele.SAR)\nsummary(Modele.SEM)\nsummary(Modele.DurbinSpatial)\nsummary(Modele.DurbinErreur)\n\n\n9.7.2 Exercice 2\n\nlibrary(sf)\nlibrary(mgcv)\nload(\"data/chap06/DonneesLyon.Rdata\")\n# Ajout des coordonnées x et y\nxy &lt;- st_coordinates(st_centroid(LyonIris))\nLyonIris$X &lt;- xy[,1]\nLyonIris$Y &lt;- xy[,2]\n# Construction du modèle avec\nformule &lt;- \"PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed\"\nModele.GAM2 &lt;- gam(PM25 ~  Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed+\n                           s(X, Y, k= 40),\n                          data = LyonIris)\nsummary(Modele.GAM2)\n\n\n9.7.3 Exercice 3\n\nlibrary(sf)\nlibrary(spgwr)\nload(\"data/chap06/DonneesLyon.Rdata\")\n# Ajout des coordonnées x et y\nxy &lt;- st_coordinates(st_centroid(LyonIris))\nLyonIris$X &lt;- xy[,1]\nLyonIris$Y &lt;- xy[,2]\n# Optimisation du nombre de voisins avec le CV\nformule &lt;- \"PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed\"\nbwaCV.voisins  &lt;- gwr.sel(formule,\n                          data = LyonIris,\n                          method = \"cv\",\n                          gweight=gwr.bisquare,\n                          adapt=TRUE,\n                          verbose = FALSE,\n                          RMSE = TRUE,\n                          longlat = FALSE,\n                          coords=cbind(LyonIris$X,LyonIris$Y))\n# Optimisation du nombre de voisins avec l'AIC\nformule &lt;- \"PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed\"\nbwaCV.voisins  &lt;- gwr.sel(formule,\n                          data = LyonIris,\n                          method = \"AIC\",\n                          gweight=gwr.bisquare,\n                          adapt=TRUE,\n                          verbose = FALSE,\n                          RMSE = TRUE,\n                          longlat = FALSE,\n                          coords=cbind(LyonIris$X,LyonIris$Y))\n# Réalisation de la GWR\nModele.GWR &lt;- gwr(formule,\n              data = LyonIris,\n              adapt=bwaCV.voisins,\n              gweight=gwr.bisquare,\n              hatmatrix=TRUE,\n              se.fit=TRUE,\n              coords=cbind(LyonIris$X,LyonIris$Y),\n              longlat=F)\n# Affichage des résultats\nModele.GWR"
  },
  {
    "objectID": "10-Exercices.html#sec-1008",
    "href": "10-Exercices.html#sec-1008",
    "title": "9  Correction des exercices",
    "section": "\n9.8 Exercices du chapitre 8",
    "text": "9.8 Exercices du chapitre 8\n\n9.8.1 Exercice 1\n\nlibrary(rgeoda)\nlibrary(sf)\nlibrary(tmap)\n## Préparation des données\nload(\"data/chap06/DonneesLyon.Rdata\")\nVarSocioEco &lt;- c(\"Pct0_14\", \"Pct_65\", \"Pct_Img\", \"Pct_brevet\", \"NivVieMed\")\nData2 &lt;- st_drop_geometry(LyonIris[VarSocioEco])\nqueen_w &lt;- queen_weights(LyonIris)\n## Classification avec k = 4\nazp5_sa  &lt;- azp_sa(p=4, w=queen_w, df=Data2, cooling_rate = 0.85)\nazp5_tab &lt;- azp_tabu(p=4, w=queen_w, df=Data2, tabu_length = 10, conv_tabu = 10)\nskater5  &lt;- rgeoda::skater(k=4, w=queen_w, df=Data2)\nredcap5  &lt;- redcap(k = 4, w = queen_w, df = Data2, method = \"fullorder-wardlinkage\")\n## Cartographie des résultats\nLyonIris$SE.azp4_sa  &lt;- as.character(azp5_tab$Clusters)\nLyonIris$SE.azp4_tab &lt;- as.character(azp5_sa$Clusters)\nLyonIris$SE.skater4 &lt;- as.character(skater5$Clusters)\nLyonIris$SE.recap4  &lt;- as.character(redcap5$Clusters)\nCarte1 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n  tm_fill(col=\"SE.azp4_sa\", palette = \"Set1\", title =\"\")+\n  tm_layout(frame=FALSE,  main.title = \"a. AZP-SA\",\n            main.title.position = \"center\", main.title.size = 1)\nCarte2 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n  tm_fill(col=\"SE.azp4_tab\", palette = \"Set1\", title =\"\")+\n  tm_layout(frame=FALSE,  main.title = \"b. AZP-TABU\",\n            main.title.position = \"center\", main.title.size = 1)\nCarte3 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n  tm_fill(col=\"SE.skater4\", palette = \"Set1\", title =\"\")+\n  tm_layout(frame=FALSE,  main.title = \"c. Skater\",\n            main.title.position = \"center\", main.title.size = 1)\nCarte4 &lt;- tm_shape(LyonIris)+tm_borders(col=\"gray\", lwd=.5)+\n  tm_fill(col=\"SE.recap4\", palette = \"Set1\", title =\"\")+\n  tm_layout(frame=FALSE,  main.title = \"d. RECAP\",\n            main.title.position = \"center\", main.title.size = 1)\n\ntmap_arrange(Carte1, Carte2, Carte3, Carte4)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliographie",
    "section": "",
    "text": "Abramson, Ian S. 1982. « On bandwidth variation in kernel\nestimates-a square root law. » The annals of Statistics:\n1217‑1223. https://www.jstor.org/stable/2240724.\n\n\nAnselin, Luc. 1995. « Local indicators of spatial\nassociation—LISA. » Geographical analysis 27 (2):\n93‑115. https://doi.org/10.1111/j.1538-4632.1995.tb00338.x.\n\n\n———. 1996. « The Moran scatterplot as an ESDA tool to Assess Local\nInstability in Spatial Association. » In Spatial Analytical\nPerspectives on GIS, sous la dir. de Manfred M Fischer, 121‑137.\nTaylor & Francis Group. https://doi.org/10.1201/9780203739051.\n\n\n———. 2019. « A local indicator of multivariate spatial\nassociation: extending Geary’s C. » Geographical\nAnalysis 51 (2): 133‑150. https://doi.org/10.1111/gean.12164.\n\n\nAnselin, Luc, Anil K Bera, Raymond Florax et Mann J Yoon. 1996.\n« Simple diagnostic tests for spatial dependence. »\nRegional science and urban economics 26 (1): 77‑104. https://doi.org/10.1016/0166-0462(95)02111-6.\n\n\nAnselin, Luc et Xun Li. 2019. « Operational local join count\nstatistics for cluster detection. » Journal of geographical\nsystems 21: 189‑210. https://doi.org/10.1007/s10109-019-00299-x.\n\n\nAnselin, Luc et Sergio J Rey. 2014. Modern spatial econometrics in\npractice: A guide to GeoDa, GeoDaSpace and PySAL. GeoDa Press LLC.\n\n\nApparicio, Philippe, Marie-Soleil Cloutier et Richard Shearmur. 2007.\n« The case of Montreal’s missing food deserts: evaluation of\naccessibility to food supermarkets. » International journal\nof health geographics 6 (1): 1‑13. https://doi.org/10.1186/1476-072X-6-4.\n\n\nApparicio, Philippe et Jérémy Gelb. 2022. Méthodes quantitatives en\nsciences sociales : un grand bol d’R. FabriqueREL, Licence CC\nBY-SA. https://laeq.github.io/LivreMethoQuantBolR/.\n\n\nApparicio, Philippe, Jérémy Gelb, Anne-Sophie Dubé, Simon Kingham, Lise\nGauvin et Éric Robitaille. 2017. « The approaches to measuring the\npotential spatial access to urban health services revisited: distance\ntypes and aggregation-error issues. » International journal\nof health geographics 16 (1): 32. https://doi.org/10.1186/s12942-017-0105-9.\n\n\nApparicio, Philippe et Anne-Marie Séguin. 2006. « Measuring the\naccessibility of services and facilities for residents of public housing\nin Montreal. » Urban studies 43 (1): 187‑211. http://dx.doi.org/10.1080/00420980500409334.\n\n\nApparicio, Philippe, Anne-Marie Séguin et Xavier Leloup. 2007.\n« Modélisation spatiale de la pauvreté à Montréal: apport\nméthodologique de la régression géographiquement pondérée. »\nThe Canadian Geographer/Le Géographe canadien 51 (4): 412‑427.\nhttps://doi.org/10.1111/j.1541-0064.2007.00189.x.\n\n\nAssunção, Renato M, Marcos Corrêa Neves, Gilberto Câmara et Corina da\nCosta Freitas. 2006. « Efficient regionalization techniques for\nsocio-economic geographical units using minimum spanning trees. »\nInternational Journal of Geographical Information Science 20\n(7): 797‑811. https://doi.org/10.1080/13658810600665111.\n\n\nBaddeley, Adrian, Ege Rubak et Rolf Turner. 2015. Spatial point\npatterns: methodology and applications with R. CRC press.\n\n\nBaddeley, Adrian et Rolf Turner. 2005. « spatstat: An\nR Package for Analyzing Spatial Point Patterns. »\nJournal of Statistical Software 12 (6): 1‑42. https://doi.org/10.18637/jss.v012.i06.\n\n\nBarbonne, Rémy, Paul Villeneuve et Marius Thériault. 2007. « La\ndynamique spatiale des marchés locaux de l’emploi au sein\ndu champ métropolitain de Québec,\n1981–2001. » The Canadian Geographer/Le\nGéographe canadien 51 (3). Wiley Online Library:\n303‑322. https://doi.org/10.1111/j.1541-0064.2007.00180.x.\n\n\nBirant, Derya et Alp Kut. 2007. « ST-DBSCAN: An algorithm for\nclustering spatial–temporal data. » Data & knowledge\nengineering 60 (1): 208‑221. https://doi.org/10.1016/j.datak.2006.01.013.\n\n\nBivand, Roger, Giovanni Millo et Gianfranco Piras. 2021. « A\nreview of software for spatial econometrics in R. »\nMathematics 9 (11): 1276. https://doi.org/10.3390/math9111276.\n\n\nBivand, Roger, Edzer J Pebesma, Virgilio Gómez-Rubio et Edzer Jan\nPebesma. 2008. Applied spatial data analysis with R. Vol.\n747248717. Springer.\n\n\nBivand, Roger, Edzer Pebesma et Virgilio Gomez-Rubio. 2013. Applied\nspatial data analysis with R, Second edition.\nSpringer, New York. https://asdar-book.org/.\n\n\nBivand, Roger et David WS Wong. 2018. « Comparing implementations\nof global and local indicators of spatial association. »\nTest 27 (3): 716‑748. https://doi.org/10.1007/s11749-018-0599-x.\n\n\nBivand, Roger et Danlin Yu. 2023. spgwr: Geographically Weighted\nRegression. s.n. https://CRAN.R-project.org/package=spgwr.\n\n\nBrewer, Cynthia A, Geoffrey W Hatchard et Mark A Harrower. 2003.\n« ColorBrewer in print: a catalog of color schemes for\nmaps. » Cartography and geographic information science\n30 (1): 5‑32. https://doi.org/10.1559/152304003100010929.\n\n\nCaha, Jan. 2023. SpatialKDE: Kernel Density Estimation for Spatial\nData. s.n. https://CRAN.R-project.org/package=SpatialKDE.\n\n\nCai, Weiling, Songcan Chen et Daoqiang Zhang. 2007. « Fast and\nrobust fuzzy c-means clustering algorithms incorporating local\ninformation for image segmentation. » Pattern\nrecognition 40 (3): 825‑838. https://doi.org/10.1016/j.patcog.2006.07.011.\n\n\nCampello, Ricardo JGB, Davoud Moulavi et Jörg Sander. 2013.\n« Density-based clustering based on hierarchical density\nestimates. » In Advances in Knowledge Discovery and Data\nMining: 17th Pacific-Asia Conference, PAKDD 2013, Gold Coast, Australia,\nApril 14-17, 2013, Proceedings, Part II 17, 160‑172. s.n. http://dx.doi.org/10.1007/978-3-642-37456-2_14.\n\n\nChavent, Marie, Vanessa Kuentz-Simonet, Amaury Labenne et Jérôme\nSaracco. 2018. « ClustGeo: an R package for hierarchical\nclustering with spatial constraints. » Computational\nStatistics 33 (4): 1799‑1822. https://doi.org/10.1007/s00180-018-0791-1.\n\n\nCliff, Andrew David et J Keith Ord. 1981. Spatial processes: models\n& applications. Taylor & Francis.\n\n\nDavies, T. M., J. C. Marshall et M. L. Hazelton. 2018. « Tutorial\non kernel estimation of continuous spatial and spatiotemporal relative\nrisk. » Statistics in Medicine 37 (7): 1191‑1221.\n\n\nDiggle, Peter. 1985. « A kernel method for smoothing point process\ndata. » Journal of the Royal Statistical Society: Series C\n(Applied Statistics) 34 (2). Wiley Online Library: 138‑147.\n\n\nDijkstra, Edsger Wybe. 1959. « A note on two problems in connexion\nwith graphs:(Numerische Mathematik, 1 (1959), p 269-271). »\nStichting Mathematisch Centrum.\n\n\nDouglas, David H et Thomas K Peucker. 1973. « Algorithms for the\nreduction of the number of points required to represent a digitized line\nor its caricature. » Cartographica: the international journal\nfor geographic information and geovisualization 10 (2): 112‑122. https://doi.org/10.3138/FM57-6770-U75U-7727.\n\n\nDubé, Jean et Diègo Legros. 2014. Econométrie spatiale\nappliquée des microdonnées. ISTE Group.\n\n\nDuque, Juan C, Luc Anselin et Sergio J Rey. 2012. « The\nmax-p-regions problem. » Journal of Regional Science 52\n(3): 397‑419. https://doi.org/10.1111/j.1467-9787.2011.00743.x.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander et Xiaowei Xu. 1996.\n« A density-based algorithm for discovering clusters in large\nspatial databases with noise. » In Proceedings of the Second\nInternational Conference on Knowledge Discovery and Data Mining\n(KDD-96), 96:226‑231. 34. s.n. https://dl.acm.org/doi/10.5555/3001460.3001507.\n\n\nFotheringham, A Stewart, Chris Brunsdon et Martin Charlton. 2003.\nGeographically weighted regression: the analysis of spatially\nvarying relationships. John Wiley & Sons.\n\n\nFotheringham, A Stewart, Ricardo Crespo et Jing Yao. 2015.\n« Geographical and temporal weighted regression (GTWR). »\nGeographical Analysis 47 (4). Wiley Online Library: 431‑452. https://doi.org/10.1111/gean.12071.\n\n\nFotheringham, A Stewart, Wenbai Yang et Wei Kang. 2017.\n« Multiscale geographically weighted regression (MGWR). »\nAnnals of the American Association of Geographers 107 (6).\nTaylor & Francis: 1247‑1265. https://doi.org/10.1080/24694452.2017.1352480.\n\n\nGarnier, Simon, Noam Ross, Robert Rudis, Antônio Pedro Camargo, Marco\nSciaini et Cédric Scherer. 2021. viridis -\nColorblind-Friendly Color Maps for R. s.n. https://sjmgarnier.github.io/viridis/.\n\n\nGeary, Robert C. 1954. « The contiguity ratio and statistical\nmapping. » The incorporated statistician 5 (3): 115‑146.\nhttps://doi.org/10.2307/2986645.\n\n\nGelb, Jeremy. 2021. « spNetwork: A Package for Network Kernel\nDensity Estimation. » The R Journal 13 (2):\n561‑577. https://doi.org/10.32614/RJ-2021-102.\n\n\nGelb, Jérémy et Philippe Apparicio. 2021. « Apport de la\nclassification floue c-means spatiale en géographie: essai de taxinomie\nsocio-résidentielle et environnementale à Lyon. » Cybergeo:\nEuropean Journal of Geography. https://doi.org/10.4000/cybergeo.36414.\n\n\nGetis, Arthur. 2009. « Spatial weights matrices. »\nGeographical Analysis 41 (4): 404‑410. https://doi.org/10.1111/j.1538-4632.2009.00768.x.\n\n\nGetis, Arthur et J Keith Ord. 1992. « The analysis of spatial\nassociation by use of distance statistics. » Geographical\nanalysis 24 (3): 189‑206. https://doi.org/10.1111/j.1538-4632.1992.tb00261.x.\n\n\nGilardi, Andrea et Robin Lovelace. 2022. osmextract: Download and\nImport Open Street Map Data Extracts. s.n. https://CRAN.R-project.org/package=osmextract.\n\n\nGiraud, Timothée et Nicolas Lambert. 2016. « cartography: Create\nand Integrate Maps in your R Workflow. » JOSS 1 (4). The\nOpen Journal. https://doi.org/10.21105/joss.00054.\n\n\nGuagliardo, Mark F. 2004. « Spatial accessibility of primary care:\nconcepts, methods and challenges. » International journal of\nhealth geographics 3 (1): 1‑13. https://doi.org/10.1186/1476-072x-3-3.\n\n\nGuo, Diansheng. 2008. « Regionalization with dynamically\nconstrained agglomerative clustering and partitioning (REDCAP). »\nInternational Journal of Geographical Information Science 22\n(7): 801‑823. https://doi.org/10.1080/13658810600665111.\n\n\nHahsler, Michael et Matthew Piekenbrock. 2022. dbscan: Density-Based\nSpatial Clustering of Applications with Noise (DBSCAN) and Related\nAlgorithms. s.n. https://CRAN.R-project.org/package=dbscan.\n\n\nHahsler, Michael, Matthew Piekenbrock et Derek Doran. 2019.\n« dbscan: Fast Density-Based Clustering with\nR. » Journal of Statistical Software 91\n(1): 1‑30. https://doi.org/10.18637/jss.v091.i01.\n\n\nHarris, Paul, Chris Brunsdon et Martin Charlton. 2011.\n« Geographically weighted principal components analysis. »\nInternational Journal of Geographical Information Science 25\n(10). Taylor & Francis: 1717‑1736. https://doi.org/10.1080/13658816.2011.554838.\n\n\nHarrower, Mark et Cynthia A Brewer. 2003. « ColorBrewer.org: an\nonline tool for selecting colour schemes for maps. » The\nCartographic Journal 40 (1): 27‑37. http://dx.doi.org/10.1179/000870403235002042.\n\n\nHewko, Jared, Karen E Smoyer-Tomic et M John Hodgson. 2002.\n« Measuring neighbourhood spatial accessibility to urban\namenities: does aggregation error matter? » Environment and\nPlanning A 34 (7): 1185‑1206. https://doi.org/10.1016/0038-0121(92)90004-O.\n\n\nHijmans, Robert J. 2022a. raster: Geographic Data Analysis and\nModeling. R package version 3.5-15. https://CRAN.R-project.org/package=raster.\n\n\n———. 2022b. terra: Spatial Data Analysis. s.n. https://CRAN.R-project.org/package=terra.\n\n\nHollister, Jeffrey, Tarak Shah, Alec L Robitaille, Marcus W Beck et Mike\nJohnson. 2023. elevatr: Access Elevation Data from Various\nAPIs. s.n. https://github.com/jhollist/elevatr/.\n\n\nJain, Anil K et Richard C Dubes. 1988. Algorithms for clustering\ndata. Prentice-Hall, Inc.\n\n\nJepson, Victoria, Philippe Apparicio et Thi-Thanh-Hien Pham. 2022.\n« Environmental equity and access to parks in Greater Montreal: an\nanalysis of spatial proximity and potential congestion issues. »\nJournal of Urbanism: International Research on Placemaking and Urban\nSustainability: 1‑19. http://dx.doi.org/10.1080/17549175.2022.2150271.\n\n\nJombart, T, S Devillard, Anne-Béatrice Dufour et D Pontier. 2008.\n« Revealing cryptic spatial patterns in genetic variability by a\nnew multivariate method. » Heredity 101 (1): 92‑103. https://doi.org/10.1038/hdy.2008.34.\n\n\nKaufman, Leonard. 1990. « Partitioning around medoids (program\npam). » Finding groups in data 344: 68‑125. https://doi.org/10.1002/9780470316801.ch2.\n\n\nKhan, Abdullah A. 1992. « An integrated approach to measuring\npotential spatial access to health care services. »\nSocio-economic planning sciences 26 (4): 275‑287. https://doi.org/10.1016/0038-0121(92)90004-O.\n\n\nKulldorff, Martin. 1997. « A spatial scan statistic. »\nCommunications in Statistics-Theory and methods 26 (6):\n1481‑1496. https://doi.org/10.1080/03610929708831995.\n\n\nLebart, Ludovic, Alain Morineau et Marie Piron. 1995. Statistique\nexploratoire multidimensionnelle. Dunod.\n\n\nLee, Sang-Il. 2001. « Developing a bivariate spatial association\nmeasure: an integration of Pearson’s r and Moran’s I. »\nJournal of geographical systems 3: 369‑385. https://doi.org/10.1007/s101090100064.\n\n\nLeSage, James P et R. Kelly Pace. 2008. An introduction to spatial\neconometrics. 123. CRC Press.\n\n\nLevine, Ned. 2006. « Crime mapping and the Crimestat\nprogram. » Geographical analysis 38 (1): 41‑56. https://doi.org/10.1111/j.0016-7363.2005.00673.x.\n\n\n———. 2021. « CrimeStat IV. » In The Encyclopedia of\nResearch Methods in Criminology and Criminal Justice, sous la dir.\nde JC Barnes et David R Forde, 1:28‑32. John Wiley & Sons. https://doi.org/10.1002/9781119111931.ch6.\n\n\nLi, Xun et Luc Anselin. 2023. rgeoda: R Library for Spatial Data\nAnalysis. s.n. https://CRAN.R-project.org/package=rgeoda.\n\n\nLoader, Clive. 2006. Local regression and likelihood. Springer\nScience & Business Media.\n\n\nLópez Castro, Marco Antonio, Marius Thériault et Marie-Hélène\nVandersmissen. 2015. « Évolution de la\nmobilité des membres de familles monoparentales dans la\nrégion métropolitaine de Québec\nde 1996 à 2006: comparaison entre les ménages\nmatricentriques et patricentriques. » Cahiers de\ngéographie du Québec 59 (167): 209‑250.\nhttps://doi.org/10.7202/1036355ar.\n\n\nLuo, Wei et Yi Qi. 2009. « An enhanced two-step floating catchment\narea (E2SFCA) method for measuring spatial accessibility to primary care\nphysicians. » Health & Place 15 (4). Elsevier:\n1100‑1107. https://doi.org/10.1016/j.healthplace.2009.06.002.\n\n\nLuo, Wei et Fahui Wang. 2003. « Measures of spatial accessibility\nto health care in a GIS environment: synthesis and a case study in the\nChicago region. » Environment and planning B: planning and\ndesign 30 (6): 865‑884. https://doi.org/10.1068/b29120.\n\n\nMcGrail, Matthew R. 2012. « Spatial accessibility of primary\nhealth care utilising the two step floating catchment area method: an\nassessment of recent improvements. » International Journal of\nHealth Geographics 11. Springer: 1‑12. https://doi.org/10.1186/1476-072X-11-50.\n\n\nMcGrail, Matthew R et John S Humphreys. 2009. « Measuring spatial\naccessibility to primary care in rural areas: Improving the\neffectiveness of the two-step floating catchment area method. »\nApplied geography 29 (4): 533‑541. https://doi.org/10.1016/j.apgeog.2008.12.003.\n\n\nMitchel, Andy. 2005. The ESRI Guide to GIS analysis, Volume 2:\nSpartial measurements and statistics. ESRI press.\n\n\nMoran, Patrick. 1950. « A test for the serial independence of\nresiduals. » Biometrika 37 (1/2): 178‑181. https://doi.org/10.2307/2332162.\n\n\nMorgan, Malcolm, Young Marcus, Robin Lovelace et Layik Hama. 2019.\n« OpenTripPlanner for R. » Journal of Open Source\nSoftware 4 (44): 1926. https://10.21105/joss.01926.\n\n\nNaimi, Babak, Nicholas AS Hamm, Thomas A Groen, Andrew K Skidmore,\nAlbertus G Toxopeus et Sara Alibakhshi. 2019. « ELSA:\nEntropy-based local indicator of spatial association. »\nSpatial statistics 29: 66‑88. https://doi.org/10.1016/j.spasta.2018.10.001.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer Palettes. s.n.\nhttps://CRAN.R-project.org/package=RColorBrewer.\n\n\nNgui, André Ngamini et Philippe Apparicio. 2011. « Optimizing the\ntwo-step floating catchment area method for measuring spatial\naccessibility to medical clinics in Montreal. » BMC health\nservices research 11 (1): 1‑12. https://doi.org/10.1186/1472-6963-11-166.\n\n\nOkabe, Atsuyuki et Kokichi Sugihara. 2012. Spatial analysis along\nnetworks: statistical and computational methods. John Wiley &\nSons.\n\n\nOpenshaw, Stan. 1977. « A geographical solution to scale and\naggregation problems in region-building, partitioning and spatial\nmodelling. » Transactions of the institute of british\ngeographers: 459‑472. https://doi.org/10.2307/622300.\n\n\nOpenshaw, Stan et Liang Rao. 1995. « Algorithms for reengineering\n1991 Census geography. » Environment and planning A 27\n(3): 425‑446. https://doi.org/10.1068/a270425.\n\n\nOrava, Jan. 2011. « K-nearest neighbour kernel density estimation,\nthe choice of optimal k. » Tatra Mountains Mathematical\nPublications 50 (1): 39‑50.\n\n\nOrd, J Keith et Arthur Getis. 1995. « Local spatial\nautocorrelation statistics: distributional issues and an\napplication. » Geographical analysis 27 (4): 286‑306. https://doi.org/10.1111/j.1538-4632.1995.tb00912.x.\n\n\nPebesma, Edzer. 2018. « Simple Features for R: Standardized\nSupport for Spatial Vector Data. » The R\nJournal 10 (1): 439‑446. https://doi.org/10.32614/RJ-2018-009.\n\n\nPebesma, Edzer et Roger Bivand. 2005. « Classes and methods for\nspatial data in R. » R News 5 (2): 9‑13. https://CRAN.R-project.org/doc/Rnews/.\n\n\nPenchansky, Roy et J William Thomas. 1981. « The concept of\naccess: definition and relationship to consumer satisfaction. »\nMedical care: 127‑140. https://doi.org/10.1097/00005650-198102000-00001.\n\n\nPereira, Rafael H. M., Marcus Saraiva, DanielHerszenhut, Carlos Kaue\nVieira Braga et Matthew Wigginton Conway. 2021. « r5r: Rapid\nRealistic Routing on Multimodal Transport Networks with R5 in\nR. » Findings. https://doi.org/10.32866/001c.21262.\n\n\nSander, Jörg, Martin Ester, Hans-Peter Kriegel et Xiaowei Xu. 1998.\n« Density-based clustering in spatial databases: The algorithm\ngdbscan and its applications. » Data mining and knowledge\ndiscovery 2: 169‑194. https://doi.org/10.1023/A:1009745219419.\n\n\nSokal, Robert R, Neal L Oden et Barbara A Thomson. 1998. « Local\nspatial autocorrelation in a biological model. » Geographical\nAnalysis 30 (4): 331‑354. https://doi.org/10.1111/j.1538-4632.1998.tb00406.x.\n\n\nSteenberghen, Thérèse, Koen Aerts et Isabelle Thomas. 2010.\n« Spatial clustering of events on a network. » Journal\nof Transport Geography 18 (3): 411‑418. https://doi.org/10.1016/j.jtrangeo.2009.08.005.\n\n\nTennekes, Martijn. 2018. « tmap: Thematic Maps in\nR. » Journal of Statistical Software 84\n(6): 1‑39. https://doi.org/10.18637/jss.v084.i06.\n\n\nTobler, Waldo R. 1970. « A computer movie simulating urban growth\nin the Detroit region. » Economic geography 46 (sup1):\n234‑240. https://doi.org/10.2307/143141.\n\n\nTveite, Håvard. 2020. « The QGIS\nStandard Deviational Ellipse\nPlugin. » http://plugins.qgis.org/plugins/SDEllipse/.\n\n\nVisvalingam, Maheswari et James D Whyatt. 1993. « Line\ngeneralisation by repeated elimination of points. » The\ncartographic journal 30 (1): 46‑51. https://doi.org/10.3138/FM57-6770-U75U-7727.\n\n\nWang, Bin, Wenzhong Shi et Zelang Miao. 2015. « Confidence\nanalysis of standard deviational ellipse and its extension into higher\ndimensional Euclidean space. » PloS one 10 (3):\ne0118537. https://doi.org/10.1371%2Fjournal.pone.0118537.\n\n\nWickham, Hadley. 2016. ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWong, David WS. 1999. « Geostatistics as measures of spatial\nsegregation. » Urban geography 20 (7): 635‑647. https://doi.org/10.2747/0272-3638.20.7.635.\n\n\nWong, David WS et Jay Lee. 2005. Statistical analysis of geographic\ninformation with ArcView GIS and ArcGIS. Wiley.\n\n\nWood, Simon N. 2011. « Fast stable restricted maximum likelihood\nand marginal likelihood estimation of semiparametric generalized linear\nmodels. » Journal of the Royal Statistical Society Series B:\nStatistical Methodology 73 (1): 3‑36. https://doi.org/10.1111/j.1467-9868.2010.00749.x.\n\n\nYamada, Ikuho et Jean-Claude Thill. 2007. « Local indicators of\nnetwork-constrained clusters in spatial point patterns. »\nGeographical analysis 39 (3). Wiley Online Library: 268‑292. https://www.jstor.org/stable/40645354."
  }
]