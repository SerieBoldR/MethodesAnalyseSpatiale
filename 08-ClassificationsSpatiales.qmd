# M√©thodes de classification non supervis√©e spatiale {#sec-chap08}

Les m√©thodes de classification visent √† regrouper des observations d'un jeu de donn√©es en plusieurs classes en fonction de leurs caract√©ristiques √©valu√©es √† partir de plusieurs variables. Appliqu√©es √† une couche spatiale (polygones, points, lignes), il s'agit alors de classifier les unit√©s spatiales sur la base de plusieurs de leurs attributs mesur√©s √† partir de variables. Dans le cadre de ce chapitre, nous abordons deux principales familles de m√©thodes de classification non supervis√©e¬†: **celle avec une contrainte spatiale** (algorithmes AZP, SKATER, REDCAP) et **celle avec une dimension spatiale** (*ClustGeo* et classification floue c-moyennes spatiale).

Sommairement, les deux derni√®res m√©thodes sont des extensions spatiales de la classification ascendante hi√©rarchique (CAH) et de l'algorithme flou c-moyennes. Par cons√©quent, la lecture de ce chapitre n√©cessite de bien ma√Ætriser le fonctionnement de la CAH et des k-moyennes (*k-means*). Si ce n'est pas le cas, nous vous invitons vivement √† lire le [chapitre suivant](https://laeq.github.io/LivreMethoQuantBolR/chap13.html) [@RBoldAir].

::: bloc_attention
::: bloc_attention-header
::: bloc_attention-icon
:::

**Bref retour sur les m√©thodes de classification**
:::

::: bloc_attention-body
Il existe de nombreuses m√©thodes de classification. Nous distinguons habituellement plusieurs familles de m√©thodes de classification, celles non supervis√©es versus supervis√©es et celles strictes versus floues¬†:

-   **Les m√©thodes de classification non supervis√©e** ¬´¬†\[...\] rel√®vent de la statistique exploratoire multidimensionnelle et permettent de regrouper automatiquement les observations sans avoir de connaissance pr√©alable sur la nature des classes pr√©sentes dans l'ensemble de donn√©es [@lebart1995statistique]. Les m√©thodes les plus connues dans ce domaine sont l'algorithme de Classification ascendante hi√©rarchique (CAH) et la m√©thode des k-moyennes (*k-means*)¬†¬ª [@ClassificationFloue, p. 1]. √Ä cela s'ajoutent d'autres m√©thodes comme les k-m√©dianes [@jain1988algorithms] ou encore les k-m√©do√Ødes [@kaufman1990partitioning] et la classification mixte combinant k-moyennes et CAH [@lebart1995statistique]. Pour regrouper les observations, ces m√©thodes (CAH, k-moyennes, k-m√©dianes, k-m√©do√Ødes, classification mixte) sont bas√©es sur la distance (proximit√©) entre les observations tandis que d'autres m√©thodes sont bas√©es sur la densit√© des observations (algorithmes DBSCAN, HDBSCAN, STDBSCAN, OPTICS abord√©s √† la [section @sec-041]).

-   **Les m√©thodes de classification supervis√©e** ¬´¬†\[...\] permettent d'affecter des observations √† partir d'un √©chantillon d√©j√† classifi√©, souvent appel√© classe d'entra√Ænement. Parmi les m√©thodes supervis√©es les plus connues, on retrouve les for√™ts d'arbres d√©cisionnels, les r√©seaux de neurones artificiels et l'analyse factorielle discriminante¬†¬ª [@ClassificationFloue, p. 1]. Plus exactement, ces m√©thodes visent √† apprendre des r√®gles bas√©es sur les attributs des observations pour d√©terminer √† quel groupe chaque observation doit √™tre attribu√©e. Ces r√®gles peuvent ensuite √™tre utilis√©es pour d√©terminer la cat√©gorie de nouvelles observations.

-   Que la classification soit ou non supervis√©e, ¬´¬†on distingue g√©n√©ralement les **m√©thodes strictes** (ou de partition) des **m√©thodes floues**. \[...\] Dans une classification stricte, chaque observation appartient √† une seule classe¬†: math√©matiquement, l'appartenance √† une classe donn√©e est binaire (0 ou 1), tandis que dans une classification floue, chaque observation a une probabilit√© d'appartenance variant de 0 √† 1 pour chacune des classes¬†¬ª [@ClassificationFloue, p. 1-2].
:::
:::

**Pourquoi recourir √† des m√©thodes de classification non supervis√©e spatiale?**

Dans un article r√©cent, Gelb et Apparicio [-@ClassificationFloue] identifient deux principales limites √† l'application d'une m√©thode de classification non supervis√©e a-spatiale (comme le CAH et le *k-means*) sur des donn√©es spatiales¬†:

1.  **La non-prise en compte de la dimension spatiale constitue une perte d'information¬†:** ¬´¬†\[...\]¬† une partie de l'information propre aux donn√©es, √† savoir leur localisation, n'est pas prise en compte dans le processus de classification. Or, la dimension g√©ographique est souvent tr√®s structurante; par cons√©quent, l'occulter revient √† perdre une quantit√© non n√©gligeable d'information. Il convient toutefois de nuancer quelque peu ce propos. G√©n√©ralement, la cartographie des m√©thodes de classification non supervis√©e a-spatiale (CAH et *k-means*) r√©v√®lent des effets de voisinage, d'autant plus que les variables introduites dans la classification sont fortement autocorr√©l√©es positivement¬†¬ª [@ClassificationFloue, p. 6].
2.  **Limiter l'effet de mitage**¬†: ¬´¬†\[...\] dans un contexte d'autocorr√©lation spatiale positive, des observations proches spatialement devraient plus vraisemblablement appartenir au m√™me groupe. Avec les m√©thodes de classification a-spatiale, il est fr√©quent d'observer des ph√©nom√®nes de mitage, c'est-√†-dire des observations appartenant √† un groupe *b* et isol√©es au milieu d'un ensemble d'observations appartenant au groupe *a*. Ce ph√©nom√®ne peut s'expliquer par la pr√©sence ici et l√† d'autocorr√©lation spatiale locale n√©gative, c'est-√†-dire des observations dont les caract√©ristiques s√©mantiques diff√®rent de leurs voisines. Souvent, la dissimilarit√© s√©mantique entre ces observations est n√©gligeable et ne justifie pas cette rupture spatiale¬†¬ª [@ClassificationFloue, p. 6].

::: bloc_package
::: bloc_package-header
::: bloc_package-icon
:::

**Liste des *packages* utilis√©s dans ce chapitre**
:::

::: bloc_package-body
-   Pour importer et manipuler des fichiers g√©ographiques¬†:
    -   `sf` pour importer et manipuler des donn√©es vectorielles.
    -   `spdep` pour construire des matrices de pond√©ration spatiale.
-   Pour construire des cartes et des graphiques¬†:
    -   `tmap` est certainement le meilleur *package* pour la cartographie.
    -   `ggplot2` pour construire des graphiques.
    -   `ggpubr` pour combiner des graphiques.
-   Pour les m√©thodes de classification avec une contrainte spatiale¬†:
    -   `rgeoda` pour les algorithmes AZP, SKATER et REDCAP.
    -   `spdep` pour l'algorithme SKATER.
-   Pour les m√©thodes de classification avec une dimension spatiale¬†:
    -   `ClustGeo` pour la m√©thode ClustGeo.
    -   `geocmeans` pour la classification k-moyennes floue et spatiale.
:::
:::

## M√©thodes de classification non supervis√©e avec contrainte spatiale {#sec-081}

Nous avons vu que l'objectif d'une m√©thode non supervis√©e appliqu√©e √† des donn√©es spatiales est de regrouper en *n* classes les unit√©s spatiales d'une couche g√©ographique. Prenons l'exemple de quatre variables environnementales cartographi√©es √† la @fig-figVarEnv pour les IRIS de la ville de Lyon, dont trois consid√©r√©es comme des nuisances (bruit, dioxyde d'azote et particules fines) et une consid√©r√©e comme avantageuse (v√©g√©tation).

```{r}
#| echo: false 
#| message: false 
#| warning: false
#| label: fig-figVarEnv
#| fig-align: center
#| fig-cap: "Cartographie des variables environnementales"
#| out-width: 100%

load("data/chap08/DonneesLyon.Rdata")
library(sf)
library(tmap)
library(spdep)
library(rgeoda)
tmap_mode("plot")
Carte.Lden <- tm_shape(LyonIris)+
  tm_fill(col="Lden", n = 5, style = "quantile", palette = "YlOrRd", title="Lden", legend.format = list(text.separator = "√†")) +
  tm_layout(frame=FALSE, 
            main.title = "Bruit", 
            main.title.position = "center", 
            main.title.size = 1)
Carte.NO2 <- tm_shape(LyonIris)+
  tm_fill(col="NO2", n = 5, style = "quantile", palette = "YlOrRd", title="¬µg/m3", legend.format = list(text.separator = "√†")) +
  tm_layout(frame=FALSE, 
            main.title = "Dixoyde d'azote", 
            main.title.position = "center", 
            main.title.size = 1)
Carte.PM25 <- tm_shape(LyonIris)+
  tm_fill(col="PM25", n = 5, style = "quantile", palette = "YlOrRd", title="¬µg/m3", legend.format = list(text.separator = "√†")) +
  tm_layout(frame=FALSE, 
            main.title = "Particule fines (PM2,5)", 
            main.title.position = "center", 
            main.title.size = 1)
Carte.Vege <- tm_shape(LyonIris)+
  tm_fill(col="VegHautPrt", n = 5, style = "quantile", palette = "YlOrRd", title="En %", legend.format = list(text.separator = "√†")) +
  tm_layout(frame=FALSE, 
            main.title = "Canop√©e", 
            main.title.position = "center", 
            main.title.size = 1) + tm_scale_bar(breaks = c(0,5,10))
tmap_arrange(Carte.Lden, Carte.NO2, Carte.PM25, Carte.Vege, ncol = 2)
```

Il est possible de regrouper les unit√©s spatiales avec ou sans contrainte spatiale¬†:

-   **Sans contrainte spatiale**, nous cherchons √† regrouper les IRIS (unit√©s spatiales) avec des valeurs similaires pour les quatre variables retenues (L~den~, NO~2~, PM~2,5~ et pourcentage de canop√©e). Cette approche est illustr√©e √† la @fig-figKmeansSkater (a) avec l'algorithme k-moyennes (*k-means* en anglais) avec cinq classes.

-   **Avec contrainte spatiale**, nous cherchons √† regrouper les IRIS (unit√©s spatiales) avec des valeurs similaires pour les quatre variables retenues, tout en nous assurant que les regroupements forment des r√©gions avec une absence de mitage. Cette approche est illustr√©e √† la @fig-figKmeansSkater (b) avec l'algorithme SKATER (*Spatial 'K'luster Analysis by Tree Edge Removal*) avec cinq classes. Autrement dit, l'objectif des m√©thodes de classification non supervis√©e avec contrainte spatiale est d'agr√©ger *n*¬†unit√©s spatiales en *m*¬†r√©gions non discontinues (avec *n*¬†\<¬†*m*) et coh√©rentes du point de vue de leurs attributs [@openshaw1995algorithms, p. 428].

```{r}
#| echo: false 
#| message: false 
#| warning: false
#| label: fig-figKmeansSkater
#| fig-align: center
#| fig-cap: "Classification non supervis√©e avec et sans contrainte spatiale"
#| out-width: 85%

set.seed(123)
VarsEnv <- c("Lden", "NO2", "PM25", "VegHautPrt")
Data <- data.frame(scale(st_drop_geometry(LyonIris)[VarsEnv]))
# Kmeans
LyonIris$Kmeans5 <- as.character(kmeans(Data[, VarsEnv], centers = 5, iter.max = 100)$cluster)
# Skater
Lyon.nb <- poly2nb(LyonIris)
lcosts <- nbcosts(Lyon.nb, Data)
Lyon.w <- nb2listw(Lyon.nb, lcosts, style="B")
Lyon.mst <- mstree(Lyon.w)
Skater5 <- spdep::skater(edges = Lyon.mst[,1:2], data = Data, method = "euclidean", ncuts = 4)
LyonIris$Skater5 <- as.character(Skater5$groups)
  # Cartographie
Carte.kmeans <-
  tm_shape(LyonIris)+
  tm_borders(col="gray", lwd=.5)+
  tm_fill(col="Kmeans5", palette = "Set1", title ="")+
  tm_layout(frame=FALSE, 
            main.title = "a. Sans contrainte spatiale", 
            main.title.position = "center", 
            main.title.size = 1)
Carte.skater <-
  tm_shape(LyonIris)+
  tm_borders(col="gray", lwd=.5)+
  tm_fill(col="Skater5", palette = "Set1", title ="")+
  tm_layout(frame=FALSE, 
            main.title = "b. Avec contrainte spatiale", 
            main.title.position = "center", 
            main.title.size = 1)+
  tm_scale_bar(breaks = c(0,5,10))
LyonIris$Skater5 <- NULL
tmap_mode("plot")
tmap_arrange(Carte.kmeans, Carte.skater, ncol = 2, nrow = 1)
```

::: bloc_objectif
::: bloc_objectif-header
::: bloc_objectif-icon
:::

**Int√©r√™t et limites des m√©thodes de classification avec une contrainte spatiale**
:::

::: bloc_objectif-body
Selon Gelb et Apparicio [-@ClassificationFloue, p. 7], le r√©sultat d'une m√©thode de classification avec une contrainte spatiale est ¬´¬†la cr√©ation de r√©gions tr√®s coh√©rentes spatialement, c'est-√†-dire avec une absence de mitage. Autrement dit, avec ces m√©thodes, il n'est pas possible d'identifier de groupes qui seraient spatialement discontinus, c'est-√†-dire compos√©s de plusieurs ensembles r√©gionaux s√©par√©s. L'impossibilit√© d'obtenir du mitage au sein des diff√©rentes r√©gions peut masquer la pr√©sence de valeurs fortement dissemblables localement, malgr√© la prise en compte de l'espace. Or, ces observations syst√©matiquement diff√©rentes de leurs voisines doivent faire l'objet d'une attention particuli√®re dans les exercices de classification int√©grant l'espace, ce que ne permettent pas ces m√©thodes d'agr√©gation spatiale.

Les limites de ces m√©thodes, particuli√®rement celles relatives au mitage, ont conduit plus r√©cemment √† la mise au point de nouvelles m√©thodes incluant l'espace dans le processus de classification, sans imposer une contrainte de contigu√Øt√©. Plus sp√©cifiquement, ces nouvelles m√©thodes sont des modifications des algorithmes classiques, tels que la CAH ou le FCM, pour int√©grer la dimension spatiale en parall√®le √† la dimension s√©mantique des donn√©es. En d'autres termes, l'espace n'est plus int√©gr√© comme une contrainte dans les algorithmes de classification, mais plut√¥t comme une donn√©e suppl√©mentaire¬†¬ª.
:::
:::

Les principaux algorithmes de classification non supervis√©e avec contrainte spatiale (*Spatially Constrained Clustering Methods* en anglais) sont¬†:

-   La m√©thode de zonage automatique (*Automatic Zoning Procedure* en anglais) (AZP) propos√©e par Openshaw [-@openshaw1977geographical], puis am√©lior√©e par Openshaw et Rao [-@openshaw1995algorithms].

-   L'algorithme SKATER (*Spatial 'K'luster Analysis by Tree Edge Removal*) [@assunccao2006efficient].

-   L'algorithme REDCAP (*Regionalization with dynamically constrained agglomerative clustering and partitioning*) [@guo2008regionalization].

-   L'algorithme du *max-p-regions problem* [@duque2012max].

Pour mettre en ≈ìuvre ces diff√©rents algorithmes, nous utilisons le *package* `rgeoda` [@packagergeoda]. Notez que l'algorithme SKATER est aussi impl√©ment√© dans le *package* `spded` (fonction `skater`).

### Algorithmes AZP {#sec-0811}

L'algorithme AZP (*Automatic Zoning Problem*) est une approche it√©rative et heuristique visant √† regrouper des polygones adjacents en *m* r√©gions, tout en maximisant la variance interr√©gionale (variance interclasse) et en minimisant la variance intrar√©gionale (variance intraclasse) calcul√©es sur les *p* variables. Autrement dit, il vise √† cr√©er des r√©gions non discontinues les plus homog√®nes possibles et les plus dissemblables entre elles sur la base des *p* variables. Pour utiliser l'AZP, il faut sp√©cifier le nombre de r√©gions (*m*) d√©sir√©. Notez qu'il existe trois algorithmes pour l'AZP¬†:

1.  AZP (*Automatic Zoning Procedure*), soit la premi√®re version par Stan Openshaw [-@openshaw1977geographical].
2.  AZP-SA (*A simulated annealing AZP method*) [@openshaw1995algorithms].
3.  AZP-TABU (*A tabu search heuristic version of AZP*) [@openshaw1995algorithms].

Pour une description d√©taill√©e de ces trois algorithmes, vous pouvez consulter Openshaw et Rao [-@openshaw1995algorithms] ou encore le [lien suivant](https://geodacenter.github.io/workbook/9d_spatial4/lab9d.html#principle).

Appliquons ces algorithmes aux 506¬†IRIS de la ville de Lyon avec les quatre variables environnementales pr√©alablement centr√©es r√©duites (bruit, dioxyde d'azote, particules fines et pourcentage de v√©g√©tation) et une matrice de contigu√Øt√© selon le partage d'un n≈ìud. Le *package* `rgeoda` comprend trois fonctions pour l'AZP¬†: `azp_greedy` (AZP), `azp_sa` (AZP-SA), `azp_tabu` (AZP-TABU). Pour l'exercice, nous fixons le nombre de r√©gions √†¬†5. Notez que par d√©faut, les variables seront centr√©es r√©duites (moyenne¬†=¬†0 et √©cart-type¬†=¬†1) avec le param√®tre `scale_method="standardize"`.

```{r}
#| echo: true 
#| message: false 
#| eval: true
library(rgeoda)
library(sf)
library(tmap)
## Variables
VarsEnv <- c("Lden", "NO2", "PM25", "VegHautPrt")
## Dataframe sans la g√©om√©trie et les quatre variables
load("data/chap08/DonneesLyon.Rdata")
Data <- st_drop_geometry(LyonIris[VarsEnv])
## Cr√©ation d'une matrice de contigu√Øt√© avec rgeoda
queen_w <- queen_weights(LyonIris)
## Calcul des trois algorithmes
azp <- rgeoda::azp_greedy(p=5,       # Nombre de r√©gions
                          w=queen_w, # Matrice contigu√Øt√©
                          df=Data,   # Tableau de donn√©es
                          scale_method = "standardize") # cote z
azp.sa <- rgeoda::azp_sa(p=5, w=queen_w, df=Data, cooling_rate = 0.85)
azp.tab <- rgeoda::azp_tabu(p=5, w=queen_w, df=Data, tabu_length = 10, conv_tabu = 10)
## Cr√©ation des trois champs dans la couche de Lyon
LyonIris$Azp <- as.character(azp$Clusters)
LyonIris$Azp_sa <- as.character(azp.sa$Clusters)
LyonIris$Azp_tab <- as.character(azp.tab$Clusters)
```

Cartographions les r√©sultats des trois algorithmes AZP (@fig-CartoAZP).

```{r}
#| echo: true 
#| message: false 
#| warning: false
#| label: fig-CartoAZP
#| fig-align: center
#| fig-cap: "Regroupements des 505 IRIS en cinq r√©gions selon les trois algorithmes AZP"
#| out-width: 100%

## Cartographie des r√©sultats
Carte.AZP1 <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="Azp", palette = "Set1", title ="")+
              tm_layout(frame=FALSE, 
                main.title = "a. AZP", 
                main.title.position = "center", 
                main.title.size = 1)
Carte.AZP2 <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="Azp_sa", palette = "Set1", title ="")+
              tm_layout(frame=FALSE, 
                main.title = "b. AZP Simulated Annealing", 
                main.title.position = "center", 
                main.title.size = 1)
Carte.AZP3 <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="Azp_tab", palette = "Set1", title ="")+
              tm_layout(frame=FALSE, 
                main.title = "c. AZP Tabu Search", 
                main.title.position = "center", 
                main.title.size = 1)
tmap_arrange(Carte.AZP1, Carte.AZP2, Carte.AZP3, ncol = 2, nrow = 2)
```

Par la suite, nous comparons les r√©sultats obtenus des trois algorithmes en reportant¬†:

1.  Les variances totale, intrar√©gionale et interr√©gionale, et surtout le ratio entre les variances intergroupe et totale. Ce ratio varie de 0 √† 1 et exprime la proportion de la variance des variables qui est expliqu√©e par les diff√©rentes r√©gions obtenues; plus il est √©lev√©, meilleur est le r√©sultat. Par cons√©quent, il peut √™tre utilis√© pour identifier la solution optimale entre les trois algorithmes.

2.  Le nombre d'observations par r√©gion.

3.  Les valeurs moyennes des variables centr√©es r√©duites par r√©gion.

```{r}
#| echo: true 
#| message: false 
#| eval: true
## Calcul du ratio entre les variances intergroupe et totale
cat("Ratio des variances interr√©gionale et totale",
    "\nAZP : ", round(azp$`The ratio of between to total sum of squares`, 3),
    "\nAZP-SA : ", round(azp.sa$`The ratio of between to total sum of squares`, 3),
    "\nAZP-TABU : ", round(azp.tab$`The ratio of between to total sum of squares`, 3)
)
```

√Ä la lecture des valeurs du ratio entre la variance interr√©gionale et la variance totale ci-dessus, la plus √©lev√©e est obtenue pour l'AZP-SA (0,518), suivie de celles de l'AZP (0,436) et de l'AZP-TABU (0,428). Nous retenons alors l'AZP-SA.

```{r}
#| echo: true 
#| message: false 
#| eval: true
## Nombre d'observations par r√©gion
table(LyonIris$Azp)
table(LyonIris$Azp_sa)
table(LyonIris$Azp_tab)
## Valeurs moyennes des variables centr√©es r√©duites par r√©gion
Data$Azp <- azp$Clusters
Data$Azp_sa <- azp.sa$Clusters
Data$Azp_tab <- azp.tab$Clusters
aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ Azp, data = Data, FUN = mean)
aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ Azp_sa, data = Data, FUN = mean)
aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ Azp_tab, data = Data, FUN = mean)
```

Les r√©sultats finaux de l'AZP-SA sont pr√©sent√©s au @tbl-dataAZPsa et √† la @fig-CartoAZPsa. L'analyse conjointe du tableau et de la carte permet ainsi d'interpr√©ter chacune des classes. En guise d'exemple, nous pouvons conclure que¬†:

-   **La r√©gion 1** comprend 221¬†IRIS localis√©s au centre de la ville de Lyon et caract√©ris√©s par des niveaux moyens √©lev√©s de bruit (57,4), de dioxyde d'azote (35) et de particules fines (18,8) √©lev√©s et un faible pourcentage de canop√©e (14,4¬†%).

-   Par contre, **la r√©gion 2** comprend 107¬†IRIS localis√©s √† l'extr√™me ouest de la ville et caract√©ris√©s par les plus faibles niveaux de polluants (51,1, 20,5 et 14,2) et une forte moyenne pour la canop√©e (28,7¬†%).

```{r}
#| label: tbl-dataAZPsa
#| tbl-cap: Valeurs moyennes des variables pour les cinq r√©gions obtenues par l'AZP-SA
#| echo: false
Temp1 <- aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ Azp_sa, data = st_drop_geometry(LyonIris), FUN = mean)
Temp2 <- data.frame(table(LyonIris$Azp_sa))
Temp1$Var1 <- Temp2$Freq
knitr::kable(Temp1,
           format.args = list(decimal.mark = ',', big.mark = " "),
		       digits = 1,
           col.names=c("R√©gion","Lden","NO2", "PM25", "V√©g√©tation", "Nombre d'IRIS"),
           align= c("c","c", "c", "c","c", "c"),
		   format = "markdown"
           )
```

```{r}
#| echo: false 
#| message: false 
#| warning: false
#| label: fig-CartoAZPsa
#| fig-align: center
#| fig-cap: "Regroupement des IRIS en cinq r√©gions selon l'AZP-TABU"
#| out-width: 100%
tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="Azp_sa", palette = "Set1", title ="R√©gion")+
              tm_layout(frame=FALSE)+tm_scale_bar(breaks=c(0,5,10))
```

Nous avons vu que pour les algorithmes AZP, il faut sp√©cifier le nombre de r√©gions. Nous l'avons fix√© arbitrairement √†¬†5. Comme pour n'importe quelle m√©thode de classification non supervis√©e, d√©terminer le nombre de classes optimal est une √©tape cruciale qui peut s'appuyer sur diff√©rentes techniques, dont la m√©thode du coude bas√©e sur l'inertie expliqu√©e (ici le ratio entre les variances interr√©gionale et totale), l'indicateur de silhouette et la m√©thode GAP. Pour une description d√©taill√©e de ces m√©thodes, consultez la [section suivante](https://laeq.github.io/LivreMethoQuantBolR/sect133.html#sect1332) [@RBoldAir]. Le code ci-dessous permet de r√©aliser un graphique avec les valeurs du ratio (inertie expliqu√©e) obtenues avec l'algorithme AZP-TABU calcul√© pour 2 √† 10 r√©gions. √Ä la lecture de la @fig-AZPCoude, nous observons deux ruptures (coudes) tr√®s nettes √† 5 et 8.

```{r}
#| echo: true 
#| eval : true
#| message: false 
#| warning: false
#| label: fig-AZPCoude
#| fig-align: center
#| fig-cap: "M√©thode du coude reposant sur l'inertie expliqu√©e pour l'AZP-TABU"
#| out-width: 75%
library(ggplot2)
nregions <- 2:10
Data <- data.frame(scale(st_drop_geometry(LyonIris)[VarsEnv]))
queen_w <- queen_weights(LyonIris)
inertie <- sapply(nregions, function(k){
  # calcul de l'AZP-TABU avec k
  resultat <- azp_tabu(p=k, w=queen_w, df=Data, tabu_length = 10, conv_tabu = 10)
  # r√©cup√©ration du ratio
  ratios <- resultat$`The ratio of between to total sum of squares`
  return(ratios)
})

df <- data.frame(k = nregions, ratio = inertie)
ggplot(df) + 
  geom_line(aes(x = k, y = ratio)) + 
  geom_point(aes(x = k, y = ratio), color = "red") + 
  labs(x = "Nombre de r√©gions", y = "Inertie expliqu√©e (%)")
```

### Algorithme SKATER {#sec-0812}

L'algorithme SKATER (*Spatial 'K'luster Analysis by Tree Edge Removal*) [@assunccao2006efficient] permet aussi de cr√©er des r√©gions sans discontinuit√©, en recourant √† une technique de la th√©orie des graphes, soit celle de l'arbre couvrant de poids minimal (*minimum spanning tree*). Succinctement, la classification est obtenue avec les √©tapes suivantes¬†:

1.  Cr√©ation d'un graphe de connectivit√© pour les polygones de la couche g√©ographique. Dans ce graphe, les n≈ìuds sont les centro√Ødes des polygones et les ar√™tes repr√©sentent les liaisons entre deux entit√©s spatiales voisines.

2.  Pour chaque ar√™te, nous calculons la dissimilarit√© (appel√©e co√ªt) des deux polygones voisins en fonction des *p* variables.

3.  Pour chaque polygone, nous retenons l'ar√™te avec le co√ªt minimal. Autrement dit, pour chaque polygone, nous retenons son polygone voisin qui lui est le plus semblable selon les *p* variables. Nous obtenons ainsi l'arbre couvrant de poids minimal.

4.  Cet arbre est ensuite √©lagu√© en supprimant les ar√™tes avec les plus forts co√ªts et en cr√©ant ainsi des sous-graphes en *m* r√©gions sans discontinuit√©.

Pour une description plus d√©taill√©e de l'algorithme, consultez l'article d'Assun√ß√£o *et al.* [-@assunccao2006efficient].

Le code ci-dessous permet de centrer et de r√©duire les quatre variables (fonction `scale`) et de construire la matrice de voisinage entre les polygones de la couche `LyonIris` (fonction `poly2nb` de `spdep`).

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true

library(spdep)
library(tmap)
## Variables
VarsEnv <- c("Lden", "NO2", "PM25", "VegHautPrt")
## Dataframe sans la g√©om√©trie et les quatre variables
load("data/chap08/DonneesLyon.Rdata")
Data <- st_drop_geometry(LyonIris[VarsEnv])
## Donn√©es centr√©es et r√©duites
LyonIrisZscore <- data.frame(scale(Data))
## Matrice voisinage
Lyon.nb <- poly2nb(LyonIris)
```

Calculons les co√ªts pour les ar√™tes reliant les n≈ìuds avec la fonction `nbcosts`. Nous constatons que le polygone¬†1 est voisin des polygones 27, 26, 44 et 74 avec des co√ªts de 1,34, 1,74, 1,15 et 16,3. Par cons√©quent, parmi ses quatre voisins, le polygone 1 est le plus semblable au polygone 44 (co√ªt minimal).

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true

## Calcul des co√ªts pour les ar√™tes
lcosts <- nbcosts(Lyon.nb, LyonIrisZscore)
head(Lyon.nb, n=1)
head(lcosts, n=1)
```

√Ä partir de ces co√ªts, nous pouvons trouver l'arbre couvrant de poids minimal (*minimum spanning tree*), objet d√©nomm√© ici `Lyon.mst` qui comprend trois colonnes¬†:

-   La premi√®re pour l'identifiant du polygone.
-   La seconde pour l'identifiant du polygone voisin.
-   La troisi√®me pour la valeur du co√ªt minimal (similarit√© selon les variables retenues).

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true

## Matrice de pond√©ration spatiale avec les co√ªts
Lyon.w <- nb2listw(Lyon.nb, lcosts, style="B")
### Trouver l'arbre couvrant de poids minimal
Lyon.mst <- mstree(Lyon.w)
head(Lyon.mst, n=3)
```

Le code ci-dessous permet de visualiser le graphe de connectivit√© et l'arbre couvrant de poids minimal (@fig-ArbreMinimum).

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: false
## Visualisation du graphe de connectivit√© 
coords <- st_coordinates(st_centroid(LyonIris))
plot(st_geometry(LyonIris), border="gray", lwd=.5, col="wheat")
plot(Lyon.nb, coords, add=TRUE, col="red", lwd=1)
## Visualisation de l'arbre couvrant de poids minimal
plot(st_geometry(LyonIris), border="gray", lwd=.5, col="wheat")
plot(Lyon.mst, coords, col="blue", cex.lab=0.7, add=TRUE)
```

![Graphe de connectivit√© et arbre couvrant de poids minimal](images/Chap08/ArbreMinimum.png){#fig-ArbreMinimum width="100%" fig-align="center"}

Le code ci-dessous permet de r√©aliser une classification SKATER avec cinq r√©gions avec le *package* `spdep`.

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true

## SKATER avec le package spdep
set.seed(123456789)
skater5.spdep <- spdep::skater(edges = Lyon.mst[,1:2], # premi√®res colonnes de l'arbre 
                               data = data.frame(LyonIrisZscore),
                               method = "euclidean",
                               ncuts = 4)  # k-1 r√©gions
table(skater5.spdep$groups)
```

Toutefois, il est plus simple d'utiliser la fonction `skater` de `rgeoda` qui ne n√©cessite pas de cr√©er au pr√©alable l'arbre couvrant de poids minimal.

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true

## SKATER avec le package rgeoda
library(rgeoda)
Data <- st_drop_geometry(LyonIris[VarsEnv])
queen_w <- queen_weights(LyonIris)
skater5.rgeoda <- rgeoda::skater(k = 5,        # k-1 r√©gions
                                 w = queen_w,  # matrice de contigu√Øt√©
                                 scale_method = "standardize",
                                 df = Data)    # dataframe
table(skater5.rgeoda$Clusters)
```

La @fig-SkaterAB d√©montre que les r√©sultats obtenus sont l√©g√®rement diff√©rents avec les deux *packages*.

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-SkaterAB
#| fig-align: center
#| fig-cap: "R√©sultats de l'algorithme SKATER avec cinq classes obtenus avec les *packages* `spdep` et `rgeoda`"
#| out-width: 85%

LyonIris$skater5spdep <- as.character(skater5.spdep$groups)
LyonIris$skater5rgeoda <- as.character(skater5.rgeoda$Clusters)

Carte.SkaterA <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="skater5spdep", palette = "Set1", title ="")+
              tm_layout(frame=FALSE, 
                main.title = "a. SKATER spdep", 
                main.title.position = "center", 
                main.title.size = 1)
Carte.SkaterB <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="skater5rgeoda", palette = "Set1", title ="")+
              tm_layout(frame=FALSE, 
                main.title = "b. SKATER rgeoda", 
                main.title.position = "center", 
                main.title.size = 1)
tmap_arrange(Carte.SkaterA, Carte.SkaterB)
```

::: bloc_aller_loin
::: bloc_aller_loin-header
::: bloc_aller_loin-icon
:::

**Algorithme SKATER avec un seuil minimal pour les classes**
:::

::: bloc_aller_loin-body
Dans une classification non supervis√©e avec une contrainte spatiale, il est possible de fixer un seuil minimal pour chaque r√©gion √† partir d'une variable. L'exemple le plus classique est l'obtention de *p* r√©gions qui doivent au moins avoir un nombre d'habitants fix√© par la personne utilisatrice. Pour ce faire, nous utilisons deux param√®tres de la fonction `spdep::skater`, soit `crit = 50000` pour fixer le seuil et `vec.crit = df$Population` pour indiquer le vecteur sur lequel est calcul√© le crit√®re.

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: false
clus10_min <- spdep::skater(edges = ct_mst[,1:2], 
                     # dataframe avec les variables centr√©es r√©duites
					 data = dfs,               
                     # seuil fix√©
					 crit = 50000,             
                     # variable population du dataframe
					 vec.crit = df$Population, 
                     ncuts = 4)
```

**Fonction `skater` : diff√©rences entre les *packages*`rgeoda` et `spdep`**

La fonction `skater` de `rgeoda` a deux principaux avantages¬†:

1.  Comme d√©crit pr√©c√©demment, l'avantage de la fonction `skater` de `rgeoda` est qu'elle ne n√©cessite pas de calculer au pr√©alable l'arbre couvrant de poids minimal.

2.  scale_method = c("raw", "standardize", "demean", "mad", "range_standardize", "range_adjust") permet de transformer directement les variables. La m√©thode par d√©faut est la cote z (moyenne¬†=¬†0 et √©cart-type¬†=¬†1).

Avec la fonction `skater` de `spdep`, vous devez pr√©alablement transformer vos variables et construire l'arbre couvrant de poids minimal. Par contre, elle int√®gre de nombreux types de distance pour √©valuer la dissimilarit√© entre les unit√©s spatiales avec le param√®tre `method = c("euclidean",  "maximum", "manhattan", "canberra", "binary", "minkowski",  "mahalanobis")` tandis que le param√®tre `distance_method = c("euclidean", "manhattan")` de `rgeoda` ne comprend que deux types de distance.
:::
:::

### Algorithmes REDCAP {#sec-0813}

Les diff√©rentes versions de l'algorithme REDCAP (*Regionalization with dynamically constrained agglomerative clustering and partitioning*) propos√© par Diansheng Guo [-@guo2008regionalization] sont aussi bas√©es sur la construction d'un arbre (*spanning tree*) dont l'√©lagage est obtenu de cinq diff√©rentes fa√ßons¬†:

-   Premier ordre et saut minimal (*First-order and Single-linkage*) qui fournit un r√©sultat identique √† l'algorithme SKATER.

-   Ordre complet et saut maximal (*Full-order and Complete-linkage*).

-   Ordre complet et saut moyen (*Full-order and Average-linkage*).

-   Ordre complet et saut minimal (*Full-order and Single-linkage*).

-   Ordre complet et crit√®re de Ward (*Full-order and Ward-linkage*).

Le code ci-dessous permet de calculer les cinq versions de l'algorithmes REDCAP avec cinq r√©gions et de comparer leurs r√©sultats √† partir du ratio (entre les variances interr√©gionale et totale) et du nombre d'observations par r√©gion.

```{r}
#| echo: true 
#| message: false 
#| eval: true
library(rgeoda)
library(sf)
## Pr√©paration des donn√©es 
Data <- st_drop_geometry(LyonIris[VarsEnv])
queen_w <- queen_weights(LyonIris)
## Algorithmes REDCAP
redcap5.A <- redcap(k = 5, w = queen_w, scale_method = "standardize", df = Data,
                    method = "firstorder-singlelinkage")
redcap5.B <- redcap(k = 5, w = queen_w, scale_method = "standardize", df = Data,
                    method = "fullorder-completelinkage")
redcap5.C <- redcap(k = 5, w = queen_w, scale_method = "standardize", df = Data,
                    method = "fullorder-averagelinkage")
redcap5.D <- redcap(k = 5, w = queen_w, scale_method = "standardize", df = Data,
                    method = "fullorder-singlelinkage")
redcap5.E <- redcap(k = 5, w = queen_w, scale_method = "standardize", df = Data,
                    method = "fullorder-wardlinkage")
## Comparaison des r√©sultats
Ratios <- data.frame(Methode = c("firstorder-singlelinkage", 
                                  "fullorder-completelinkage", 
                                  "fullorder-averagelinkage",
                                  "fullorder-singlelinkage", 
                                  "fullorder-wardlinkage"),
                      ratio = c(redcap5.A$`The ratio of between to total sum of squares`,
                                redcap5.B$`The ratio of between to total sum of squares`,
                                redcap5.C$`The ratio of between to total sum of squares`,
                                redcap5.D$`The ratio of between to total sum of squares`,
                                redcap5.E$`The ratio of between to total sum of squares`)
                      )
Nobs <- data.frame(rbind(table(redcap5.A$Clusters), 
              table(redcap5.B$Clusters), 
              table(redcap5.C$Clusters), 
              table(redcap5.D$Clusters), 
              table(redcap5.E$Clusters))
              )
names(Nobs) <- c("C1", "C2", "C3", "C4", "C5")
Ratios <- cbind(Ratios, Nobs)
Ratios
```

√Ä la lecture des valeurs du ratio ci-dessous, la meilleure classification serait celle obtenue avec un ordre complet et le crit√®re de Ward. Cartographions les r√©sultats des quatre versions de l'algorithme RECAP avec un ordre complet (@fig-CartoREDCAP).

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-CartoREDCAP
#| fig-align: center
#| fig-cap: Regroupements des 505 IRIS en cinq r√©gions selon les quatre versions de l'algorithme REDCAP avec un lien complet
#| out-width: 100%

## Ajout des champs dans la couche
LyonIris$RC5.FOcompletelinkage  <- as.character(redcap5.B$Clusters)
LyonIris$RC5.FOaveragelinkage   <- as.character(redcap5.C$Clusters)
LyonIris$RC5.FOsinglelinkage    <- as.character(redcap5.D$Clusters)
LyonIris$RC5.FOwardlinkage      <- as.character(redcap5.E$Clusters)
## Cartographie des r√©sultats
Carte.RCb <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="RC5.FOcompletelinkage", palette = "Set1", title ="")+
              tm_layout(frame=FALSE, 
                main.title = "a. Saut maximal", 
                main.title.position = "center", 
                main.title.size = 1)
Carte.RCc <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="RC5.FOaveragelinkage", palette = "Set1", title ="")+
              tm_layout(frame=FALSE, 
                main.title = "b. Saut moyen", 
                main.title.position = "center", 
                main.title.size = 1)
Carte.RCd <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="RC5.FOsinglelinkage", palette = "Set1", title ="")+
              tm_layout(frame=FALSE, 
                main.title = "c. Saut minimal", 
                main.title.position = "center", 
                main.title.size = 1)
Carte.RCe <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="RC5.FOwardlinkage", palette = "Set1", title ="")+
              tm_layout(frame=FALSE, 
                main.title = "d. Crit√®re de Ward", 
                main.title.position = "center", 
                main.title.size = 1)
tmap_arrange(Carte.RCb, Carte.RCc, Carte.RCd, Carte.RCe, ncol = 2, nrow = 2)
```

### Algorithme du *max-p-regions problem* {#sec-0814}

Cet algorithme, propos√© par Duque *et al.* [-@duque2012max], n'est pas d√©crit ici. Notez qu'il peut √™tre calcul√© avec trois fonctions du *package* `rgeoda`, soit `maxp_greedy`, `maxp_sa` et `maxp_tabu`.

## M√©thodes de classification non supervis√©e avec une dimension spatiale {#sec-082}

Nous avons vu que les m√©thodes de classification avec une contrainte spatiale visent √† obtenir des r√©gions non discontinues, c'est-√†-dire sans mitage spatial. L'objectif des m√©thodes de classification non supervis√©e avec une dimension spatiale est quelque peu diff√©rent¬†: classifier les observations en tenant compte de l'espace (proximit√©, voisinage entre les unit√©s spatiales) afin de limiter les effets de mitage, sans toutefois l'interdire.

Dans le cadre de cette section, nous d√©crivons deux de ces m√©thodes qui int√®grent la dimension spatiale de mani√®re diff√©rente¬†:

1.  **La m√©thode *ClustGeo***, qui est une extension de la classification ascendante hi√©rarchique, est une m√©thode de classification non supervis√©e, spatiale et stricte. Cette m√©thode repose sur deux matrices de dissimilarit√©¬†: une **matrice des distances s√©mantiques (attributaires)** calcul√©e sur les valeurs de plusieurs variables caract√©risant les observations et une **matrice de distances** (euclidienne le plus souvent) entre les entit√©s g√©ographiques. Nous cherchons ainsi √† regrouper les observations qui se ressemblent √† la fois selon leurs attributs et selon leur proximit√© spatiale.

2.  **La m√©thode k-moyennes spatiale et floue (*Spatial fuzzy c-means*)**, qui est une extension de la m√©thode k-moyennes, est une m√©thode de classification non supervis√©e, spatiale et floue. Cette m√©thode repose sur deux matrices de dissimilarit√©¬†: une **matrice s√©mantique** calcul√©e sur les valeurs de plusieurs variables caract√©risant les entit√©s g√©ographiques et une **matrice s√©mantique spatialement d√©cal√©e**. Nous cherchons ainsi √† regrouper les observations qui se ressemblent √† la fois selon leurs caract√©ristiques et celles de leurs unit√©s spatiales adjacentes ou proches.

Autrement dit, dans la m√©thode *ClustGeo*, l'espace est introduit sous la forme d'une matrice de distances entre les entit√©s spatiales (**agencement spatial**) tandis que dans la m√©thode du *Spatial fuzzy c-means*, il est introduit sous la forme d'une matrice de donn√©es s√©mantiques spatialement d√©cal√©es (**information s√©mantique dans l'environnement imm√©diat**).

### Classification ascendante hi√©rarchique spatiale (*ClustGeo*) {#sec-0821}

#### Description de la m√©thode *ClustGeo* {#sec-08211}

La m√©thode *ClustGeo*, propos√©e par Marie Chavent et ses coll√®gues [-@packageClustGeo], est une extension de la classification ascendante hi√©rarchique (CAH) qui int√®gre la dimension spatiale des entit√©s g√©ographiques. Cette m√©thode repose sur une id√©e brillante, soit de classer (regrouper) les observations (unit√©s spatiales) en combinant deux matrices de dissimilarit√©¬†:

-   Une matrice s√©mantique calcul√©e sur *p* variables caract√©risant les unit√©s spatiales ($D_0$).

-   Une matrice spatiale calcul√©e √† partir des distances spatiales (habituellement euclidienne) entre les unit√©s spatiales ($D_1$). Ces deux matrices sont ensuite fusionn√©es en une seule matrice finale repr√©sentant la combinaison de la distance spatiale et de la dissimilarit√© s√©mantique (attributaire) entre les observations. Notez qu'un param√®tre $\alpha$, variant de 0 √† 1, permet de d√©finir le poids de la matrice spatiale comparativement √† celui de la s√©mantique¬†:

    -   Avec $\alpha=0$, le poids accord√© √† la matrice spatiale est nul. Nous obtenons ainsi une CAH classique puisque seules les diff√©rences attributaires sont conserv√©es.

    -   Avec $\alpha=1$, le poids accord√© √† la matrice spatiale est maximal; la classification est alors purement spatiale et ignore les diff√©rences attributaires.

Par cons√©quent, ¬´¬†\[...\] l'enjeu principal est de fixer la valeur du param√®tre ùõº, consid√©rant qu'une augmentation de ùõº revient √† am√©liorer l'inertie expliqu√©e de la matrice spatiale, au d√©triment d'une perte de l'inertie expliqu√©e sur le plan s√©mantique¬†¬ª [@ClassificationFloue, p. 16].

#### Calcul de la CAH classique {#sec-08212}

::: bloc_attention
::: bloc_attention-header
::: bloc_attention-icon
:::

**Retour sur la classification ascendante hi√©rarchique (CAH)**
:::

::: bloc_attention-body
Pour une description d√©taill√©e de la CAH, consultez la [section suivante](https://laeq.github.io/LivreMethoQuantBolR/sect133.html) [@RBoldAir].
:::
:::

Le code ci-dessous permet de construire l'arbre de classification selon le crit√®re de Ward √† partir de la matrice s√©mantique (@fig-Dendrogramme).

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true
#| label: fig-Dendrogramme
#| fig-align: center
#| fig-cap: Arbre de classification (dendrogramme)
#| out-width: 100%

## Variables pour la CAH
VarsEnv <- c("Lden", "NO2", "PM25", "VegHautPrt")
## Dataframe sans la g√©om√©trie et les quatre variables
load("data/chap08/DonneesLyon.Rdata")
Data <- st_drop_geometry(LyonIris[VarsEnv])
## Centrage (moyenne = 0) et r√©duction des donn√©es (variance = 1)
DataZscore <- data.frame(scale(Data))
## Matrice s√©mantique : dissimilarit√© des observations selon les variables
Matrice.Semantique <- dist(DataZscore, method = "euclidean")
# Calcul du dendrogramme avec le crit√®re WARD
Arbre <- hclust(Matrice.Semantique, method = "ward.D")
plot(Arbre, hang = -1, label = FALSE,
     main = "Dendrogramme \n(arbre de classification selon le crit√®re de Ward)",
     sub = "", ylab = "Hauteur", xlab = ""
     )
```

√Ä la lecture de la @fig-CAHCoude, nous ne d√©tectons pas de seuils marqu√©s dans l'inertie expliqu√©e en fonction du nombre de groupes. Par cons√©quent, nous fixons arbitrairement le nombre de groupes √† 5.

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-CAHCoude
#| fig-align: center
#| fig-cap: M√©thode du coude reposant sur l'inertie expliqu√©e pour CAH
#| out-width: 65%

library(ggplot2)
# Fonction pour l'inertie expliqu√©e par les classes
prop_inert_cutree <- function(K,tree,n){
  P <- cutree(tree,k=K)
  W <- sum(tree$height[1:(n-K)])
  Tot <- sum(tree$height)
  return(1-W/Tot)
}
# Inertie expliqu√©e par des CAH de 2 √† 10 classes
df.inertie <- data.frame(NGroupes = 2:10,
                         Inertie = sapply(2:10,
                                          prop_inert_cutree,
                                          tree=Arbre,
                                          n=nrow(DataZscore)))
ggplot(df.inertie)+
  geom_line(aes(x=NGroupes,y=Inertie))+
  geom_point(aes(x=NGroupes,y=Inertie), color = "red") +
  labs(y = "Inertie expliqu√©e", x = "Nombre de groupes")
```

Nous pouvons visualiser l'arbre avec une coupure √† cinq classes.

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true

plot(Arbre, labels = FALSE, 
     main = "Partition en 2, 5 ou 9 classes", 
     xlab = "", ylab = "", sub = "", axes = FALSE, hang = -1)
rect.hclust(Arbre, 5, border = "red")
## Coupure de l'arbre √† cinq classes
LyonIris$CAH5 <- as.character(cutree(Arbre, k=5))
## Nombre d'observations par classe
table(LyonIris$CAH5)
## Valeurs moyennes des classes
aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ CAH5, data = st_drop_geometry(LyonIris), FUN = mean)
```

Les r√©sultats de la CAH sont cartographi√©s √† la @fig-CAHCarto tandis que le nombre d'observations et les valeurs moyennes des classes sont report√©s au tableau \@tbl-dataCAHtab.

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-CAHCarto
#| fig-align: center
#| fig-cap: CAH avec le crit√®re de Ward avec cinq classes
#| out-width: 65%

library(tmap)
## Cartographie des r√©sultats
Carte.CAH5 <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
              tm_fill(col="CAH5", palette = "Set1", title ="")+
              tm_layout(frame=FALSE, 
                main.title = "CAH (crit√®re de Ward)", 
                main.title.position = "center", 
                main.title.size = 1)
Carte.CAH5
```

```{r}
#| label: tbl-dataCAHtab
#| tbl-cap: Valeurs moyennes des variables pour les cinq classes obtenues avec la CAH
#| echo: false

Temp1 <- aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ CAH5, data = st_drop_geometry(LyonIris), FUN = mean)
Temp2 <- data.frame(table(LyonIris$CAH5))
Temp1$Var1 <- Temp2$Freq
knitr::kable(Temp1,
           format.args = list(decimal.mark = ',', big.mark = " "),
		       digits = 1,
           col.names=c("Classe","Lden","NO2", "PM25", "V√©g√©tation", "Nombre d'IRIS"),
           align= c("c","c", "c", "c","c", "c"),
           format = "markdown"
           )
```

#### Calcul de la m√©thode *ClustGeo* {#sec-08213}

**Calcul des deux matrices (s√©mantique et spatiale)**

Dans un premier temps, nous cr√©ons les matrices s√©mantique ($D_0$) et spatiale ($D_1$). Notez ici que les distances spatiales utilis√©es correspondent aux distances euclidiennes entre les centro√Ødes des IRIS.

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true

library(sf)
library(ClustGeo)
## Variables
VarsEnv <- c("Lden", "NO2", "PM25", "VegHautPrt")
## Dataframe sans la g√©om√©trie et les quatre variables
load("data/chap08/DonneesLyon.Rdata")
Data <- st_drop_geometry(LyonIris[VarsEnv])
## Centrage (moyenne = 0) et r√©duction des donn√©es (variance = 1)
DataZscore <- data.frame(scale(Data))
## Matrice s√©mantique : dissimilarit√© des observations selon les variables
Matrice.Semantique <- dist(DataZscore, method = "euclidean")
## Matrice spatiale entre les unit√©s spatiales
xy <- st_coordinates(st_centroid(LyonIris))
Matrice.Spatiale <- dist(xy, method = "euclidean")
```

**Optimisation de la valeur de** $\alpha$

Pour la m√©thode *ClustGeo* (avec k¬†=¬†5), nous √©valuons l'impact du param√®tre Œ± pour des valeurs de 0 √† 1, avec un saut de 0,05. Pour ce faire, nous utilisons la fonction `choicealpha` du *package* `ClustGeo`. √Ä la lecture de la @fig-GraphAlpha, nous constatons que¬†:

-   Plus la valeur de $\alpha$ augmente, plus l'inertie expliqu√©e par la matrice s√©mantique diminue (trait noir) et inversement, plus l'inertie expliqu√©e par la matrice spatiale est forte.

-   Avec $\alpha = \text{0,30}$, l'inertie expliqu√©e par la matrice spatiale est de 50¬†% pour une perte d'inertie expliqu√©e par la matrice s√©mantique de seulement 7¬†%. Avec $\alpha = \text{0,35}$, nous constatons une chute importante de l'inertie s√©mantique expliqu√©e. Par cons√©quent, nous retenons la valeur de 0,30 pour la m√©thode *ClustGeo*. Ce choix donne un excellent compromis entre la pr√©servation de l'inertie expliqu√©e des donn√©es attributaires et la coh√©rence spatiale de la classification finale.

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-GraphAlpha
#| fig-align: center
#| fig-cap: Impact des deux matrices dans la classification
#| out-width: 75%

alphas <- seq(0, 1, 0.05)
result <- choicealpha(D0 = Matrice.Semantique, # matrice s√©mantique
                      D1 = Matrice.Spatiale,   # matrice spatiale 
                      range.alpha = alphas,    # valeurs de alpha
                      K = 5,                   # nombre de classes
                      wt = NULL, scale = TRUE, graph = FALSE)
# Graphique avec Alpha
df.alpha <- data.frame(result$Q)
df.alpha$alpha <- alphas
ggplot(df.alpha)+
  geom_line(aes(x=alphas,y= Q0), color = "black")+
  geom_point(aes(x=alphas,y= Q0), color = "black", size=3) +
  geom_line(aes(x=alphas,y= Q1), color = "red")+
  geom_point(aes(x=alphas,y= Q1), color = "red", size=3) +
  labs(y = "Pseudo-inertie", 
       x = "Param√®tre alpha",
       subtitle = "Matrice s√©mantique (noir) et matrice spatiale (rouge)")
```

**R√©alisation de la m√©thode *ClustGeo***

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true

## Dendrogramme avec ClustGeo
Arbre.ClustGeo <- hclustgeo(D0 = Matrice.Semantique, D1 = Matrice.Spatiale, alpha = 0.30)
## Coupure de l'arbre √† cinq classes
LyonIris$ClustGeo5 <- as.character(cutree(Arbre.ClustGeo, k=5))
## Nombre d'observations par classe
table(LyonIris$ClustGeo5)
## Valeurs moyennes des classes
aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ ClustGeo5, 
          data = st_drop_geometry(LyonIris), FUN = mean)

```

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-GraphAlpha2
#| fig-align: center
#| fig-cap: "Classification ClustGeo avec alpha = 0,30"
#| out-width: 75%

## Cartographie des r√©sultats
Carte.ClusteGeo <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
                    tm_fill(col="ClustGeo5", palette = "Set1", title ="")+
                    tm_layout(frame=FALSE, 
                      main.title = "ClustGeo avec alpha = 0,30", 
                      main.title.position = "center", 
                      main.title.size = 1)
Carte.ClusteGeo
```

```{r}
#| label: tbl-dataClusteGeotab
#| tbl-cap: Valeurs moyennes des variables pour cinq classes obtenues par la m√©thode ClustGeo (alpha = 0,30)
#| echo: false
#| message: false
#| warning: false

Temp1 <- aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ ClustGeo5, 
                   data = st_drop_geometry(LyonIris), FUN = mean)
Temp2 <- data.frame(table(LyonIris$ClustGeo5))
Temp1$Var1 <- Temp2$Freq
knitr::kable(Temp1,
           format.args = list(decimal.mark = ',', big.mark = " "),
		   digits = 1,
           col.names=c("Classe","Lden","NO2", "PM25", "V√©g√©tation", "Nombre d'IRIS"),
           align= c("c","c", "c", "c","c", "c"),
		   format = "markdown"
           )
```

La comparaison des typologies obtenues avec la CAH et la ClusteGeo d√©montre clairement que l'introduction d'une matrice spatiale dans la classification *ClustGeo* g√©n√®re des classes qui sont moins discontinues spatialement (@fig-ComparaisonCAHClusteo).

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-ComparaisonCAHClusteo
#| fig-align: center
#| fig-cap: Impact des deux matrices dans la classification
#| out-width: 75%

tmap_arrange(Carte.CAH5, Carte.ClusteGeo, nrow = 1, ncol = 2)
```

### *Spatial fuzzy c-means* {#sec-0822}

La m√©thode SFCM (*Spatial fuzzy c-means*), propos√©e par Weiling Cai et ses coll√®gues [-@cai2007fast], est une extension de FCM, soit une version floue de l'algorithme des k-moyennes. Le principe de base est le suivant¬†:

¬´¬†Comparativement au FCM classique, le SFCM introduit dans son calcul, en plus du jeu de donn√©es original ($D_0$), une version spatialement d√©cal√©e ($D_s$) de ce dernier [@cai2007fast]. En analyse d'image, cela revient √† calculer $D_s$ en appliquant un filtre moyen ou m√©dian √† $D_0$ (la m√©diane √©tant moins sensible aux valeurs extr√™mes locales). Ce processus peut facilement s'appliquer √† des entit√©s g√©ographiques vectorielles, en cr√©ant une matrice de pond√©ration spatiale $W_{kl}$ (*l* √©tant les voisins de *k* et la diagonale de cette matrice valant 0) [@getis2009spatial] et en utilisant les poids de cette matrice dans le calcul d'une moyenne ou d'une m√©diane pond√©r√©e locale¬†¬ª [@ClassificationFloue, p. 8].

Comme pour la m√©thode *ClustGeo*, il est possible de fixer une pond√©ration √† la dimension spatiale ($D_s$) avec un param√®tre $\alpha$, qui varie de 0 √† $\infty$. Une valeur de 0 signale qu'aucun poids n'est accord√© √† la dimension spatiale, ce qui revient √† calculer un FCM classique. Si $\alpha = \text{2}$, alors la version spatialement d√©cal√©e aura deux fois plus de poids dans la classification que le jeu de donn√©es original.

::: bloc_attention
::: bloc_attention-header
::: bloc_attention-icon
:::

**Retour sur une variable spatialement d√©cal√©e et la classification k-moyennes**
:::

::: bloc_attention-body
La notion de variable spatialement d√©cal√©e a √©t√© abord√©e au chapitre¬†2 (@fig-Chap02FigureVariableSpatialementDecalee). Pour une description d√©taill√©e de la classification k-moyennes, consultez la [section suivante](https://laeq.github.io/LivreMethoQuantBolR/sect133.html) [@RBoldAir].
:::
:::

#### Calcul de la classification c-moyennes floue classique (*fuzzy c-means*) {#sec-08221}

√Ä des fins de comparaison avec la CAH, nous proposons de calculer une classification c-moyennes floue classique (*fuzzy c-means*) avec cinq classes (*k*¬†=¬†5). Puis, pour d√©terminer la valeur optimale de *m* (soit le degr√© de logique floue), nous calculons l'inertie expliqu√©e et l'indice de silhouette pour des valeurs de *m* variant de 1,1 √† 3 (avec un incr√©ment de 0,1). Les r√©sultats de ces deux indicateurs, pr√©sent√©s √† la @fig-GraphCmeans, signalent que¬†:

-   Plus le param√®tre *m* augmente, plus l'inertie expliqu√©e diminue (@fig-GraphCmeans, a).
-   La valeur de l'indice de silhouette est maximale avec *m*¬†=¬†1,8 (@fig-GraphCmeans, b).

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-GraphCmeans
#| fig-align: center
#| fig-cap: √âvaluation de la qualit√© de la classification FCM avec cinq classes selon le degr√© de flou (m)
#| out-width: 75%

library(geocmeans)
library(ggpubr)
## Variables pour la CAH
VarsEnv <- c("Lden", "NO2", "PM25", "VegHautPrt")
## Dataframe sans la g√©om√©trie et les quatre variables
load("data/chap08/DonneesLyon.Rdata")
Data <- st_drop_geometry(LyonIris[VarsEnv])
## Centrage (moyenne = 0) et r√©duction des donn√©es (variance = 1)
DataZscore <- data.frame(scale(Data))
## Dataframe pour les diff√©rents param√®tres avec k = 5 et degr√© de flou
FCM_selection <- select_parameters(algo = "FCM",
                  data = DataZscore,
                  k = 5, # nous pourrions ici tester avec k=2:10
                  m = seq(1.1,3,0.1),
                  classidx = TRUE, spconsist = FALSE,
                  tol = 0.001, seed = 456,
                  verbose = FALSE)
# Graphique avec l'inertie expliqu√©e
G1 <- ggplot(FCM_selection) +
      geom_line(aes(x = m, y = Explained.inertia)) +
      geom_point(aes(x = m,  y = Explained.inertia), color = "red")+
      labs(title ="a. Variation des donn√©es expliqu√©es",
           y = "Inertie expliqu√©e", x = "Param√®tre m")
# Graphique avec l'indice de silhouette
G2 <- ggplot(FCM_selection) +
      geom_line(aes(x = m, y = Silhouette.index)) +
      geom_point(aes(x = m,  y = Silhouette.index), color = "red")+
      labs(title ="b. Consistance des groupes",
           y = "Crit√®re de silhouette floue", x = "Param√®tre m")
# Combinaison des deux graphiques dans la figure
ggarrange(G1, G2)
```

R√©alisons la classification c-moyennes floue et cartographions les probabilit√©s d'appartenance √† chacune des classes (@fig-FigureCmeansProb), puis l'appartenance √† une classe (@fig-FigureCmeansCluster).

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-FigureCmeansProb
#| fig-align: center
#| fig-cap: Cartographie des probabilit√©s d'appartenance aux cinq classes avec la classification c-moyennes
#| out-width: 100%

## Classification du c-moyennes 
FCM <- CMeans(DataZscore, k = 5, m = 1.8, tol = 0.0001, verbose = FALSE, seed = 456)
## Calcul des indicateurs de qualit√©
calcqualityIndexes(DataZscore, FCM$Belongings, 1.5)

## Cartographie des probabilit√©s d'appartenance √† chaque classe
Cartes.FCM <- mapClusters(LyonIris,FCM$Belongings)
names(Cartes.FCM)
tmap_arrange(Cartes.FCM$ProbaMaps, ncol = 2)
```

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-FigureCmeansCluster
#| fig-align: center
#| fig-cap: Cartographie des classes issues de la classification c-moyennes
#| out-width: 75%

Cartes.FCM$ClusterPlot
```

#### Calcul de la classification c-moyennes floue et spatiale (*spatial fuzzy c-means*) {#sec-08222}

Dans l'exemple ci-dessous, nous calculons une classification SFCM (*spatial fuzzy c-means*). Nous avons d√©termin√© avec l'analyse du simple FCM que la valeur de 1,8 pour *m* semblait satisfaisante avec cinq groupes (*k*¬†=¬†5). Nous devons maintenant d√©terminer la valeur de *alpha*, soit le poids accord√© aux variables spatialement d√©cal√©es comparativement aux donn√©es originales.

Pour cela, nous calculons toutes les valeurs possibles d'*alpha* entre 0 et 2 avec un intervalle de 0,05. Nous comparons ensuite les valeurs des indices de silhouette (qualit√© s√©mantique de la classification) et d'incoh√©rence spatiale des diff√©rentes solutions.

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-SFCMChoiceAlpha
#| fig-align: center
#| fig-cap: Comparaison de diff√©rentes valeurs d'alpha pour le SFCM
#| out-width: 75%

library(spdep)
library(ggplot2)
# Cr√©ation d'une matrice de contigu√Øt√© standardis√©e
Neighbours <- poly2nb(LyonIris, queen = TRUE)
WMat <- nb2listw(Neighbours, style="W", zero.policy = TRUE)
SFCM_selection <- select_parameters(algo = "SFCM",
                  data = DataZscore,
                  k = 5,
                  m = 1.8,
                  nblistw = WMat,
                  alpha = seq(0,2,0.05),
                  classidx = TRUE,
                  tol = 0.001, seed = 456,
                  spconsist  = TRUE,
                  verbose = FALSE)

graph1 <- ggplot(SFCM_selection) + 
  geom_line(aes(x = alpha, y = Silhouette.index), color = 'black') + 
  geom_point(aes(x = alpha, y = Silhouette.index), color = 'red')+
  labs(x = "Alpha", y = "Indice de silhouette", )

graph2 <- ggplot(SFCM_selection) + 
  geom_line(aes(x = alpha, y = spConsistency), color = 'black') + 
  geom_point(aes(x = alpha, y = spConsistency), color = 'red')+
  labs(x = "Alpha", y = "Indice d'incoh√©rence spatiale")

ggarrange(graph1, graph2, ncol = 2, nrow = 1)

```

La @fig-SFCMChoiceAlpha d√©montre que l'augmentation d'*alpha* a pour effet de r√©duire la qualit√© s√©mantique de la classification (indice de silhouette), mais aussi de limiter l'inconsistance spatiale des r√©sultats produits. Nous constatons aussi que l'augmentation d'*alpha* a un effet quasiment lin√©aire sur la d√©gradation de l'indice de silhouette et un effet curvilin√©aire sur l'inconsistance spatiale. Cette derni√®re est donc plus impact√©e par les premi√®res augmentations d'*alpha*. Nous proposons ici de retenir une solution avec $\alpha$¬†=¬†0,55, car elle correspond √† une baisse relativement faible de l'indice de silhouette (0,55 versus 0,50) pour une augmentation importante de la coh√©rence spatiale des r√©sultats (0,39 versus 0,22).

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true

# Calcul du SFCM
SFCM <- SFCMeans(DataZscore, WMat, 
                 k = 5, 
                 m = 1.8, 
                 alpha = 0.55,
                 tol = 0.0001, standardize = FALSE,
                 verbose = FALSE, seed = 456)
```

**R√©sultats des indicateurs de qualit√© du SFCM**

```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: true
calcqualityIndexes(DataZscore, SFCM$Belongings, 1.5)
```

**Cartographie des probabilit√©s d'appartenance √† chaque classe**

La cartographie des probabilit√© d'appartenance pour chacune des cinq classes est pr√©sent√©e aux figures
[-@fig-FigureSFCMProb1], [-@fig-FigureSFCMProb2], [-@fig-FigureSFCMProb3], [-@fig-FigureSFCMProb4] et [-@fig-FigureSFCMProb4].

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-FigureSFCMProb1
#| fig-align: center
#| fig-cap: Cartographie des probabilit√©s d'appartenance issues de la classification SFCM (classe 1)
#| out-width: 100%
Cartes.SFCM <- mapClusters(LyonIris, SFCM$Belongings, undecided = 0.45)
Cartes.SFCM$ProbaMaps[[1]]
```


```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-FigureSFCMProb2
#| fig-align: center
#| fig-cap: Cartographie des probabilit√©s d'appartenance issues de la classification SFCM (classe 2)
#| out-width: 100%
Cartes.SFCM$ProbaMaps[[2]]
```

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-FigureSFCMProb3
#| fig-align: center
#| fig-cap: Cartographie des probabilit√©s d'appartenance issues de la classification SFCM (classe 3)
#| out-width: 100%
Cartes.SFCM$ProbaMaps[[3]]
```

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-FigureSFCMProb4
#| fig-align: center
#| fig-cap: Cartographie des probabilit√©s d'appartenance issues de la classification SFCM (classe 4)
#| out-width: 100%
Cartes.SFCM$ProbaMaps[[4]]
```

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-FigureSFCMProb5
#| fig-align: center
#| fig-cap: Cartographie des probabilit√©s d'appartenance issues de la classification SFCM (classe 5)
#| out-width: 100%
Cartes.SFCM$ProbaMaps[[5]]
```

**Cartographie des probabilit√©s d'appartenance √† chaque classe du SFCM (@fig-FigureSFCMCluster)**

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
#| label: fig-FigureSFCMCluster
#| fig-align: center
#| fig-cap: Cartographie des classes issues de la classification SFCM
#| out-width: 75%
Cartes.SFCM$ClusterPlot
```

Il est int√©ressant de noter que les groupes construits par les deux algorithmes (ClustGeo et SFCM) produisent des solutions assez diff√©rentes. Nous retrouvons dans les deux classifications un groupe caract√©risant les quartiers centraux, puis une distinction assez nette entre les secteurs est et ouest de l'agglom√©ration. Cependant, *ClustGeo* regroupe les Iris du sud dans un groupe sp√©cifique alors que SFCM a identifi√© deux groupes plus atypiques spatialement. Le groupe 5 correspond aux Iris situ√©s le long du boulevard p√©riph√©rique encerclant Lyon.

Le groupe 5 correspond donc √† un ensemble d'Iris avec les plus hauts niveaux de concentration de pollutions atmosph√©rique et sonore (figure ci-dessous). Il est similaire en cela au groupe 1, qui est cependant marqu√© par des niveaux de bruit un peu moins √©lev√©s.

```{r}
#| echo: true 
#| eval: true 
#| message: false 
#| warning: false
spiderPlots(Data,SFCM$Belongings,
            chartcolors = c("darkorange3",
							"grey4",
							"darkgreen",
							"royalblue", 
							"blueviolet"))
```

::: bloc_aller_loin
::: bloc_aller_loin-header
::: bloc_aller_loin-icon
:::

**De multiples saveurs du SFCM**
:::

::: bloc_aller_loin-body
Plusieurs variantes du SFCM sont propos√©es par le *package* `geocmeans` qui offrent une grande flexibilit√©¬†:

-   La version g√©n√©ralis√©e du SFCM (appel√©e SFGCM) avec l'ajout d'un param√®tre suppl√©mentaire (*beta*) qui acc√©l√®re la convergence du SFCM en limitant la probabilit√© attribu√©e au groupe le moins probable de chaque observation.
-   Les versions robustes du SFCM et SFGCM, qui normalisent les distances calcul√©es entre les observations pour r√©aliser des classifications non sph√©riques.
-   L'ajout d'un groupe de r√©sidus, permettant d'attribuer les observations tr√®s incertaines √† un groupe ¬´¬†poubelle¬†¬ª, ce qui √©vite d'influencer la coh√©rence des ¬´¬†vrais groupes¬†¬ª avec des observations tr√®s atypiques.
:::
:::

## Quiz de r√©vision du chapitre {#sec-083}

```{r}
#| label: quizChapitre08
#| echo: true 
#| message: false 
#| warning: false 
#| results: asis

source("code_complementaire/QuizzFunctions.R")
Chap08Quiz <- quizz("quiz/Chap08.yml", "Chap08")
render_quizz(Chap08Quiz)
```

## Exercices de r√©vision {#sec-084}

::: bloc_exercice
::: bloc_exercice-header
::: bloc_exercice-icon
:::

**Exercice 1.** R√©alisation de classification avec contrainte spatiale avec des variables socio√©conomiques
:::

::: bloc_exercice-body
```{r}
#| echo: true 
#| message: false 
#| warning: false 
#| eval: false
library(rgeoda)
library(sf)
library(tmap)
## Pr√©paration des donn√©es
load("data/chap08/DonneesLyon.Rdata")
VarSocioEco <- c("Pct0_14", "Pct_65", "Pct_Img", "Pct_brevet", "NivVieMed")
Data2 <- st_drop_geometry(LyonIris[VarSocioEco])
queen_w <- queen_weights(LyonIris)
## Classification avec k = 4
azp5_sa  <- √† compl√©ter
azp5_tab <- √† compl√©ter
skater5  <- rgeoda::skater(√† compl√©ter)
redcap5  <- √† compl√©ter
## Cartographie des r√©sultats
LyonIris$SE.azp4_sa  <- √† compl√©ter
LyonIris$SE.azp4_tab <- √† compl√©ter
LyonIris$SE.skater4 <- √† compl√©ter
LyonIris$SE.recap4  <- √† compl√©ter
Carte1 <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
  tm_fill(col="SE.azp4_sa", palette = "Set1", title ="")+
  tm_layout(frame=FALSE,  main.title = "a. AZP-SA",
            main.title.position = "center", main.title.size = 1)
Carte2 <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
  tm_fill(col="SE.azp4_tab", palette = "Set1", title ="")+
  tm_layout(frame=FALSE,  main.title = "b. AZP-TABU",
            main.title.position = "center", main.title.size = 1)
Carte3 <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
  tm_fill(col="SE.skater4", palette = "Set1", title ="")+
  tm_layout(frame=FALSE,  main.title = "c. Skater",
            main.title.position = "center", main.title.size = 1)
Carte4 <- tm_shape(LyonIris)+tm_borders(col="gray", lwd=.5)+
  tm_fill(col="SE.recap4", palette = "Set1", title ="")+
  tm_layout(frame=FALSE,  main.title = "d. RECAP",
            main.title.position = "center", main.title.size = 1)

tmap_arrange(√† compl√©ter)
```

Correction √† la [section @sec-12081].
:::
:::
